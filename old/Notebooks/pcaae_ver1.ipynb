{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "#from tensorboardX import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.out_params = kwargs\n",
    "        self.encoder = self._create_encoder()\n",
    "        # Save encoder weights for the next model\n",
    "        self.encoder_weights = self.encoder.state_dict().copy()\n",
    "        self.decoder = self._create_decoder()\n",
    "\n",
    "\n",
    "    def increase_latentdim(self):\n",
    "        with torch.no_grad():\n",
    "            # Increase size of the bottleneck\n",
    "            self.hidden_layers[-1] += 1\n",
    "            # Create new encoder\n",
    "            self.encoder = self._create_encoder()\n",
    "            # Copy weights from the old encoder\n",
    "            self.copy_encoder_weights()\n",
    "            # Create a new decoder\n",
    "            self.decoder = self._create_decoder()\n",
    "\n",
    "\n",
    "    def copy_encoder_weights(self):\n",
    "        # Copy weights from the old encoder\n",
    "        new_state_dict = self.encoder.state_dict().copy()\n",
    "        for param in new_state_dict.keys():\n",
    "            if param.endswith('.weight') or param.endswith('.bias'):\n",
    "                if new_state_dict[param].size() != self.encoder_weights[param].size():\n",
    "                    oldsize = tuple(slice(None, dim) for dim in self.encoder_weights[param].size())\n",
    "                    new_state_dict[param][oldsize] = self.encoder_weights[param]\n",
    "                else:\n",
    "                    new_state_dict[param] = self.encoder_weights[param]\n",
    "        # Loading the new state dict into the encoder\n",
    "        self.encoder.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "    def save_encoder_weights(self):\n",
    "        # Saving new state dict as the encoder weigths\n",
    "        self.encoder_weights = self.encoder.state_dict().copy()\n",
    "            \n",
    "\n",
    "    def _create_encoder(self):\n",
    "        # Input layer\n",
    "        encoder = nn.ModuleList()\n",
    "        encoder.append(nn.Linear(self.input_size, self.hidden_layers[0]))\n",
    "        encoder.append(nn.BatchNorm1d(self.hidden_layers[0]))\n",
    "        # Creating the subsequent layers of the encoder\n",
    "        for i in range(1, len(self.hidden_layers)):\n",
    "            encoder.append(nn.Linear(self.hidden_layers[i-1], self.hidden_layers[i]))\n",
    "            # Last layer in encoder should be linear no matter what\n",
    "            if i != len(self.hidden_layers) -1:\n",
    "                #encoder.append(nn.BatchNorm1d(self.hidden_layers[i]))\n",
    "                encoder.append(nn.ReLU())\n",
    "        return encoder\n",
    "\n",
    "\n",
    "    def _create_decoder(self):\n",
    "        # Layers before output layer\n",
    "        decoder = nn.ModuleList()\n",
    "        for i in reversed(range(1, len(self.hidden_layers))):\n",
    "            decoder.append(nn.Linear(self.hidden_layers[i], self.hidden_layers[i-1]))\n",
    "            #decoder.append(nn.BatchNorm1d(self.hidden_layers[i-1]))\n",
    "            decoder.append(nn.ReLU())\n",
    "        # Output layer\n",
    "        decoder.append(nn.Linear(self.hidden_layers[0], self.input_size))\n",
    "        return decoder\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            # if isinstance(layer, nn.BatchNorm1d):\n",
    "            #     x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def decode(self, x):\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "            # if isinstance(layer, nn.BatchNorm1d):\n",
    "            #     x = F.relu(x)\n",
    "        # If an output activation function is specified\n",
    "        if 'out_act' in self.out_params.keys():\n",
    "            return self.out_params['out_act'](x)\n",
    "        # If a separate activation function is specified for the nhumeric and the categoric outputs\n",
    "        elif 'num_act' in self.out_params.keys():\n",
    "            try:\n",
    "                x_num = self.out_params['num_act'](x[:, :self.out_params['num_features']])\n",
    "                x_cat = self.out_params['cat_act'](x[:, self.out_params['num_features']:])\n",
    "                x = torch.cat((x_num, x_cat), dim=1)\n",
    "                return x\n",
    "            except KeyError as e:\n",
    "                print(f\"{e}: Specify all output parameters\")\n",
    "        # If no activation function is specified, use a linear one\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self.decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, goal_hidden_dim, optimizer, loss_func, epochs, trainloader, testloader, print_every):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    outer_steps = 0\n",
    "    total_steps = 0\n",
    "    total_train_losses, total_test_losses = [], []\n",
    "    total_min_testloss = np.Inf\n",
    "    \n",
    "    while model.hidden_layers[-1] != goal_hidden_dim:\n",
    "        if not outer_steps == 0:\n",
    "            # Increasing latent space\n",
    "            model.increase_latentdim()\n",
    "            model.to(device)\n",
    "            # Freezing encoder layers\n",
    "            all_layers = len([layer for layer in model.encoder.parameters()])\n",
    "            for i, layer in enumerate(model.encoder.parameters()):\n",
    "                # Last layer in encoder is not freezed\n",
    "                if i == all_layers - 2:\n",
    "                    break\n",
    "                else:\n",
    "                    layer.requires_grad = False\n",
    "\n",
    "        outer_steps += 1\n",
    "        print(f\"Training with hidden dim: {model.hidden_layers[-1]}\")\n",
    "        steps = 0\n",
    "        train_losses, test_losses = [], []\n",
    "        min_test_loss = np.Inf\n",
    "\n",
    "        # Training loop\n",
    "        for e in range(epochs):\n",
    "            running_loss = 0\n",
    "            # Only for printing it\n",
    "            running_loss_ = 0\n",
    "            for X, y in trainloader:\n",
    "                steps += 1\n",
    "                total_steps += 1\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                y_hat = model(X)\n",
    "                loss = loss_func(y_hat, y)\n",
    "                loss.backward()\n",
    "                # TODO zero out gradient weights for all but the last added weight in the bottleneck layer of the encoder\n",
    "                all_layers = len([layer for layer in model.encoder.parameters()])\n",
    "                for i, layer in enumerate(model.encoder.parameters()):\n",
    "                    # Last layer in encoder is not freezed\n",
    "                    if i == all_layers - 2 or i == all_layers - 1:\n",
    "                        layer.grad[0:-1] = torch.zeros_like(layer.grad[0:-1])\n",
    "                    else:\n",
    "                        pass\n",
    "                optimizer.step()\n",
    "                # Copy the weights from the old latent dim to the new\n",
    "                #model.copy_encoder_weights()\n",
    "                running_loss += loss.item()*X.size(0)\n",
    "                running_loss_ += loss.item()\n",
    "\n",
    "                if steps % print_every == 0:\n",
    "                    print(f\"Epoch: {e + 1}/{epochs}, Step {steps}, Train loss: {running_loss_/print_every:.3f}\")\n",
    "                    running_loss_ = 0\n",
    "\n",
    "            # Running model on the test data  \n",
    "            else:\n",
    "                running_testloss = 0\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    for X, y in testloader:\n",
    "                        X, y = X.to(device), y.to(device)\n",
    "                        y_hat = model(X)\n",
    "                        test_loss = loss_func(y_hat, y)\n",
    "                        running_testloss += test_loss.item()*X.size(0)\n",
    "                model.train()\n",
    "\n",
    "                train_losses.append(running_loss/len(trainloader.dataset))\n",
    "                total_train_losses.append(running_loss/len(trainloader.dataset))\n",
    "                test_losses.append(running_testloss/len(testloader.dataset))\n",
    "                total_test_losses.append(running_testloss/len(testloader.dataset))\n",
    "\n",
    "                # Saving model when test loss improved\n",
    "                if test_losses[-1] <= min_test_loss:\n",
    "                    print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(min_test_loss,test_losses[-1]))\n",
    "                    # Saving the best weigths for the encoder to load back\n",
    "                    model.save_encoder_weights()\n",
    "                    torch.save(model.state_dict(), f'PCAAE_hidden_dim{model.hidden_layers[-1]}.pt')\n",
    "                    min_test_loss = test_losses[-1]\n",
    "                \n",
    "                print(f'Epoch {e+1}/{epochs}, Train Loss: {running_loss/len(trainloader.dataset):.3f}, Test Loss: {running_testloss/len(testloader.dataset):.3f}')\n",
    "\n",
    "    return total_train_losses, total_test_losses, total_steps              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for tabular data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "inputFeature = pd.read_csv('../../Data/NIBRS_ND_2021/processed/input.csv', index_col='Unnamed: 0')\n",
    "# Separating numerical and categorical features\n",
    "numerical_features=['population','victim_seq_num','age_num_victim','incident_hour','incident_month','incident_day','incident_dayofmonth','incident_weekofyear']\n",
    "categorical_features = ['resident_status_code','race_desc_victim',\n",
    "'ethnicity_name_victim','pub_agency_name','offense_name','location_name','weapon_name'\n",
    ",'injury_name','relationship_name','incident_isweekend']\n",
    "# Onehot-encoding categorical features\n",
    "inputFeature_1h = pd.get_dummies(inputFeature, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object columns to numeric if they represent categories\n",
    "for column in inputFeature_1h.select_dtypes(include=['object']):\n",
    "    inputFeature_1h[column] = inputFeature_1h[column].astype('category').cat.codes\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(inputFeature_1h, test_size=0.1, random_state=42)\n",
    "\n",
    "# Normalizing numerical features\n",
    "for feature in numerical_features:\n",
    "  train[feature] = (train[feature] - train[feature].min()) / (train[feature].max() - train[feature].min())\n",
    "  test[feature] = (test[feature] - test[feature].min()) / (test[feature].max() - test[feature].min())\n",
    "\n",
    "# Converting data to tensors\n",
    "X_train = torch.nan_to_num(torch.Tensor(train.values.astype(np.float32)))\n",
    "y_train = torch.nan_to_num(torch.Tensor(train.values.astype(np.float32)))\n",
    "\n",
    "X_test = torch.nan_to_num(torch.Tensor(test.values.astype(np.float32)))\n",
    "y_test = torch.nan_to_num(torch.Tensor(test.values.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "trainset = MyDataset(X_train, y_train)\n",
    "testset = MyDataset(X_test, y_test)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCAAutoencoder(X_train.shape[1], [32, 16, 1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 20\n",
    "print_every = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoder.0.weight', 'encoder.0.bias', 'encoder.1.weight', 'encoder.1.bias', 'encoder.1.running_mean', 'encoder.1.running_var', 'encoder.1.num_batches_tracked', 'encoder.2.weight', 'encoder.2.bias', 'encoder.4.weight', 'encoder.4.bias', 'decoder.0.weight', 'decoder.0.bias', 'decoder.2.weight', 'decoder.2.bias', 'decoder.4.weight', 'decoder.4.bias'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden dim: 1\n",
      "Epoch: 1/20, Step 40, Train loss: 0.024\n",
      "Epoch: 1/20, Step 80, Train loss: 0.024\n",
      "Epoch: 1/20, Step 120, Train loss: 0.023\n",
      "Epoch: 1/20, Step 160, Train loss: 0.023\n",
      "Epoch: 1/20, Step 200, Train loss: 0.023\n",
      "Epoch: 1/20, Step 240, Train loss: 0.023\n",
      "Epoch: 1/20, Step 280, Train loss: 0.022\n",
      "Epoch: 1/20, Step 320, Train loss: 0.023\n",
      "Epoch: 1/20, Step 360, Train loss: 0.023\n",
      "Epoch: 1/20, Step 400, Train loss: 0.022\n",
      "Epoch: 1/20, Step 440, Train loss: 0.022\n",
      "Epoch: 1/20, Step 480, Train loss: 0.023\n",
      "Test loss decreased (inf --> 0.022402).  Saving model ...\n",
      "Epoch 1/20, Train Loss: 0.023, Test Loss: 0.022\n",
      "Epoch: 2/20, Step 520, Train loss: 0.022\n",
      "Epoch: 2/20, Step 560, Train loss: 0.022\n",
      "Epoch: 2/20, Step 600, Train loss: 0.022\n",
      "Epoch: 2/20, Step 640, Train loss: 0.022\n",
      "Epoch: 2/20, Step 680, Train loss: 0.022\n",
      "Epoch: 2/20, Step 720, Train loss: 0.022\n",
      "Epoch: 2/20, Step 760, Train loss: 0.022\n",
      "Epoch: 2/20, Step 800, Train loss: 0.022\n",
      "Epoch: 2/20, Step 840, Train loss: 0.022\n",
      "Epoch: 2/20, Step 880, Train loss: 0.022\n",
      "Epoch: 2/20, Step 920, Train loss: 0.022\n",
      "Epoch: 2/20, Step 960, Train loss: 0.022\n",
      "Test loss decreased (0.022402 --> 0.022088).  Saving model ...\n",
      "Epoch 2/20, Train Loss: 0.022, Test Loss: 0.022\n",
      "Epoch: 3/20, Step 1000, Train loss: 0.022\n",
      "Epoch: 3/20, Step 1040, Train loss: 0.023\n",
      "Epoch: 3/20, Step 1080, Train loss: 0.023\n",
      "Epoch: 3/20, Step 1120, Train loss: 0.022\n",
      "Epoch: 3/20, Step 1160, Train loss: 0.022\n",
      "Epoch: 3/20, Step 1200, Train loss: 0.022\n",
      "Epoch: 3/20, Step 1240, Train loss: 0.022\n",
      "Epoch: 3/20, Step 1280, Train loss: 0.022\n",
      "Epoch: 3/20, Step 1320, Train loss: 0.022\n",
      "Epoch: 3/20, Step 1360, Train loss: 0.022\n",
      "Epoch: 3/20, Step 1400, Train loss: 0.022\n",
      "Epoch: 3/20, Step 1440, Train loss: 0.022\n",
      "Test loss decreased (0.022088 --> 0.021817).  Saving model ...\n",
      "Epoch 3/20, Train Loss: 0.022, Test Loss: 0.022\n",
      "Epoch: 4/20, Step 1480, Train loss: 0.022\n",
      "Epoch: 4/20, Step 1520, Train loss: 0.021\n",
      "Epoch: 4/20, Step 1560, Train loss: 0.022\n",
      "Epoch: 4/20, Step 1600, Train loss: 0.022\n",
      "Epoch: 4/20, Step 1640, Train loss: 0.021\n",
      "Epoch: 4/20, Step 1680, Train loss: 0.022\n",
      "Epoch: 4/20, Step 1720, Train loss: 0.021\n",
      "Epoch: 4/20, Step 1760, Train loss: 0.022\n",
      "Epoch: 4/20, Step 1800, Train loss: 0.021\n",
      "Epoch: 4/20, Step 1840, Train loss: 0.021\n",
      "Epoch: 4/20, Step 1880, Train loss: 0.021\n",
      "Epoch: 4/20, Step 1920, Train loss: 0.021\n",
      "Test loss decreased (0.021817 --> 0.021322).  Saving model ...\n",
      "Epoch 4/20, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 5/20, Step 1960, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2000, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2040, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2080, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2120, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2160, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2200, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2240, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2280, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2320, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2360, Train loss: 0.021\n",
      "Epoch: 5/20, Step 2400, Train loss: 0.021\n",
      "Test loss decreased (0.021322 --> 0.020780).  Saving model ...\n",
      "Epoch 5/20, Train Loss: 0.021, Test Loss: 0.021\n",
      "Epoch: 6/20, Step 2440, Train loss: 0.021\n",
      "Epoch: 6/20, Step 2480, Train loss: 0.021\n",
      "Epoch: 6/20, Step 2520, Train loss: 0.020\n",
      "Epoch: 6/20, Step 2560, Train loss: 0.021\n",
      "Epoch: 6/20, Step 2600, Train loss: 0.021\n",
      "Epoch: 6/20, Step 2640, Train loss: 0.021\n",
      "Epoch: 6/20, Step 2680, Train loss: 0.021\n",
      "Epoch: 6/20, Step 2720, Train loss: 0.020\n",
      "Epoch: 6/20, Step 2760, Train loss: 0.021\n",
      "Epoch: 6/20, Step 2800, Train loss: 0.021\n",
      "Epoch: 6/20, Step 2840, Train loss: 0.020\n",
      "Epoch: 6/20, Step 2880, Train loss: 0.021\n",
      "Test loss decreased (0.020780 --> 0.020492).  Saving model ...\n",
      "Epoch 6/20, Train Loss: 0.021, Test Loss: 0.020\n",
      "Epoch: 7/20, Step 2920, Train loss: 0.021\n",
      "Epoch: 7/20, Step 2960, Train loss: 0.020\n",
      "Epoch: 7/20, Step 3000, Train loss: 0.021\n",
      "Epoch: 7/20, Step 3040, Train loss: 0.020\n",
      "Epoch: 7/20, Step 3080, Train loss: 0.020\n",
      "Epoch: 7/20, Step 3120, Train loss: 0.020\n",
      "Epoch: 7/20, Step 3160, Train loss: 0.021\n",
      "Epoch: 7/20, Step 3200, Train loss: 0.020\n",
      "Epoch: 7/20, Step 3240, Train loss: 0.020\n",
      "Epoch: 7/20, Step 3280, Train loss: 0.020\n",
      "Epoch: 7/20, Step 3320, Train loss: 0.020\n",
      "Epoch: 7/20, Step 3360, Train loss: 0.020\n",
      "Test loss decreased (0.020492 --> 0.020251).  Saving model ...\n",
      "Epoch 7/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 8/20, Step 3400, Train loss: 0.020\n",
      "Epoch: 8/20, Step 3440, Train loss: 0.020\n",
      "Epoch: 8/20, Step 3480, Train loss: 0.020\n",
      "Epoch: 8/20, Step 3520, Train loss: 0.020\n",
      "Epoch: 8/20, Step 3560, Train loss: 0.020\n",
      "Epoch: 8/20, Step 3600, Train loss: 0.021\n",
      "Epoch: 8/20, Step 3640, Train loss: 0.020\n",
      "Epoch: 8/20, Step 3680, Train loss: 0.020\n",
      "Epoch: 8/20, Step 3720, Train loss: 0.020\n",
      "Epoch: 8/20, Step 3760, Train loss: 0.020\n",
      "Epoch: 8/20, Step 3800, Train loss: 0.020\n",
      "Epoch: 8/20, Step 3840, Train loss: 0.020\n",
      "Test loss decreased (0.020251 --> 0.020044).  Saving model ...\n",
      "Epoch 8/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 9/20, Step 3880, Train loss: 0.020\n",
      "Epoch: 9/20, Step 3920, Train loss: 0.020\n",
      "Epoch: 9/20, Step 3960, Train loss: 0.020\n",
      "Epoch: 9/20, Step 4000, Train loss: 0.020\n",
      "Epoch: 9/20, Step 4040, Train loss: 0.020\n",
      "Epoch: 9/20, Step 4080, Train loss: 0.020\n",
      "Epoch: 9/20, Step 4120, Train loss: 0.020\n",
      "Epoch: 9/20, Step 4160, Train loss: 0.020\n",
      "Epoch: 9/20, Step 4200, Train loss: 0.020\n",
      "Epoch: 9/20, Step 4240, Train loss: 0.020\n",
      "Epoch: 9/20, Step 4280, Train loss: 0.020\n",
      "Epoch: 9/20, Step 4320, Train loss: 0.020\n",
      "Test loss decreased (0.020044 --> 0.019947).  Saving model ...\n",
      "Epoch 9/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 10/20, Step 4360, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4400, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4440, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4480, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4520, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4560, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4600, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4640, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4680, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4720, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4760, Train loss: 0.020\n",
      "Epoch: 10/20, Step 4800, Train loss: 0.019\n",
      "Test loss decreased (0.019947 --> 0.019899).  Saving model ...\n",
      "Epoch 10/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 11/20, Step 4840, Train loss: 0.020\n",
      "Epoch: 11/20, Step 4880, Train loss: 0.020\n",
      "Epoch: 11/20, Step 4920, Train loss: 0.020\n",
      "Epoch: 11/20, Step 4960, Train loss: 0.020\n",
      "Epoch: 11/20, Step 5000, Train loss: 0.020\n",
      "Epoch: 11/20, Step 5040, Train loss: 0.020\n",
      "Epoch: 11/20, Step 5080, Train loss: 0.020\n",
      "Epoch: 11/20, Step 5120, Train loss: 0.020\n",
      "Epoch: 11/20, Step 5160, Train loss: 0.020\n",
      "Epoch: 11/20, Step 5200, Train loss: 0.020\n",
      "Epoch: 11/20, Step 5240, Train loss: 0.020\n",
      "Epoch: 11/20, Step 5280, Train loss: 0.020\n",
      "Test loss decreased (0.019899 --> 0.019840).  Saving model ...\n",
      "Epoch 11/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 12/20, Step 5320, Train loss: 0.020\n",
      "Epoch: 12/20, Step 5360, Train loss: 0.020\n",
      "Epoch: 12/20, Step 5400, Train loss: 0.020\n",
      "Epoch: 12/20, Step 5440, Train loss: 0.020\n",
      "Epoch: 12/20, Step 5480, Train loss: 0.019\n",
      "Epoch: 12/20, Step 5520, Train loss: 0.020\n",
      "Epoch: 12/20, Step 5560, Train loss: 0.020\n",
      "Epoch: 12/20, Step 5600, Train loss: 0.020\n",
      "Epoch: 12/20, Step 5640, Train loss: 0.020\n",
      "Epoch: 12/20, Step 5680, Train loss: 0.020\n",
      "Epoch: 12/20, Step 5720, Train loss: 0.020\n",
      "Epoch: 12/20, Step 5760, Train loss: 0.020\n",
      "Test loss decreased (0.019840 --> 0.019812).  Saving model ...\n",
      "Epoch 12/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 13/20, Step 5800, Train loss: 0.020\n",
      "Epoch: 13/20, Step 5840, Train loss: 0.020\n",
      "Epoch: 13/20, Step 5880, Train loss: 0.020\n",
      "Epoch: 13/20, Step 5920, Train loss: 0.019\n",
      "Epoch: 13/20, Step 5960, Train loss: 0.020\n",
      "Epoch: 13/20, Step 6000, Train loss: 0.020\n",
      "Epoch: 13/20, Step 6040, Train loss: 0.020\n",
      "Epoch: 13/20, Step 6080, Train loss: 0.020\n",
      "Epoch: 13/20, Step 6120, Train loss: 0.020\n",
      "Epoch: 13/20, Step 6160, Train loss: 0.020\n",
      "Epoch: 13/20, Step 6200, Train loss: 0.020\n",
      "Epoch: 13/20, Step 6240, Train loss: 0.020\n",
      "Test loss decreased (0.019812 --> 0.019728).  Saving model ...\n",
      "Epoch 13/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 14/20, Step 6280, Train loss: 0.019\n",
      "Epoch: 14/20, Step 6320, Train loss: 0.020\n",
      "Epoch: 14/20, Step 6360, Train loss: 0.020\n",
      "Epoch: 14/20, Step 6400, Train loss: 0.020\n",
      "Epoch: 14/20, Step 6440, Train loss: 0.020\n",
      "Epoch: 14/20, Step 6480, Train loss: 0.020\n",
      "Epoch: 14/20, Step 6520, Train loss: 0.020\n",
      "Epoch: 14/20, Step 6560, Train loss: 0.020\n",
      "Epoch: 14/20, Step 6600, Train loss: 0.019\n",
      "Epoch: 14/20, Step 6640, Train loss: 0.020\n",
      "Epoch: 14/20, Step 6680, Train loss: 0.020\n",
      "Epoch: 14/20, Step 6720, Train loss: 0.019\n",
      "Test loss decreased (0.019728 --> 0.019645).  Saving model ...\n",
      "Epoch 14/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 15/20, Step 6760, Train loss: 0.020\n",
      "Epoch: 15/20, Step 6800, Train loss: 0.019\n",
      "Epoch: 15/20, Step 6840, Train loss: 0.020\n",
      "Epoch: 15/20, Step 6880, Train loss: 0.019\n",
      "Epoch: 15/20, Step 6920, Train loss: 0.019\n",
      "Epoch: 15/20, Step 6960, Train loss: 0.020\n",
      "Epoch: 15/20, Step 7000, Train loss: 0.019\n",
      "Epoch: 15/20, Step 7040, Train loss: 0.020\n",
      "Epoch: 15/20, Step 7080, Train loss: 0.020\n",
      "Epoch: 15/20, Step 7120, Train loss: 0.020\n",
      "Epoch: 15/20, Step 7160, Train loss: 0.020\n",
      "Epoch: 15/20, Step 7200, Train loss: 0.020\n",
      "Epoch 15/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 16/20, Step 7240, Train loss: 0.019\n",
      "Epoch: 16/20, Step 7280, Train loss: 0.019\n",
      "Epoch: 16/20, Step 7320, Train loss: 0.019\n",
      "Epoch: 16/20, Step 7360, Train loss: 0.020\n",
      "Epoch: 16/20, Step 7400, Train loss: 0.020\n",
      "Epoch: 16/20, Step 7440, Train loss: 0.019\n",
      "Epoch: 16/20, Step 7480, Train loss: 0.020\n",
      "Epoch: 16/20, Step 7520, Train loss: 0.020\n",
      "Epoch: 16/20, Step 7560, Train loss: 0.020\n",
      "Epoch: 16/20, Step 7600, Train loss: 0.019\n",
      "Epoch: 16/20, Step 7640, Train loss: 0.019\n",
      "Epoch: 16/20, Step 7680, Train loss: 0.020\n",
      "Test loss decreased (0.019645 --> 0.019622).  Saving model ...\n",
      "Epoch 16/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 17/20, Step 7720, Train loss: 0.020\n",
      "Epoch: 17/20, Step 7760, Train loss: 0.020\n",
      "Epoch: 17/20, Step 7800, Train loss: 0.019\n",
      "Epoch: 17/20, Step 7840, Train loss: 0.019\n",
      "Epoch: 17/20, Step 7880, Train loss: 0.019\n",
      "Epoch: 17/20, Step 7920, Train loss: 0.020\n",
      "Epoch: 17/20, Step 7960, Train loss: 0.019\n",
      "Epoch: 17/20, Step 8000, Train loss: 0.020\n",
      "Epoch: 17/20, Step 8040, Train loss: 0.019\n",
      "Epoch: 17/20, Step 8080, Train loss: 0.020\n",
      "Epoch: 17/20, Step 8120, Train loss: 0.019\n",
      "Epoch: 17/20, Step 8160, Train loss: 0.020\n",
      "Test loss decreased (0.019622 --> 0.019567).  Saving model ...\n",
      "Epoch 17/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 18/20, Step 8200, Train loss: 0.020\n",
      "Epoch: 18/20, Step 8240, Train loss: 0.019\n",
      "Epoch: 18/20, Step 8280, Train loss: 0.020\n",
      "Epoch: 18/20, Step 8320, Train loss: 0.019\n",
      "Epoch: 18/20, Step 8360, Train loss: 0.019\n",
      "Epoch: 18/20, Step 8400, Train loss: 0.019\n",
      "Epoch: 18/20, Step 8440, Train loss: 0.019\n",
      "Epoch: 18/20, Step 8480, Train loss: 0.019\n",
      "Epoch: 18/20, Step 8520, Train loss: 0.019\n",
      "Epoch: 18/20, Step 8560, Train loss: 0.020\n",
      "Epoch: 18/20, Step 8600, Train loss: 0.019\n",
      "Epoch: 18/20, Step 8640, Train loss: 0.019\n",
      "Test loss decreased (0.019567 --> 0.019551).  Saving model ...\n",
      "Epoch 18/20, Train Loss: 0.020, Test Loss: 0.020\n",
      "Epoch: 19/20, Step 8680, Train loss: 0.019\n",
      "Epoch: 19/20, Step 8720, Train loss: 0.020\n",
      "Epoch: 19/20, Step 8760, Train loss: 0.019\n",
      "Epoch: 19/20, Step 8800, Train loss: 0.019\n",
      "Epoch: 19/20, Step 8840, Train loss: 0.019\n",
      "Epoch: 19/20, Step 8880, Train loss: 0.019\n",
      "Epoch: 19/20, Step 8920, Train loss: 0.020\n",
      "Epoch: 19/20, Step 8960, Train loss: 0.019\n",
      "Epoch: 19/20, Step 9000, Train loss: 0.019\n",
      "Epoch: 19/20, Step 9040, Train loss: 0.020\n",
      "Epoch: 19/20, Step 9080, Train loss: 0.020\n",
      "Epoch: 19/20, Step 9120, Train loss: 0.019\n",
      "Test loss decreased (0.019551 --> 0.019489).  Saving model ...\n",
      "Epoch 19/20, Train Loss: 0.019, Test Loss: 0.019\n",
      "Epoch: 20/20, Step 9160, Train loss: 0.019\n",
      "Epoch: 20/20, Step 9200, Train loss: 0.019\n",
      "Epoch: 20/20, Step 9240, Train loss: 0.020\n",
      "Epoch: 20/20, Step 9280, Train loss: 0.019\n",
      "Epoch: 20/20, Step 9320, Train loss: 0.020\n",
      "Epoch: 20/20, Step 9360, Train loss: 0.019\n",
      "Epoch: 20/20, Step 9400, Train loss: 0.019\n",
      "Epoch: 20/20, Step 9440, Train loss: 0.019\n",
      "Epoch: 20/20, Step 9480, Train loss: 0.020\n",
      "Epoch: 20/20, Step 9520, Train loss: 0.019\n",
      "Epoch: 20/20, Step 9560, Train loss: 0.019\n",
      "Epoch: 20/20, Step 9600, Train loss: 0.020\n",
      "Test loss decreased (0.019489 --> 0.019478).  Saving model ...\n",
      "Epoch 20/20, Train Loss: 0.019, Test Loss: 0.019\n",
      "Training with hidden dim: 2\n",
      "Epoch: 1/20, Step 40, Train loss: 0.157\n",
      "Epoch: 1/20, Step 80, Train loss: 0.156\n",
      "Epoch: 1/20, Step 120, Train loss: 0.157\n",
      "Epoch: 1/20, Step 160, Train loss: 0.157\n",
      "Epoch: 1/20, Step 200, Train loss: 0.156\n",
      "Epoch: 1/20, Step 240, Train loss: 0.157\n",
      "Epoch: 1/20, Step 280, Train loss: 0.157\n",
      "Epoch: 1/20, Step 320, Train loss: 0.154\n",
      "Epoch: 1/20, Step 360, Train loss: 0.158\n",
      "Epoch: 1/20, Step 400, Train loss: 0.159\n",
      "Epoch: 1/20, Step 440, Train loss: 0.157\n",
      "Epoch: 1/20, Step 480, Train loss: 0.156\n",
      "Test loss decreased (inf --> 0.164559).  Saving model ...\n",
      "Epoch 1/20, Train Loss: 0.157, Test Loss: 0.165\n",
      "Epoch: 2/20, Step 520, Train loss: 0.159\n",
      "Epoch: 2/20, Step 560, Train loss: 0.155\n",
      "Epoch: 2/20, Step 600, Train loss: 0.154\n",
      "Epoch: 2/20, Step 640, Train loss: 0.158\n",
      "Epoch: 2/20, Step 680, Train loss: 0.158\n",
      "Epoch: 2/20, Step 720, Train loss: 0.157\n",
      "Epoch: 2/20, Step 760, Train loss: 0.161\n",
      "Epoch: 2/20, Step 800, Train loss: 0.155\n",
      "Epoch: 2/20, Step 840, Train loss: 0.157\n",
      "Epoch: 2/20, Step 880, Train loss: 0.154\n",
      "Epoch: 2/20, Step 920, Train loss: 0.156\n",
      "Epoch: 2/20, Step 960, Train loss: 0.153\n",
      "Epoch 2/20, Train Loss: 0.156, Test Loss: 0.166\n",
      "Epoch: 3/20, Step 1000, Train loss: 0.159\n",
      "Epoch: 3/20, Step 1040, Train loss: 0.157\n",
      "Epoch: 3/20, Step 1080, Train loss: 0.158\n",
      "Epoch: 3/20, Step 1120, Train loss: 0.155\n",
      "Epoch: 3/20, Step 1160, Train loss: 0.156\n",
      "Epoch: 3/20, Step 1200, Train loss: 0.156\n",
      "Epoch: 3/20, Step 1240, Train loss: 0.159\n",
      "Epoch: 3/20, Step 1280, Train loss: 0.158\n",
      "Epoch: 3/20, Step 1320, Train loss: 0.157\n",
      "Epoch: 3/20, Step 1360, Train loss: 0.159\n",
      "Epoch: 3/20, Step 1400, Train loss: 0.155\n",
      "Epoch: 3/20, Step 1440, Train loss: 0.155\n",
      "Test loss decreased (0.164559 --> 0.162534).  Saving model ...\n",
      "Epoch 3/20, Train Loss: 0.157, Test Loss: 0.163\n",
      "Epoch: 4/20, Step 1480, Train loss: 0.157\n",
      "Epoch: 4/20, Step 1520, Train loss: 0.157\n",
      "Epoch: 4/20, Step 1560, Train loss: 0.154\n",
      "Epoch: 4/20, Step 1600, Train loss: 0.155\n",
      "Epoch: 4/20, Step 1640, Train loss: 0.156\n",
      "Epoch: 4/20, Step 1680, Train loss: 0.156\n",
      "Epoch: 4/20, Step 1720, Train loss: 0.158\n",
      "Epoch: 4/20, Step 1760, Train loss: 0.158\n",
      "Epoch: 4/20, Step 1800, Train loss: 0.157\n",
      "Epoch: 4/20, Step 1840, Train loss: 0.156\n",
      "Epoch: 4/20, Step 1880, Train loss: 0.156\n",
      "Epoch: 4/20, Step 1920, Train loss: 0.157\n",
      "Test loss decreased (0.162534 --> 0.161714).  Saving model ...\n",
      "Epoch 4/20, Train Loss: 0.156, Test Loss: 0.162\n",
      "Epoch: 5/20, Step 1960, Train loss: 0.156\n",
      "Epoch: 5/20, Step 2000, Train loss: 0.157\n",
      "Epoch: 5/20, Step 2040, Train loss: 0.157\n",
      "Epoch: 5/20, Step 2080, Train loss: 0.153\n",
      "Epoch: 5/20, Step 2120, Train loss: 0.157\n",
      "Epoch: 5/20, Step 2160, Train loss: 0.157\n",
      "Epoch: 5/20, Step 2200, Train loss: 0.156\n",
      "Epoch: 5/20, Step 2240, Train loss: 0.157\n",
      "Epoch: 5/20, Step 2280, Train loss: 0.159\n",
      "Epoch: 5/20, Step 2320, Train loss: 0.156\n",
      "Epoch: 5/20, Step 2360, Train loss: 0.156\n",
      "Epoch: 5/20, Step 2400, Train loss: 0.157\n",
      "Epoch 5/20, Train Loss: 0.157, Test Loss: 0.166\n",
      "Epoch: 6/20, Step 2440, Train loss: 0.157\n",
      "Epoch: 6/20, Step 2480, Train loss: 0.156\n",
      "Epoch: 6/20, Step 2520, Train loss: 0.157\n",
      "Epoch: 6/20, Step 2560, Train loss: 0.156\n",
      "Epoch: 6/20, Step 2600, Train loss: 0.158\n",
      "Epoch: 6/20, Step 2640, Train loss: 0.157\n",
      "Epoch: 6/20, Step 2680, Train loss: 0.157\n",
      "Epoch: 6/20, Step 2720, Train loss: 0.155\n",
      "Epoch: 6/20, Step 2760, Train loss: 0.157\n",
      "Epoch: 6/20, Step 2800, Train loss: 0.154\n",
      "Epoch: 6/20, Step 2840, Train loss: 0.158\n",
      "Epoch: 6/20, Step 2880, Train loss: 0.154\n",
      "Test loss decreased (0.161714 --> 0.161640).  Saving model ...\n",
      "Epoch 6/20, Train Loss: 0.156, Test Loss: 0.162\n",
      "Epoch: 7/20, Step 2920, Train loss: 0.157\n",
      "Epoch: 7/20, Step 2960, Train loss: 0.157\n",
      "Epoch: 7/20, Step 3000, Train loss: 0.155\n",
      "Epoch: 7/20, Step 3040, Train loss: 0.158\n",
      "Epoch: 7/20, Step 3080, Train loss: 0.157\n",
      "Epoch: 7/20, Step 3120, Train loss: 0.158\n",
      "Epoch: 7/20, Step 3160, Train loss: 0.159\n",
      "Epoch: 7/20, Step 3200, Train loss: 0.157\n",
      "Epoch: 7/20, Step 3240, Train loss: 0.158\n",
      "Epoch: 7/20, Step 3280, Train loss: 0.157\n",
      "Epoch: 7/20, Step 3320, Train loss: 0.154\n",
      "Epoch: 7/20, Step 3360, Train loss: 0.154\n",
      "Epoch 7/20, Train Loss: 0.157, Test Loss: 0.162\n",
      "Epoch: 8/20, Step 3400, Train loss: 0.157\n",
      "Epoch: 8/20, Step 3440, Train loss: 0.157\n",
      "Epoch: 8/20, Step 3480, Train loss: 0.159\n",
      "Epoch: 8/20, Step 3520, Train loss: 0.157\n",
      "Epoch: 8/20, Step 3560, Train loss: 0.156\n",
      "Epoch: 8/20, Step 3600, Train loss: 0.156\n",
      "Epoch: 8/20, Step 3640, Train loss: 0.155\n",
      "Epoch: 8/20, Step 3680, Train loss: 0.156\n",
      "Epoch: 8/20, Step 3720, Train loss: 0.160\n",
      "Epoch: 8/20, Step 3760, Train loss: 0.155\n",
      "Epoch: 8/20, Step 3800, Train loss: 0.156\n",
      "Epoch: 8/20, Step 3840, Train loss: 0.154\n",
      "Epoch 8/20, Train Loss: 0.157, Test Loss: 0.164\n",
      "Epoch: 9/20, Step 3880, Train loss: 0.155\n",
      "Epoch: 9/20, Step 3920, Train loss: 0.157\n",
      "Epoch: 9/20, Step 3960, Train loss: 0.156\n",
      "Epoch: 9/20, Step 4000, Train loss: 0.155\n",
      "Epoch: 9/20, Step 4040, Train loss: 0.158\n",
      "Epoch: 9/20, Step 4080, Train loss: 0.156\n",
      "Epoch: 9/20, Step 4120, Train loss: 0.158\n",
      "Epoch: 9/20, Step 4160, Train loss: 0.157\n",
      "Epoch: 9/20, Step 4200, Train loss: 0.157\n",
      "Epoch: 9/20, Step 4240, Train loss: 0.156\n",
      "Epoch: 9/20, Step 4280, Train loss: 0.158\n",
      "Epoch: 9/20, Step 4320, Train loss: 0.155\n",
      "Epoch 9/20, Train Loss: 0.157, Test Loss: 0.162\n",
      "Epoch: 10/20, Step 4360, Train loss: 0.157\n",
      "Epoch: 10/20, Step 4400, Train loss: 0.156\n",
      "Epoch: 10/20, Step 4440, Train loss: 0.157\n",
      "Epoch: 10/20, Step 4480, Train loss: 0.156\n",
      "Epoch: 10/20, Step 4520, Train loss: 0.158\n",
      "Epoch: 10/20, Step 4560, Train loss: 0.158\n",
      "Epoch: 10/20, Step 4600, Train loss: 0.155\n",
      "Epoch: 10/20, Step 4640, Train loss: 0.155\n",
      "Epoch: 10/20, Step 4680, Train loss: 0.155\n",
      "Epoch: 10/20, Step 4720, Train loss: 0.156\n",
      "Epoch: 10/20, Step 4760, Train loss: 0.157\n",
      "Epoch: 10/20, Step 4800, Train loss: 0.157\n",
      "Test loss decreased (0.161640 --> 0.160329).  Saving model ...\n",
      "Epoch 10/20, Train Loss: 0.156, Test Loss: 0.160\n",
      "Epoch: 11/20, Step 4840, Train loss: 0.158\n",
      "Epoch: 11/20, Step 4880, Train loss: 0.156\n",
      "Epoch: 11/20, Step 4920, Train loss: 0.158\n",
      "Epoch: 11/20, Step 4960, Train loss: 0.155\n",
      "Epoch: 11/20, Step 5000, Train loss: 0.159\n",
      "Epoch: 11/20, Step 5040, Train loss: 0.157\n",
      "Epoch: 11/20, Step 5080, Train loss: 0.160\n",
      "Epoch: 11/20, Step 5120, Train loss: 0.157\n",
      "Epoch: 11/20, Step 5160, Train loss: 0.154\n",
      "Epoch: 11/20, Step 5200, Train loss: 0.157\n",
      "Epoch: 11/20, Step 5240, Train loss: 0.156\n",
      "Epoch: 11/20, Step 5280, Train loss: 0.156\n",
      "Epoch 11/20, Train Loss: 0.157, Test Loss: 0.161\n",
      "Epoch: 12/20, Step 5320, Train loss: 0.155\n",
      "Epoch: 12/20, Step 5360, Train loss: 0.157\n",
      "Epoch: 12/20, Step 5400, Train loss: 0.156\n",
      "Epoch: 12/20, Step 5440, Train loss: 0.155\n",
      "Epoch: 12/20, Step 5480, Train loss: 0.156\n",
      "Epoch: 12/20, Step 5520, Train loss: 0.156\n",
      "Epoch: 12/20, Step 5560, Train loss: 0.159\n",
      "Epoch: 12/20, Step 5600, Train loss: 0.159\n",
      "Epoch: 12/20, Step 5640, Train loss: 0.158\n",
      "Epoch: 12/20, Step 5680, Train loss: 0.157\n",
      "Epoch: 12/20, Step 5720, Train loss: 0.156\n",
      "Epoch: 12/20, Step 5760, Train loss: 0.157\n",
      "Test loss decreased (0.160329 --> 0.152662).  Saving model ...\n",
      "Epoch 12/20, Train Loss: 0.157, Test Loss: 0.153\n",
      "Epoch: 13/20, Step 5800, Train loss: 0.155\n",
      "Epoch: 13/20, Step 5840, Train loss: 0.157\n",
      "Epoch: 13/20, Step 5880, Train loss: 0.154\n",
      "Epoch: 13/20, Step 5920, Train loss: 0.157\n",
      "Epoch: 13/20, Step 5960, Train loss: 0.157\n",
      "Epoch: 13/20, Step 6000, Train loss: 0.156\n",
      "Epoch: 13/20, Step 6040, Train loss: 0.154\n",
      "Epoch: 13/20, Step 6080, Train loss: 0.159\n",
      "Epoch: 13/20, Step 6120, Train loss: 0.158\n",
      "Epoch: 13/20, Step 6160, Train loss: 0.156\n",
      "Epoch: 13/20, Step 6200, Train loss: 0.157\n",
      "Epoch: 13/20, Step 6240, Train loss: 0.159\n",
      "Epoch 13/20, Train Loss: 0.157, Test Loss: 0.163\n",
      "Epoch: 14/20, Step 6280, Train loss: 0.157\n",
      "Epoch: 14/20, Step 6320, Train loss: 0.158\n",
      "Epoch: 14/20, Step 6360, Train loss: 0.159\n",
      "Epoch: 14/20, Step 6400, Train loss: 0.154\n",
      "Epoch: 14/20, Step 6440, Train loss: 0.157\n",
      "Epoch: 14/20, Step 6480, Train loss: 0.157\n",
      "Epoch: 14/20, Step 6520, Train loss: 0.155\n",
      "Epoch: 14/20, Step 6560, Train loss: 0.155\n",
      "Epoch: 14/20, Step 6600, Train loss: 0.159\n",
      "Epoch: 14/20, Step 6640, Train loss: 0.156\n",
      "Epoch: 14/20, Step 6680, Train loss: 0.157\n",
      "Epoch: 14/20, Step 6720, Train loss: 0.158\n",
      "Epoch 14/20, Train Loss: 0.157, Test Loss: 0.153\n",
      "Epoch: 15/20, Step 6760, Train loss: 0.153\n",
      "Epoch: 15/20, Step 6800, Train loss: 0.155\n",
      "Epoch: 15/20, Step 6840, Train loss: 0.156\n",
      "Epoch: 15/20, Step 6880, Train loss: 0.159\n",
      "Epoch: 15/20, Step 6920, Train loss: 0.157\n",
      "Epoch: 15/20, Step 6960, Train loss: 0.154\n",
      "Epoch: 15/20, Step 7000, Train loss: 0.160\n",
      "Epoch: 15/20, Step 7040, Train loss: 0.155\n",
      "Epoch: 15/20, Step 7080, Train loss: 0.156\n",
      "Epoch: 15/20, Step 7120, Train loss: 0.155\n",
      "Epoch: 15/20, Step 7160, Train loss: 0.158\n",
      "Epoch: 15/20, Step 7200, Train loss: 0.157\n",
      "Epoch 15/20, Train Loss: 0.156, Test Loss: 0.158\n",
      "Epoch: 16/20, Step 7240, Train loss: 0.158\n",
      "Epoch: 16/20, Step 7280, Train loss: 0.159\n",
      "Epoch: 16/20, Step 7320, Train loss: 0.155\n",
      "Epoch: 16/20, Step 7360, Train loss: 0.159\n",
      "Epoch: 16/20, Step 7400, Train loss: 0.156\n",
      "Epoch: 16/20, Step 7440, Train loss: 0.153\n",
      "Epoch: 16/20, Step 7480, Train loss: 0.160\n",
      "Epoch: 16/20, Step 7520, Train loss: 0.154\n",
      "Epoch: 16/20, Step 7560, Train loss: 0.155\n",
      "Epoch: 16/20, Step 7600, Train loss: 0.157\n",
      "Epoch: 16/20, Step 7640, Train loss: 0.157\n",
      "Epoch: 16/20, Step 7680, Train loss: 0.155\n",
      "Epoch 16/20, Train Loss: 0.157, Test Loss: 0.165\n",
      "Epoch: 17/20, Step 7720, Train loss: 0.157\n",
      "Epoch: 17/20, Step 7760, Train loss: 0.154\n",
      "Epoch: 17/20, Step 7800, Train loss: 0.158\n",
      "Epoch: 17/20, Step 7840, Train loss: 0.155\n",
      "Epoch: 17/20, Step 7880, Train loss: 0.158\n",
      "Epoch: 17/20, Step 7920, Train loss: 0.157\n",
      "Epoch: 17/20, Step 7960, Train loss: 0.156\n",
      "Epoch: 17/20, Step 8000, Train loss: 0.155\n",
      "Epoch: 17/20, Step 8040, Train loss: 0.156\n",
      "Epoch: 17/20, Step 8080, Train loss: 0.155\n",
      "Epoch: 17/20, Step 8120, Train loss: 0.159\n",
      "Epoch: 17/20, Step 8160, Train loss: 0.157\n",
      "Epoch 17/20, Train Loss: 0.156, Test Loss: 0.162\n",
      "Epoch: 18/20, Step 8200, Train loss: 0.158\n",
      "Epoch: 18/20, Step 8240, Train loss: 0.156\n",
      "Epoch: 18/20, Step 8280, Train loss: 0.157\n",
      "Epoch: 18/20, Step 8320, Train loss: 0.156\n",
      "Epoch: 18/20, Step 8360, Train loss: 0.156\n",
      "Epoch: 18/20, Step 8400, Train loss: 0.159\n",
      "Epoch: 18/20, Step 8440, Train loss: 0.156\n",
      "Epoch: 18/20, Step 8480, Train loss: 0.156\n",
      "Epoch: 18/20, Step 8520, Train loss: 0.160\n",
      "Epoch: 18/20, Step 8560, Train loss: 0.154\n",
      "Epoch: 18/20, Step 8600, Train loss: 0.155\n",
      "Epoch: 18/20, Step 8640, Train loss: 0.157\n",
      "Epoch 18/20, Train Loss: 0.157, Test Loss: 0.159\n",
      "Epoch: 19/20, Step 8680, Train loss: 0.157\n",
      "Epoch: 19/20, Step 8720, Train loss: 0.156\n",
      "Epoch: 19/20, Step 8760, Train loss: 0.157\n",
      "Epoch: 19/20, Step 8800, Train loss: 0.158\n",
      "Epoch: 19/20, Step 8840, Train loss: 0.156\n",
      "Epoch: 19/20, Step 8880, Train loss: 0.156\n",
      "Epoch: 19/20, Step 8920, Train loss: 0.156\n",
      "Epoch: 19/20, Step 8960, Train loss: 0.160\n",
      "Epoch: 19/20, Step 9000, Train loss: 0.154\n",
      "Epoch: 19/20, Step 9040, Train loss: 0.155\n",
      "Epoch: 19/20, Step 9080, Train loss: 0.157\n",
      "Epoch: 19/20, Step 9120, Train loss: 0.157\n",
      "Epoch 19/20, Train Loss: 0.157, Test Loss: 0.165\n",
      "Epoch: 20/20, Step 9160, Train loss: 0.157\n",
      "Epoch: 20/20, Step 9200, Train loss: 0.157\n",
      "Epoch: 20/20, Step 9240, Train loss: 0.156\n",
      "Epoch: 20/20, Step 9280, Train loss: 0.155\n",
      "Epoch: 20/20, Step 9320, Train loss: 0.157\n",
      "Epoch: 20/20, Step 9360, Train loss: 0.155\n",
      "Epoch: 20/20, Step 9400, Train loss: 0.156\n",
      "Epoch: 20/20, Step 9440, Train loss: 0.157\n",
      "Epoch: 20/20, Step 9480, Train loss: 0.158\n",
      "Epoch: 20/20, Step 9520, Train loss: 0.158\n",
      "Epoch: 20/20, Step 9560, Train loss: 0.154\n",
      "Epoch: 20/20, Step 9600, Train loss: 0.157\n",
      "Epoch 20/20, Train Loss: 0.156, Test Loss: 0.167\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, steps = train_model(model=model, goal_hidden_dim=2, \n",
    "                                         optimizer=optimizer, loss_func=criterion, epochs=epochs,\n",
    "                                         trainloader=trainloader, testloader=testloader, print_every=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [  -24.0415,  -187.4760, -1068.5631,  -485.7148,  -306.7005,  -495.7879,\n",
      "          -131.9182,  -141.3991,  -262.7227,   -36.3583,  -267.1220,   -44.7624,\n",
      "          -292.0692,  -429.5790,  -386.0551,   -21.4532]], device='cuda:0')\n",
      "tensor([   0.0000, -205.0350], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for par in model.encoder.parameters():\n",
    "    print(par.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for par in model.parameters():\n",
    "    print(par.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-5.3316e-02, -1.0766e-02,  5.3088e-02,  ...,  1.3072e-02,\n",
      "         -3.3415e-02,  1.7314e-03],\n",
      "        [-3.1922e-02, -5.3540e-02, -4.4421e-02,  ..., -2.9399e-02,\n",
      "          1.2919e-01, -1.8372e-01],\n",
      "        [-2.5331e-02, -2.2117e-03,  2.8156e-02,  ..., -1.4062e-02,\n",
      "         -5.1629e-01,  5.5479e-01],\n",
      "        ...,\n",
      "        [ 8.1333e-04,  1.4892e-02, -9.6123e-03,  ...,  6.7478e-03,\n",
      "         -6.7952e-02,  3.8890e-02],\n",
      "        [-3.9455e-02,  2.1686e-02,  2.1005e-02,  ..., -6.8418e-02,\n",
      "          7.6207e-02, -1.0561e-01],\n",
      "        [ 4.4607e-02, -4.9350e-05,  1.8621e-02,  ...,  3.4179e-02,\n",
      "          2.7628e-01, -1.7743e-01]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([-0.0449,  0.0387,  0.0321, -0.0169, -0.0530, -0.0135, -0.0645, -0.0486,\n",
      "        -0.0055, -0.0187, -0.0161,  0.0199, -0.0650,  0.0377, -0.0110, -0.0293,\n",
      "         0.0071,  0.0197,  0.0210, -0.0186,  0.0302,  0.0053,  0.0565,  0.0151,\n",
      "        -0.0592,  0.0636,  0.0283, -0.0166,  0.0552,  0.0393,  0.0472,  0.0148],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0.9255, 1.1339, 1.1661, 0.9822, 1.0650, 1.1261, 0.9199, 1.0202, 1.1206,\n",
      "        1.3499, 1.0549, 1.2116, 1.0588, 0.9229, 1.0459, 1.1447, 1.0651, 1.1229,\n",
      "        1.0030, 1.0985, 1.1674, 0.9700, 1.0435, 1.1248, 0.9295, 0.8036, 1.0676,\n",
      "        0.9994, 1.0411, 1.2076, 0.8786, 1.2391], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([ 0.1194,  0.0342,  0.1220, -0.0746, -0.2014, -0.0936, -0.4842, -0.2527,\n",
      "        -0.1969,  0.2408, -0.8335,  0.3093,  0.0542, -0.1451,  0.2967,  0.0198,\n",
      "        -0.1360,  0.0940,  0.2512,  0.0319, -0.0773, -0.2079,  0.0073,  0.1141,\n",
      "         0.2591,  0.3642,  0.1000, -0.1628, -0.1947, -0.1050, -0.0426, -0.1535],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-2.4935e-04,  2.1721e-01, -1.0655e-01,  7.8436e-02,  2.3440e-01,\n",
      "         -1.1778e-01, -1.5754e-01,  1.7949e-02, -2.4235e-01,  2.0308e-01,\n",
      "         -1.6315e-01,  4.0089e-02,  4.6741e-02,  1.0998e-01, -2.8373e-03,\n",
      "         -3.1243e-01, -2.7406e-01,  9.1786e-02,  2.4172e-01,  8.5719e-02,\n",
      "          4.0907e-02, -4.6019e-02, -1.1591e-01,  1.0374e-01,  4.9011e-02,\n",
      "          7.0191e-02,  2.5455e-01, -1.5456e-01,  1.9553e-01,  3.9559e-02,\n",
      "         -1.8055e-02,  1.0444e-01],\n",
      "        [ 1.7445e-01,  1.8935e-01, -2.6621e-01,  2.0111e-01,  7.0819e-02,\n",
      "          1.3813e-01,  2.6064e-01,  5.6732e-02, -2.1227e-01,  1.6336e-02,\n",
      "          6.0953e-01, -2.7473e-01,  1.6710e-01,  1.0321e-01,  3.4065e-02,\n",
      "         -2.5028e-01, -7.8082e-02,  4.6747e-02, -1.2759e-01,  6.0936e-02,\n",
      "          2.4740e-01,  1.1158e-01,  9.8412e-03, -2.2425e-01,  5.0939e-02,\n",
      "         -1.2067e-01,  2.7310e-01, -1.0503e-01,  1.5378e-01, -1.5535e-01,\n",
      "          1.3506e-01,  2.4570e-01],\n",
      "        [ 1.0583e-01,  2.1158e-01, -1.5672e-01,  1.7654e-01,  8.1840e-03,\n",
      "          3.4915e-02, -1.5131e-01, -5.3506e-02, -1.7759e-01, -3.6494e-01,\n",
      "          3.8399e-02,  3.1493e-02, -1.4019e-01, -9.3924e-02, -1.4009e-02,\n",
      "          9.8511e-02,  6.5019e-05,  2.5934e-01,  1.6633e-01, -1.5890e-01,\n",
      "          2.7400e-02, -1.8262e-01,  4.3729e-03, -4.5223e-02,  8.8625e-02,\n",
      "          8.3974e-02,  1.0006e-01, -2.6010e-01,  1.4235e-02,  1.2941e-01,\n",
      "          1.5397e-01, -2.1039e-02],\n",
      "        [-8.7221e-02, -2.5071e-01,  5.3118e-02, -6.1820e-02, -1.5617e-02,\n",
      "         -1.6403e-01, -9.1729e-02,  3.1504e-01, -6.7154e-03,  9.8994e-02,\n",
      "         -3.5080e-02,  2.2189e-02, -2.6688e-01,  1.7104e-01, -4.8634e-02,\n",
      "         -3.0646e-02,  1.0846e-02, -8.2305e-02,  2.5731e-02, -2.3666e-01,\n",
      "         -2.8398e-01, -6.3296e-02,  1.7178e-01,  2.9762e-01, -8.6655e-02,\n",
      "         -6.7688e-02, -1.3645e-01, -9.3907e-02, -8.9633e-02,  3.0884e-02,\n",
      "         -2.0643e-01, -7.0782e-02],\n",
      "        [ 9.1906e-02, -9.2881e-03, -4.1882e-01,  1.3346e-01,  2.7395e-01,\n",
      "          2.5414e-01,  3.7937e-01,  8.1809e-02, -5.9415e-02, -2.4448e-02,\n",
      "          6.6377e-01, -3.9801e-01,  1.7692e-01,  2.2586e-01,  3.3710e-02,\n",
      "         -2.7670e-01, -1.1523e-01, -3.0272e-02, -2.0692e-01,  1.9278e-01,\n",
      "          3.6392e-01,  2.7383e-02, -2.7453e-01, -2.1037e-01,  9.4512e-02,\n",
      "         -1.4036e-01,  2.3075e-01, -1.1666e-01,  3.6212e-01, -9.2610e-02,\n",
      "          1.7061e-01,  3.3936e-01],\n",
      "        [-1.4558e-01, -2.5320e-02,  2.5861e-01,  1.2579e-02,  9.9482e-02,\n",
      "         -3.1984e-01, -7.6133e-03,  4.9597e-02,  5.9582e-02,  7.8982e-02,\n",
      "          2.2038e-01, -1.4903e-01,  2.5490e-01, -1.6539e-01, -2.5766e-01,\n",
      "         -3.5084e-01,  5.0622e-02, -1.5084e-01, -2.4579e-01,  4.6122e-01,\n",
      "         -1.2887e-02,  9.6926e-02,  6.5601e-03, -7.2516e-02,  4.1698e-02,\n",
      "          1.0659e-01, -1.3896e-02, -4.8025e-02, -7.6979e-02, -2.7789e-01,\n",
      "         -4.1806e-02,  7.4500e-02],\n",
      "        [ 7.2248e-02, -1.1571e-01, -1.7654e-02,  1.7707e-01, -2.2264e-02,\n",
      "         -4.0597e-01, -1.2155e-01, -1.8081e-01, -2.0157e-02,  2.2547e-01,\n",
      "         -8.0459e-02, -5.1557e-02, -7.3920e-02, -4.8564e-02, -1.1267e-01,\n",
      "         -2.1290e-01, -9.9846e-02, -1.4572e-01, -2.2777e-01,  1.5284e-01,\n",
      "         -8.2880e-02,  4.1213e-02,  5.7649e-02,  1.5940e-01, -7.0857e-02,\n",
      "         -6.6441e-02, -1.0425e-01,  1.8079e-01, -1.5013e-01,  6.3641e-02,\n",
      "          7.8040e-03,  1.6753e-01],\n",
      "        [-1.1406e-01, -2.1602e-01, -9.4007e-03, -2.2628e-02, -1.9755e-01,\n",
      "          2.8389e-02, -2.0775e-01,  1.2935e-01, -2.7782e-01,  2.3515e-01,\n",
      "         -3.2800e-02, -2.6022e-01,  2.3867e-01, -6.2190e-02, -3.1907e-01,\n",
      "         -1.2063e-01, -2.2439e-02, -2.0928e-01,  1.4795e-01, -2.8319e-02,\n",
      "         -1.2151e-02,  1.6960e-01,  5.7107e-02, -2.0225e-01, -1.3912e-01,\n",
      "          3.5974e-02,  5.7679e-02,  6.4254e-02,  4.6457e-03, -2.5409e-01,\n",
      "         -4.8744e-02,  3.3078e-01],\n",
      "        [-7.5526e-02, -1.4804e-01, -1.5835e-01,  3.1689e-02, -1.5763e-01,\n",
      "          2.8615e-02, -7.3838e-02,  1.9766e-01,  6.3064e-02,  1.9464e-01,\n",
      "          6.3863e-02, -1.5139e-01,  1.0270e-01, -4.9217e-03, -1.6391e-01,\n",
      "          2.4372e-01, -2.5109e-01, -2.7392e-01, -2.2747e-01,  2.1538e-01,\n",
      "          2.2812e-01,  7.5025e-02, -1.7766e-01, -2.1627e-01,  2.2456e-01,\n",
      "          2.1811e-01,  1.1211e-01, -2.0773e-02, -2.0037e-01, -2.6625e-01,\n",
      "         -1.7813e-01,  5.6439e-02],\n",
      "        [-1.9671e-01, -2.2509e-01,  9.6749e-02,  1.4528e-01,  1.3460e-01,\n",
      "         -2.7353e-01,  8.2190e-02, -1.3727e-01,  1.1173e-01,  1.9527e-01,\n",
      "          1.7425e-01, -2.1620e-01, -4.5683e-02,  2.8726e-01, -1.5765e-01,\n",
      "         -2.2081e-01, -5.0238e-02,  9.4476e-02,  8.8575e-02, -2.9616e-01,\n",
      "         -4.0504e-02, -6.8074e-02,  1.6271e-01,  3.6206e-01, -1.5352e-01,\n",
      "         -2.5442e-01,  4.2703e-02,  1.5261e-01,  2.4717e-01,  3.0411e-01,\n",
      "          3.8367e-02,  1.2414e-01],\n",
      "        [ 1.6466e-01, -1.3650e-01,  4.1445e-02, -1.5500e-02, -4.1032e-02,\n",
      "         -1.3536e-02,  2.5115e-03, -6.9145e-02,  5.9180e-02, -8.0534e-03,\n",
      "         -9.3787e-02,  8.0326e-02,  1.3084e-02,  1.4685e-01,  1.2914e-01,\n",
      "         -1.1814e-01, -8.9957e-02,  1.2396e-01,  1.1740e-01,  5.5556e-02,\n",
      "         -7.9711e-02, -2.0660e-01, -1.0684e-01,  7.0398e-02,  2.5626e-01,\n",
      "         -1.6598e-01,  8.1520e-02, -1.3329e-01, -6.6052e-02,  2.5138e-02,\n",
      "          7.0012e-02, -1.0903e-01],\n",
      "        [ 8.2431e-02,  2.3085e-01, -7.8999e-02,  1.1476e-01,  1.6320e-01,\n",
      "          7.0012e-02,  1.8244e-02, -1.6011e-01,  2.2607e-01, -3.4137e-01,\n",
      "          4.5256e-01,  8.1028e-02, -7.5811e-02,  1.8301e-01, -2.8549e-02,\n",
      "         -3.9282e-04,  1.5010e-01,  2.1036e-01,  1.1036e-01, -7.7147e-02,\n",
      "          3.8808e-02, -2.1079e-01, -1.9832e-01,  5.6733e-02,  3.1788e-02,\n",
      "         -2.5082e-01,  2.1164e-02, -2.5790e-01,  1.1522e-01,  2.2042e-01,\n",
      "          1.3758e-01, -6.9613e-02],\n",
      "        [ 2.1788e-01,  1.7944e-01, -4.1041e-01,  1.9037e-01,  2.2000e-01,\n",
      "          2.6840e-01,  4.0695e-01,  3.9132e-02, -2.0533e-01, -7.9384e-02,\n",
      "          6.1684e-01, -3.8662e-01, -3.1660e-03,  1.2157e-01, -1.1041e-01,\n",
      "         -2.6643e-01, -6.8406e-02, -1.3955e-01, -2.9979e-02,  3.5274e-01,\n",
      "          3.7078e-01, -1.3227e-02, -1.4248e-01, -2.5351e-01,  1.1332e-01,\n",
      "         -1.2249e-01,  2.6414e-01, -1.6005e-01,  2.5999e-01, -5.8775e-02,\n",
      "         -9.7938e-02,  2.2803e-01],\n",
      "        [ 1.4472e-01,  2.5103e-01,  4.8268e-02,  3.2821e-02, -1.8434e-02,\n",
      "          2.4953e-01, -4.8994e-02, -2.0069e-01, -7.0681e-02, -2.1707e-01,\n",
      "         -1.2627e-01,  2.2453e-03, -1.4382e-02,  1.0554e-01,  6.7929e-02,\n",
      "         -4.8763e-02, -8.9766e-03, -5.5431e-04,  2.0070e-01,  1.1013e-01,\n",
      "          1.6229e-01, -1.4341e-01, -5.0550e-02,  3.7514e-02,  9.4120e-02,\n",
      "         -8.5620e-02,  7.4240e-02, -9.5309e-02,  8.6797e-02,  1.5631e-02,\n",
      "         -8.3596e-02,  3.9841e-02],\n",
      "        [ 1.0233e-01,  2.1930e-01, -1.7561e-01,  6.7342e-02,  2.0335e-01,\n",
      "          7.2922e-02,  1.1266e-03, -2.2959e-01,  3.6239e-02, -2.4944e-01,\n",
      "         -1.2740e-01,  7.5138e-02,  7.1718e-02, -8.3118e-02,  1.0748e-01,\n",
      "          1.4372e-01,  1.3334e-01, -1.0653e-02,  9.3840e-02,  7.4277e-02,\n",
      "          2.1797e-02, -2.1037e-02, -1.4305e-01,  3.0588e-02,  8.6384e-02,\n",
      "         -5.7615e-02, -5.6158e-03, -1.4288e-01,  3.9324e-02,  7.3914e-02,\n",
      "          5.4039e-02, -1.4815e-01],\n",
      "        [ 2.1022e-01,  1.3536e-01, -3.7482e-01,  1.0161e-01,  2.8595e-01,\n",
      "          7.1713e-02,  4.9543e-01,  1.4755e-01, -2.7701e-01, -9.2742e-02,\n",
      "          5.7176e-01, -4.7135e-01,  2.0057e-01,  6.9103e-02, -1.2338e-01,\n",
      "         -1.3672e-01, -3.0673e-01,  9.9932e-02, -1.2763e-01,  2.4577e-01,\n",
      "          2.9846e-01, -7.0305e-02, -2.6311e-01, -1.2946e-01,  1.1865e-02,\n",
      "         -3.6935e-02,  2.8746e-01, -5.7747e-02,  3.3132e-01, -4.4144e-02,\n",
      "          5.1390e-03,  3.1430e-01]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([ 0.3302, -0.1443,  0.5372, -0.0467,  0.0023, -0.0265,  0.1552,  0.2230,\n",
      "         0.1404, -0.2120,  0.4066, -0.5298, -0.0958,  0.3274,  0.4701, -0.0463],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.2902,  0.3294,  0.1854,  0.0844,  0.3641,  0.1927,  0.1490, -0.3020,\n",
      "          0.1690,  0.2044,  0.2130, -0.3089,  0.2478,  0.1056,  0.1276,  0.3823],\n",
      "        [ 0.0274, -0.0680,  0.1394, -0.1795,  0.1420,  0.1492,  0.2450, -0.1415,\n",
      "         -0.1316,  0.1458,  0.2352, -0.1108, -0.0969,  0.0071, -0.1633,  0.1858],\n",
      "        [ 0.0320, -0.0628,  0.0280,  0.2343, -0.0233,  0.1989,  0.0969,  0.1613,\n",
      "          0.0356, -0.1519,  0.0340, -0.0801,  0.0858, -0.1916, -0.0939,  0.2407],\n",
      "        [ 0.1620, -0.0146, -0.1775, -0.1767,  0.0582,  0.0272,  0.0427, -0.1471,\n",
      "         -0.0829, -0.1283,  0.1622, -0.1791, -0.2036, -0.1850, -0.2243,  0.0747],\n",
      "        [-0.0635, -0.2042, -0.1429,  0.1843,  0.1712,  0.1971,  0.2315,  0.1487,\n",
      "         -0.0283, -0.1112, -0.1946, -0.2291, -0.0841, -0.0589,  0.1601, -0.0660]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0963, -0.2329, -0.0179, -0.2149,  0.0421], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for par in model.encoder.parameters():\n",
    "    print(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCAAutoencoder(\n",
       "  (encoder): ModuleList(\n",
       "    (0): Linear(in_features=227, out_features=32, bias=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): Linear(in_features=1, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=227, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSANADANSYS\\AppData\\Local\\Temp\\ipykernel_21564\\2117146445.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = torch.load(\"PCAAE_hidden_dim1.pt\")\n"
     ]
    }
   ],
   "source": [
    "a = torch.load(\"PCAAE_hidden_dim1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSANADANSYS\\AppData\\Local\\Temp\\ipykernel_21564\\152823176.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  b = torch.load(\"PCAAE_hidden_dim2.pt\")\n"
     ]
    }
   ],
   "source": [
    "b = torch.load(\"PCAAE_hidden_dim2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.0.weight',\n",
       "              tensor([[-0.1396,  0.0467, -0.0312,  ...,  0.0007, -0.2838,  0.3086],\n",
       "                      [ 0.0017, -0.0041, -0.0094,  ...,  0.0116,  0.1530, -0.0595],\n",
       "                      [ 0.0976,  0.0505,  0.0010,  ...,  0.1117, -0.0056, -0.0236],\n",
       "                      ...,\n",
       "                      [ 0.0509,  0.0253, -0.0583,  ...,  0.0244,  0.2125, -0.2639],\n",
       "                      [ 0.0624,  0.0081, -0.0249,  ...,  0.0044,  0.0583, -0.0241],\n",
       "                      [ 0.0240, -0.0344, -0.0038,  ...,  0.0660, -0.3947,  0.2876]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.0.bias',\n",
       "              tensor([-0.0289,  0.0653, -0.0274,  0.0476, -0.0014,  0.0434,  0.0270,  0.0249,\n",
       "                      -0.0039,  0.0494,  0.0095,  0.0098, -0.0311,  0.0158,  0.0211, -0.0311,\n",
       "                      -0.0307, -0.0588, -0.0579, -0.0377, -0.0281,  0.0350,  0.0059,  0.0590,\n",
       "                       0.0149,  0.0271,  0.0285, -0.0099, -0.0548,  0.0600,  0.0193,  0.0628],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.1.weight',\n",
       "              tensor([1.0626, 1.2037, 0.9204, 1.0411, 1.4316, 1.2731, 1.2978, 0.9720, 1.1429,\n",
       "                      1.1054, 1.3724, 1.5383, 1.0619, 1.0928, 0.8090, 1.2034, 1.0598, 1.3547,\n",
       "                      1.1372, 0.9635, 1.0841, 0.9085, 0.9770, 0.8138, 0.9653, 1.0998, 1.2223,\n",
       "                      0.8843, 1.0040, 1.0144, 1.0770, 1.1458], device='cuda:0')),\n",
       "             ('encoder.1.bias',\n",
       "              tensor([ 5.7665e-03,  1.3979e-01,  1.6285e-01,  9.2658e-02,  3.3826e-01,\n",
       "                       8.0893e-02,  2.7830e-01, -1.1338e-01,  7.5435e-02,  5.3593e-02,\n",
       "                       6.9571e-02,  9.3067e-03,  4.9135e-02,  3.9190e-01,  4.9306e-04,\n",
       "                       3.4275e-01,  6.1127e-02,  4.8600e-01, -5.9802e-01, -4.4879e-02,\n",
       "                       3.5488e-02, -4.0731e-01, -9.1112e-02, -3.3908e-01,  2.7485e-01,\n",
       "                       8.4087e-02,  1.5644e-01, -1.3318e-03, -5.5935e-02, -1.8767e-01,\n",
       "                       2.9655e-02,  3.7130e-02], device='cuda:0')),\n",
       "             ('encoder.1.running_mean',\n",
       "              tensor([ 0.1040,  0.0581, -0.1072,  0.4035, -0.3518,  0.1920, -0.1999, -0.2307,\n",
       "                      -0.0708,  0.1873,  0.0056,  0.1341,  0.3277, -0.1127, -0.4573, -0.2778,\n",
       "                       0.3541, -0.3330,  0.2724, -0.2133, -0.0582,  0.0937, -0.3409, -0.2355,\n",
       "                       0.1526,  0.1545,  0.0994, -0.4478,  0.3433, -0.2055, -0.1170, -0.2924],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.1.running_var',\n",
       "              tensor([0.1638, 0.0817, 0.0661, 0.1409, 0.1148, 0.2500, 0.1043, 0.0916, 0.1297,\n",
       "                      0.1155, 0.1826, 0.1309, 0.1338, 0.1718, 0.1485, 0.0993, 0.1240, 0.4682,\n",
       "                      0.2281, 0.1416, 0.0879, 0.1157, 0.1626, 0.1436, 0.1105, 0.1324, 0.1403,\n",
       "                      0.1850, 0.1478, 0.1863, 0.0893, 0.1400], device='cuda:0')),\n",
       "             ('encoder.1.num_batches_tracked', tensor(9800, device='cuda:0')),\n",
       "             ('encoder.2.weight',\n",
       "              tensor([[-8.8927e-02, -1.2431e-01,  2.7515e-02,  1.5683e-01, -1.0409e-01,\n",
       "                        7.5310e-02,  6.9601e-02,  8.8314e-02,  4.5385e-02, -1.5436e-01,\n",
       "                       -5.4526e-02,  1.5425e-01, -4.0708e-02, -2.8177e-02, -4.6669e-02,\n",
       "                        1.4565e-02, -7.9849e-02,  2.3996e-01,  8.9691e-02,  3.9420e-02,\n",
       "                       -1.0320e-01, -1.2972e-01,  4.8631e-02, -3.3028e-02, -1.2960e-01,\n",
       "                       -7.9077e-03,  9.3299e-02,  7.6786e-02,  4.0696e-02,  1.6242e-01,\n",
       "                       -2.0453e-01, -7.6377e-02],\n",
       "                      [-2.4128e-02,  2.7576e-01,  2.2893e-01, -1.7600e-01, -2.8435e-02,\n",
       "                       -1.5553e-03,  1.9705e-01, -1.5626e-01, -2.0973e-01, -2.9483e-02,\n",
       "                       -4.7123e-03, -1.3544e-02, -3.0205e-04,  1.1225e-01, -6.4975e-02,\n",
       "                        7.3269e-02,  9.1603e-02,  3.2646e-02, -1.6643e-01,  5.1918e-02,\n",
       "                       -5.1887e-02, -8.3088e-02,  1.5850e-01, -2.2873e-01,  1.6319e-01,\n",
       "                        1.2023e-01,  2.4621e-01,  1.4839e-01, -1.2775e-01,  1.0128e-01,\n",
       "                       -6.5777e-02, -9.5819e-03],\n",
       "                      [-3.2316e-01,  3.5127e-01, -2.2701e-01,  9.6158e-03, -4.4909e-01,\n",
       "                        3.9497e-01,  5.2396e-01,  1.6098e-02,  5.0392e-01,  3.0326e-01,\n",
       "                       -4.6966e-01,  4.6463e-01,  4.2087e-01,  3.5477e-01,  2.2831e-02,\n",
       "                       -6.6963e-03, -1.2203e-01, -4.3740e-01,  4.6359e-01, -2.7940e-01,\n",
       "                        3.6043e-01,  3.3244e-01,  1.2004e-01, -2.2255e-01,  2.1979e-01,\n",
       "                        3.1330e-01,  4.1441e-01, -1.0245e-01,  4.0614e-01, -1.4853e-01,\n",
       "                        3.7898e-01, -9.7099e-02],\n",
       "                      [-7.2979e-02,  2.2370e-01, -1.8206e-01, -2.4353e-02, -2.9756e-01,\n",
       "                        1.1137e-01,  2.6352e-01, -2.6120e-02,  1.1357e-01,  6.2308e-02,\n",
       "                       -2.4780e-01,  3.0581e-01, -1.0069e-01,  2.7326e-01, -6.8923e-03,\n",
       "                        6.8330e-02, -1.3769e-01, -2.1035e-01,  3.0569e-01, -9.9032e-02,\n",
       "                        2.3316e-01,  1.9300e-01, -6.6174e-02, -5.3439e-02,  3.6518e-02,\n",
       "                        2.8159e-01,  6.2721e-02,  2.9772e-02,  1.9629e-01, -1.0185e-01,\n",
       "                        1.0734e-01, -1.5194e-01],\n",
       "                      [-2.4895e-01,  2.7395e-02,  7.9423e-02, -1.3841e-01, -4.3761e-01,\n",
       "                        3.1482e-01, -2.0044e-02,  2.5828e-01,  1.1321e-01,  2.1395e-01,\n",
       "                       -3.7983e-01,  3.7388e-01,  5.9949e-02, -1.1724e-01,  1.0365e-01,\n",
       "                       -5.6051e-02, -3.6499e-01, -5.5869e-01,  4.9618e-01,  1.0198e-01,\n",
       "                        1.5674e-01, -6.4057e-02,  2.3684e-01, -1.9839e-02, -7.2154e-02,\n",
       "                       -2.1227e-01,  1.8598e-01,  9.1913e-02,  9.7347e-02,  2.6274e-01,\n",
       "                        1.9070e-01, -3.8913e-01],\n",
       "                      [-5.8527e-03,  1.1020e-01,  1.4702e-02,  4.2336e-02,  5.8370e-02,\n",
       "                        1.1466e-01,  1.7731e-01,  6.6129e-02,  1.4087e-01,  1.9098e-01,\n",
       "                       -1.4586e-01,  1.3243e-01,  6.6717e-02,  4.1631e-01, -1.6675e-02,\n",
       "                        9.0460e-02, -1.6382e-01,  8.7826e-02, -1.7986e-01, -6.2815e-02,\n",
       "                        1.4662e-01, -2.3302e-01, -1.2021e-01, -9.1921e-02,  1.1167e-01,\n",
       "                        6.1836e-02,  1.3800e-01, -3.5418e-03, -1.0547e-02, -1.3817e-01,\n",
       "                        1.6480e-01, -5.5434e-02],\n",
       "                      [-1.5742e-01, -1.8856e-01, -4.3305e-02,  9.7283e-03, -1.2246e-01,\n",
       "                        2.9118e-01, -2.0560e-01,  2.1766e-02,  1.2386e-01, -1.0502e-01,\n",
       "                       -6.9740e-02,  3.8420e-02,  1.0565e-01, -3.5674e-02,  1.8798e-01,\n",
       "                        1.6125e-02, -3.8172e-02, -3.7261e-01,  1.8998e-01, -3.5593e-02,\n",
       "                       -2.3200e-01,  1.3111e-01,  1.9368e-01, -2.7550e-01,  7.2626e-02,\n",
       "                        4.8978e-02, -1.1274e-01, -6.1627e-03,  1.0773e-01,  1.4484e-01,\n",
       "                       -1.3645e-01, -6.4111e-02],\n",
       "                      [ 3.5038e-02,  7.3537e-02, -9.3378e-02,  2.0526e-02,  1.1377e-02,\n",
       "                        2.6735e-02, -2.2548e-01, -1.2875e-02,  1.3657e-01,  4.8695e-02,\n",
       "                        1.1046e-01, -4.7034e-02,  1.0460e-01,  1.3818e-01,  5.8274e-02,\n",
       "                        1.7901e-02,  1.1846e-01, -7.5435e-02,  9.4030e-02, -1.7046e-01,\n",
       "                       -1.5997e-01,  1.1470e-01, -2.0115e-01, -1.6867e-01,  1.4435e-01,\n",
       "                        1.4653e-01,  8.0173e-02, -1.4283e-01,  1.4661e-01, -8.5264e-02,\n",
       "                       -2.8535e-01, -4.8354e-02],\n",
       "                      [ 1.5388e-01,  1.6467e-01, -9.5231e-02, -6.1568e-02,  9.2000e-02,\n",
       "                       -2.9310e-02,  9.8248e-02, -4.4077e-02,  1.1665e-01,  2.2777e-01,\n",
       "                       -3.4037e-02, -4.1058e-02,  1.7562e-01,  1.2179e-01, -2.3777e-03,\n",
       "                        1.5003e-01,  3.4075e-02, -1.2382e-01, -4.4499e-02, -2.4822e-01,\n",
       "                        1.1691e-01,  1.3547e-01, -1.7916e-01, -3.5433e-02,  2.1021e-01,\n",
       "                        2.2148e-01, -9.9672e-02, -4.7074e-02,  6.9263e-02, -6.6885e-02,\n",
       "                        4.4622e-02, -1.8343e-02],\n",
       "                      [-8.2406e-02,  1.9809e-02,  9.3970e-02,  2.2631e-01, -6.7449e-02,\n",
       "                        3.8773e-02,  4.8623e-02,  1.5650e-01, -5.0516e-02, -1.1726e-01,\n",
       "                        5.9093e-02,  2.6334e-01, -8.2158e-02, -1.6451e-02,  4.5297e-02,\n",
       "                       -1.4504e-01, -1.1223e-01,  9.8103e-02,  1.4770e-01,  1.7448e-01,\n",
       "                       -2.0850e-01, -6.1633e-02,  1.8413e-01,  9.9880e-04,  4.6305e-02,\n",
       "                       -2.2574e-01,  1.6685e-01,  3.2701e-02, -7.4830e-02,  1.3668e-01,\n",
       "                       -1.4548e-01, -1.2185e-01],\n",
       "                      [-2.0122e-01,  6.6695e-02, -1.0473e-01, -1.4337e-01, -3.7331e-01,\n",
       "                        3.1114e-01, -1.2967e-01,  2.2368e-01, -1.6883e-02,  1.4775e-01,\n",
       "                       -3.8305e-01,  1.4533e-01,  1.3530e-01, -4.2680e-02,  2.0376e-01,\n",
       "                       -1.0899e-01, -2.7915e-01, -4.1902e-01,  5.3484e-01, -3.5783e-02,\n",
       "                        9.4345e-02,  3.1231e-02,  1.7433e-01, -3.8550e-02, -1.6528e-01,\n",
       "                       -1.0870e-01,  2.5413e-01,  1.5222e-01,  4.7121e-02,  2.1335e-01,\n",
       "                        4.7492e-02, -1.7181e-01],\n",
       "                      [-3.0094e-02,  6.5570e-02,  1.8987e-01,  8.6820e-02, -7.2166e-02,\n",
       "                       -3.4338e-02,  1.9931e-01, -7.9412e-02,  9.8595e-02, -6.3496e-02,\n",
       "                        1.5824e-01,  7.2487e-03, -1.6381e-01, -1.6211e-01, -6.4291e-02,\n",
       "                        3.9262e-02,  7.7681e-02,  1.0542e-01, -1.6917e-01,  7.8353e-02,\n",
       "                        1.4323e-03, -1.1845e-01,  3.8737e-02, -1.5383e-02, -2.3431e-02,\n",
       "                       -2.5958e-01,  1.9886e-01,  1.0580e-01,  4.4184e-02,  1.0830e-01,\n",
       "                       -1.1241e-01,  1.9680e-02],\n",
       "                      [-9.6539e-02,  2.8888e-02, -1.5568e-01,  2.6025e-01, -3.3818e-02,\n",
       "                        1.0797e-01,  6.7323e-02,  1.9329e-01,  5.9426e-02,  1.2057e-01,\n",
       "                       -3.0234e-01,  3.6695e-01,  1.9020e-01,  1.9547e-01, -2.2137e-01,\n",
       "                       -1.7042e-02, -2.2468e-01,  2.6361e-01, -1.4995e-02,  2.8863e-02,\n",
       "                        1.1246e-01, -1.7561e-01, -1.6541e-01,  8.1955e-02,  1.0903e-01,\n",
       "                       -7.3937e-02,  2.7347e-01, -1.2407e-01,  5.8340e-02, -1.0089e-01,\n",
       "                        2.0591e-01,  8.3456e-02],\n",
       "                      [-1.8982e-01,  2.4618e-01,  1.0883e-01, -2.7209e-01, -1.5482e-01,\n",
       "                        2.6327e-01,  3.7109e-01, -1.8373e-01,  5.0681e-02,  1.1653e-01,\n",
       "                       -9.4930e-03, -1.1480e-02,  2.1743e-01,  1.2140e-01, -6.6392e-02,\n",
       "                        1.4977e-01,  2.2393e-01, -2.0652e-01, -1.8316e-01, -3.2304e-02,\n",
       "                        1.0248e-01,  6.1889e-02,  5.3504e-02, -1.2223e-01,  4.0113e-01,\n",
       "                        6.8572e-02, -3.1241e-02,  9.7589e-02, -2.7913e-02,  2.3025e-01,\n",
       "                        2.6910e-02, -1.2969e-01],\n",
       "                      [-5.5165e-02,  1.1971e-01,  8.0840e-02,  3.3391e-01, -4.7330e-01,\n",
       "                        8.2346e-02, -1.6671e-01, -2.1084e-01, -1.1052e-01,  2.0716e-01,\n",
       "                       -6.4010e-02,  4.1427e-01,  1.6361e-01, -6.5368e-02, -2.1427e-01,\n",
       "                       -6.0496e-01,  2.5912e-01, -2.4257e-01,  3.2786e-01, -1.3908e-01,\n",
       "                        1.8933e-01,  2.4251e-02, -8.9898e-02, -6.4283e-02,  3.7862e-02,\n",
       "                        2.0094e-01,  2.3457e-01, -1.8488e-01,  3.1840e-01,  1.2874e-02,\n",
       "                        3.3199e-02, -2.4417e-01],\n",
       "                      [-2.8410e-02, -3.7985e-01, -2.5680e-01,  4.9945e-02,  1.4781e-02,\n",
       "                       -1.6779e-01, -3.5330e-01,  2.9295e-01, -1.0728e-01, -1.5008e-01,\n",
       "                       -1.3511e-01, -9.2882e-02, -1.0356e-01, -4.7628e-02,  8.5674e-02,\n",
       "                       -2.9210e-02,  4.4653e-02,  1.4019e-01,  2.2886e-01,  1.5986e-01,\n",
       "                        4.9433e-02,  1.6913e-01,  1.8519e-02,  1.7504e-01, -2.1971e-01,\n",
       "                       -6.1355e-02,  3.9847e-02, -2.5246e-01, -3.6820e-03, -2.0411e-01,\n",
       "                        8.7835e-02,  2.3736e-01]], device='cuda:0')),\n",
       "             ('encoder.2.bias',\n",
       "              tensor([ 0.0256,  0.2405,  0.0340,  0.1483, -0.4355,  0.6744,  0.1338,  0.0800,\n",
       "                      -0.1821,  0.1681, -0.1440,  0.2882, -0.0880,  0.1412, -0.1981, -0.1074],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.4.weight',\n",
       "              tensor([[-0.1313, -0.1296,  0.3713,  0.1293, -0.4086,  0.3379, -0.0946,  0.0728,\n",
       "                       -0.1704, -0.2329, -0.2439, -0.0671, -0.1989,  0.1137,  0.4443,  0.0750]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.4.bias', tensor([0.1411], device='cuda:0')),\n",
       "             ('decoder.0.weight',\n",
       "              tensor([[-0.6455],\n",
       "                      [ 0.1003],\n",
       "                      [-1.0186],\n",
       "                      [-0.3252],\n",
       "                      [-0.6542],\n",
       "                      [-1.5880],\n",
       "                      [ 0.0744],\n",
       "                      [ 0.6300],\n",
       "                      [-0.7761],\n",
       "                      [ 0.6265],\n",
       "                      [-0.0470],\n",
       "                      [-0.8517],\n",
       "                      [ 0.4910],\n",
       "                      [-0.6285],\n",
       "                      [ 0.1750],\n",
       "                      [ 0.6835]], device='cuda:0')),\n",
       "             ('decoder.0.bias',\n",
       "              tensor([-0.9990, -0.9761, -0.0871, -0.8815,  0.6756,  0.6615,  0.7377,  0.2528,\n",
       "                      -0.6931,  0.6946,  0.7773,  0.8097,  0.5125,  0.9604,  0.4772,  0.3078],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.2.weight',\n",
       "              tensor([[-2.8596e-01,  6.0154e-02, -5.0488e-04, -1.4459e-01, -1.2450e-01,\n",
       "                        3.3587e-01, -1.5456e-01, -1.3001e-01, -3.8186e-01,  1.3037e-01,\n",
       "                        1.4161e-01, -1.1656e-02, -1.5958e-01, -1.0577e-01,  1.5949e-01,\n",
       "                        1.5150e-01],\n",
       "                      [-3.6003e-01, -6.9639e-02,  1.9231e-01, -1.8154e-01,  1.0237e-01,\n",
       "                       -3.0445e-02,  1.7774e-01, -6.5841e-02, -1.9337e-01, -2.1633e-01,\n",
       "                        3.6708e-02,  1.1102e-01,  2.0858e-01, -2.7305e-02,  1.0551e-01,\n",
       "                       -1.0304e-03],\n",
       "                      [ 5.9300e-02,  1.6292e-01, -3.7400e-01,  9.4774e-02,  2.7038e-01,\n",
       "                       -3.0849e-01, -1.9211e-02, -1.8770e-01, -1.5577e-01,  1.8195e-01,\n",
       "                       -9.4999e-02,  3.7196e-01,  1.3599e-01,  1.1845e-01, -1.5396e-01,\n",
       "                        1.2577e-01],\n",
       "                      [ 1.3715e-01, -9.9159e-02,  1.2735e-01,  2.8456e-01,  1.8303e-01,\n",
       "                       -3.6725e-01,  2.3678e-01, -1.5829e-01,  1.4087e-02,  2.1922e-02,\n",
       "                       -5.9603e-02,  8.6699e-02,  2.0485e-01,  2.4731e-01,  4.8238e-02,\n",
       "                       -1.6513e-01],\n",
       "                      [-3.6933e-01,  2.4550e-01,  3.1613e-01, -4.1980e-01,  8.0484e-02,\n",
       "                       -3.0473e-02,  5.9233e-02, -1.6962e+00, -6.2192e-01,  6.6198e-02,\n",
       "                        3.8119e-02,  2.5899e-01, -2.5669e-01,  5.1851e-02,  2.4361e-01,\n",
       "                       -1.6497e+00],\n",
       "                      [ 8.1363e-02,  2.2961e-01, -1.4610e-01,  1.8448e-03,  1.5813e-02,\n",
       "                       -1.8452e-01, -3.4330e-02, -1.2445e-01, -1.8389e-01, -2.2734e-01,\n",
       "                       -2.0104e-01,  1.1093e-01,  1.1433e-01,  2.0690e-01,  2.2632e-01,\n",
       "                       -8.3438e-02],\n",
       "                      [ 2.7158e-01,  2.0419e-01, -1.5510e-01, -8.6070e-01, -1.2575e-01,\n",
       "                        5.4898e-02,  4.8313e-02, -1.6784e-01,  4.4828e-01,  1.2948e-02,\n",
       "                       -1.8094e-01,  1.7265e-01, -4.0742e-02,  5.3103e-03, -2.3029e-01,\n",
       "                       -2.4063e-01],\n",
       "                      [ 1.4652e-01,  1.9044e-02,  1.6443e-01, -1.5586e-01,  1.9784e-01,\n",
       "                       -1.2059e-01,  1.2253e-01, -5.4456e-02, -1.5139e-01, -1.5194e-01,\n",
       "                       -1.6461e-01, -6.7441e-03,  9.0462e-02, -9.7971e-02,  5.2748e-03,\n",
       "                       -8.8898e-03],\n",
       "                      [-1.1357e-01, -4.4364e-02,  2.1991e-01,  1.3412e-01,  3.9262e-03,\n",
       "                        2.2352e-01, -4.2060e-02,  1.0057e-01, -2.5260e-01,  4.3262e-02,\n",
       "                        1.5790e-01, -1.9367e-01, -3.3305e-02, -2.5108e-01,  5.9209e-02,\n",
       "                       -8.8395e-02],\n",
       "                      [-8.3900e-02,  6.3239e-02, -8.3730e-02,  8.2193e-02, -8.7014e-02,\n",
       "                        1.8467e-01,  1.1806e-01, -1.3908e-01,  7.9331e-02, -1.1268e-01,\n",
       "                       -2.3572e-01, -9.5029e-02,  7.4850e-02, -1.1032e-01, -6.4501e-03,\n",
       "                        1.2642e-01],\n",
       "                      [ 8.1992e-02, -1.2756e-01,  7.3646e-02,  8.0680e-02, -2.6570e-01,\n",
       "                        7.0909e-02, -7.9586e-02, -1.0507e-01,  1.0562e-01,  9.3488e-02,\n",
       "                       -1.2401e-01, -1.6591e-01,  5.0814e-02, -7.6191e-03, -1.1249e-01,\n",
       "                       -2.9671e-03],\n",
       "                      [-2.8556e-01,  6.0010e-02, -2.2104e-01, -3.8938e-01, -6.8118e-02,\n",
       "                        2.6099e-01, -5.4695e-02, -3.8569e-02, -1.6236e-01,  9.0558e-02,\n",
       "                        1.3728e-01,  3.4683e-02,  1.0359e-01,  6.5483e-02,  7.4120e-02,\n",
       "                        5.2788e-02],\n",
       "                      [-4.3577e-02, -6.0958e-02, -8.2066e-01,  2.2912e-01,  1.3300e-01,\n",
       "                       -1.9496e-01, -2.1552e-01, -1.7851e-01, -1.3648e-02, -1.0729e-02,\n",
       "                        1.9971e-01,  2.6329e-01,  6.9341e-02, -7.3673e-03,  1.3973e-01,\n",
       "                        2.6660e-01],\n",
       "                      [ 1.3508e-01, -1.8416e-01, -8.5376e-02, -6.2345e-01,  3.2856e-01,\n",
       "                       -3.7176e-01,  1.4375e-02, -2.4495e-01,  6.4048e-02,  8.0390e-02,\n",
       "                        2.8471e-01,  1.1145e-01, -1.8178e-01,  3.6355e-01,  5.5834e-02,\n",
       "                       -8.4218e-02],\n",
       "                      [ 7.3002e-02,  5.7134e-02,  2.0654e-01,  2.3132e-01,  1.8457e-01,\n",
       "                       -1.8692e-01,  1.0356e-01, -3.8576e-02, -3.6206e-03, -5.6364e-02,\n",
       "                       -9.0196e-03, -1.1732e-01,  2.6796e-02,  7.9438e-02,  9.3272e-02,\n",
       "                        2.1013e-02],\n",
       "                      [-2.1448e-01, -1.0694e-01, -3.5488e-02, -1.5846e-01,  1.1113e-02,\n",
       "                       -1.3510e+00, -7.7044e-03,  4.3579e-02,  2.1309e-01, -1.7839e-01,\n",
       "                       -1.5632e-01, -9.3639e-02, -9.5849e-02,  5.7534e-01,  2.0351e-01,\n",
       "                        1.8790e-01],\n",
       "                      [ 1.8665e-02,  4.4170e-02, -2.0904e-01, -6.6766e-02, -2.1272e-01,\n",
       "                       -5.0280e-02,  1.1710e-01,  1.2096e-01, -2.4448e-01, -2.3508e-01,\n",
       "                       -9.9658e-02,  2.2714e-01, -1.5733e-01, -1.0519e-01, -9.6394e-02,\n",
       "                       -2.1567e-01],\n",
       "                      [-4.7299e-03, -2.4130e-01,  6.3817e-01,  1.2764e-01,  3.6913e-01,\n",
       "                       -7.9620e-01,  1.2038e-01,  1.1259e-01, -8.9134e-01, -4.0316e-02,\n",
       "                        1.7998e-02,  3.3967e-01,  8.8784e-02, -2.0030e-02, -1.5756e-01,\n",
       "                       -2.8540e-01],\n",
       "                      [-1.2324e-01,  1.3339e-02,  1.3385e-01,  8.7304e-02,  1.7425e-01,\n",
       "                        6.2827e-02, -4.6117e-02, -4.5359e-02, -3.4738e-01,  1.3834e-01,\n",
       "                        2.6708e-02, -8.0324e-02,  8.4446e-02,  4.6176e-02,  7.0625e-02,\n",
       "                        1.3299e-01],\n",
       "                      [-5.7855e-02, -1.6475e-01, -1.1572e-01, -1.6399e-01, -4.4849e-01,\n",
       "                        4.1579e-01,  9.2445e-02, -3.6925e-01,  2.8373e-04,  1.0946e-01,\n",
       "                        4.1754e-01, -3.8201e-01, -8.6845e-03, -8.7039e-02,  7.5879e-02,\n",
       "                       -9.5031e-02],\n",
       "                      [-2.8132e-02,  7.5065e-03, -8.9311e-02, -4.0581e-01,  2.2529e-01,\n",
       "                        2.1897e-02, -1.5394e-01, -1.6007e-01, -3.2940e-01, -7.8546e-01,\n",
       "                       -6.0862e-02,  1.3470e-01, -1.0271e+00,  2.0314e-01, -4.5882e-01,\n",
       "                        9.4290e-02],\n",
       "                      [-1.9775e-01, -4.0532e-02,  8.4488e-02, -7.3583e-02, -2.2471e-01,\n",
       "                       -1.1811e-01,  5.7974e-02, -9.2405e-02, -2.4680e-01, -2.2253e-01,\n",
       "                       -2.0244e-01, -9.7329e-02, -2.4243e-01,  4.5888e-02, -1.6301e-01,\n",
       "                       -8.3634e-02],\n",
       "                      [-2.8558e-01, -2.4556e-02, -1.1828e-02,  2.8155e-01,  2.8068e-01,\n",
       "                        2.5848e-01,  2.9975e-01, -3.6218e-01, -5.4034e-01, -5.7991e-01,\n",
       "                       -4.6203e-02, -2.3187e-02, -5.9005e-01,  2.9057e-02,  2.1822e-01,\n",
       "                       -3.5403e-01],\n",
       "                      [-3.1018e-01, -1.5738e-01, -7.4081e-01,  1.0248e-01,  1.6057e-01,\n",
       "                        2.3540e-01,  2.5845e-01, -3.4024e-01, -2.4659e-01, -1.8286e-01,\n",
       "                        2.3735e-01,  6.0688e-02, -1.0731e-01,  4.3151e-01, -9.7251e-02,\n",
       "                       -2.3428e-01],\n",
       "                      [-1.3248e-01,  1.6224e-01, -2.5619e+00,  2.3357e-01, -1.9806e-01,\n",
       "                        8.8566e-01, -7.6045e-02, -1.4882e-02,  9.2534e-03, -4.5402e-02,\n",
       "                        2.4137e-01, -2.0174e-01,  9.5096e-02, -3.8300e-01,  2.2262e-01,\n",
       "                       -9.3951e-02],\n",
       "                      [-3.9357e-01, -6.3647e-02, -7.8499e-02,  2.7353e-02, -5.7092e-04,\n",
       "                        4.4018e-02,  1.1097e-01, -1.8243e-01, -1.9766e-01, -4.7986e-02,\n",
       "                       -1.4741e-01,  2.0999e-01, -1.2265e-02,  1.9719e-02,  2.1994e-01,\n",
       "                        8.1226e-02],\n",
       "                      [ 1.7691e-01,  2.1547e-01, -9.8344e-02, -1.1432e-01,  2.5908e-02,\n",
       "                       -1.4620e-02,  3.0292e-01, -1.9601e-01,  9.7139e-02,  2.5551e-01,\n",
       "                        1.2063e-01, -1.9827e-01, -1.6028e-01,  1.9391e-01,  2.8384e-01,\n",
       "                       -1.7010e-01],\n",
       "                      [ 2.2423e-01, -1.7426e-01, -1.5924e+00,  1.1410e-01,  2.5921e-01,\n",
       "                       -4.0275e-01,  4.5382e-02,  8.0894e-02, -1.7527e-01,  7.0381e-02,\n",
       "                        7.7011e-02,  2.8555e-01, -9.1000e-02, -1.5965e-02,  1.5895e-01,\n",
       "                        1.0415e-01],\n",
       "                      [ 8.9925e-02,  1.9965e-01,  9.2159e-02, -2.2193e-01, -1.4731e-01,\n",
       "                       -2.9432e-01, -6.1357e-02, -1.5864e-01,  9.7727e-02,  9.5441e-02,\n",
       "                        8.4714e-02,  2.1869e-01, -8.4368e-02,  1.2662e-01,  6.8858e-02,\n",
       "                       -2.7367e-01],\n",
       "                      [-2.1292e-01, -7.8931e-02, -2.4109e-01, -6.5250e-02, -1.5964e-02,\n",
       "                        1.4708e-01, -5.8513e-02, -7.5020e-01, -4.1632e-01,  1.6421e-01,\n",
       "                        5.7472e-02,  1.8103e-01,  1.3496e-01,  2.2966e-01, -2.3833e-02,\n",
       "                       -7.8754e-01],\n",
       "                      [-2.1330e-01,  1.0974e-01,  1.7922e-01, -2.8321e-01, -1.4362e-01,\n",
       "                        2.0321e-01,  1.3675e-01, -1.1040e+00, -5.0771e-01,  2.3413e-01,\n",
       "                        1.7472e-01,  2.5688e-01,  2.7701e-02, -1.8809e-01,  1.7316e-02,\n",
       "                       -1.0205e+00],\n",
       "                      [ 7.0131e-02,  6.6169e-02,  1.1453e-01, -9.5494e-02,  6.9036e-02,\n",
       "                       -3.5665e-01,  1.7339e-01, -8.5727e-02,  4.9117e-02,  1.9296e-01,\n",
       "                       -4.4333e-03,  3.5594e-01, -6.2970e-02,  4.5853e-02,  1.3419e-01,\n",
       "                       -1.1196e-01]], device='cuda:0')),\n",
       "             ('decoder.2.bias',\n",
       "              tensor([-0.0635, -0.1336, -0.2395,  0.0010,  0.2033, -0.1070, -0.1757, -0.2816,\n",
       "                       0.2824,  0.0543, -0.0759, -0.1557, -0.1760, -0.1651,  0.1621,  0.2639,\n",
       "                       0.2072,  0.1883, -0.1454,  0.3077, -0.2103,  0.0375, -0.1675,  0.2088,\n",
       "                       0.0261, -0.0516,  0.0573, -0.0491,  0.1566, -0.0828,  0.1819, -0.0888],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.4.weight',\n",
       "              tensor([[ 0.1331, -0.1253,  0.1024,  ...,  0.1811, -0.0935,  0.0454],\n",
       "                      [ 0.0556,  0.0471,  0.0372,  ..., -0.0634,  0.0104,  0.1105],\n",
       "                      [ 0.1699,  0.1396, -0.0426,  ...,  0.0861, -0.0665,  0.2183],\n",
       "                      ...,\n",
       "                      [-0.0517,  0.1347, -0.0806,  ...,  0.1045,  0.0268,  0.0012],\n",
       "                      [ 0.4184,  0.0047, -0.2167,  ..., -0.3536, -0.8878, -0.1579],\n",
       "                      [-0.3198,  0.1990,  0.0961,  ...,  0.2567,  0.7437,  0.3843]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.4.bias',\n",
       "              tensor([-1.9725e-02,  1.1533e-01,  9.0266e-03,  1.3399e-01,  2.1290e-01,\n",
       "                       1.3870e-01,  1.2153e-01,  1.3011e-01,  2.1341e-01,  1.8368e-02,\n",
       "                      -5.2208e-02,  8.1782e-02, -7.3205e-03,  1.4523e-02, -1.5063e-02,\n",
       "                      -2.7225e-04,  7.3819e-02,  1.0067e-01,  3.1639e-01,  6.2246e-02,\n",
       "                      -4.7642e-02,  2.8355e-03, -6.0637e-02, -7.0145e-02, -9.3890e-02,\n",
       "                      -5.1345e-02,  3.8550e-03, -2.8796e-02,  1.6319e-01, -6.7966e-02,\n",
       "                      -1.0754e-01, -1.2071e-01,  2.4326e-02, -3.1763e-02, -6.7958e-02,\n",
       "                       6.5446e-02,  9.3924e-02, -3.3624e-02,  3.0843e-03,  8.8330e-03,\n",
       "                       6.8455e-02, -1.1345e-01,  3.1527e-02,  6.3307e-02, -4.1381e-02,\n",
       "                       1.1512e-01,  9.6966e-02, -4.5793e-02,  1.5073e-02, -8.5656e-02,\n",
       "                       4.2277e-02,  1.0267e-01,  4.9408e-02,  2.6854e-02,  3.0898e-02,\n",
       "                       2.6938e-02,  2.0049e-02,  1.1996e-02,  5.6975e-02,  5.0542e-02,\n",
       "                       2.7147e-02, -5.9104e-02,  8.5107e-02, -5.1924e-02,  1.2531e-02,\n",
       "                       1.1276e-01, -3.2665e-03,  1.1011e-02, -6.1013e-02,  8.4355e-02,\n",
       "                      -5.7609e-02, -1.1336e-01, -6.0003e-02, -7.6165e-02,  1.0200e-01,\n",
       "                       3.3685e-02,  3.9177e-02, -1.8706e-02, -4.5292e-03,  1.3053e-01,\n",
       "                      -1.7994e-02, -3.5274e-03,  9.3012e-03,  5.9168e-02,  5.2498e-02,\n",
       "                      -2.8551e-02, -1.2136e-03, -6.9941e-02, -7.6537e-02, -1.9469e-03,\n",
       "                       9.0847e-02,  1.4698e-02,  5.4401e-02,  3.9669e-02,  2.7055e-02,\n",
       "                       1.4819e-02, -5.2585e-02, -8.6692e-02,  6.7294e-02,  6.9618e-02,\n",
       "                      -3.0465e-02,  8.6697e-03,  2.2330e-02,  7.6536e-02, -8.6707e-02,\n",
       "                       2.5491e-02,  2.5586e-02,  3.8694e-02, -3.3481e-02,  2.9608e-02,\n",
       "                      -5.3843e-02,  9.4562e-02,  8.3824e-02,  7.7095e-02, -4.3084e-02,\n",
       "                       2.1791e-02,  1.8092e-02,  8.0192e-02, -4.3446e-02,  7.0159e-02,\n",
       "                       1.8539e-02,  3.2239e-02,  3.4235e-02, -4.9821e-02, -2.4766e-02,\n",
       "                       1.9997e-01, -5.3870e-02, -1.1023e-01, -6.0679e-03, -7.1745e-02,\n",
       "                      -2.0320e-02, -1.2557e-02, -6.2479e-02, -4.8012e-02,  1.4674e-01,\n",
       "                      -2.9585e-02,  2.9103e-01,  1.7602e-02, -5.5954e-03, -3.2942e-02,\n",
       "                      -1.1390e-01, -2.1392e-03, -3.6056e-02, -1.2019e-04, -7.2416e-02,\n",
       "                       5.4057e-02,  5.6679e-03,  1.9422e-02,  9.7672e-02,  5.8540e-03,\n",
       "                      -1.7430e-02,  5.6231e-02,  2.9971e-02,  5.2627e-02,  5.2287e-02,\n",
       "                       6.9030e-02, -5.0288e-02,  4.9192e-02, -7.5538e-02, -9.9818e-03,\n",
       "                       3.1838e-02, -1.1480e-02,  1.4022e-02,  7.9774e-02,  1.0065e-01,\n",
       "                      -6.9339e-02, -8.6256e-02,  7.8429e-02,  4.1373e-02, -1.6488e-02,\n",
       "                       2.0683e-02, -1.0862e-01,  5.1278e-02,  1.0101e-01,  1.5414e-01,\n",
       "                      -5.0642e-02, -5.7614e-02, -7.8068e-02, -1.3518e-01, -3.0108e-02,\n",
       "                      -1.2753e-01,  8.1198e-03,  7.1838e-02,  7.1645e-02, -1.0744e-01,\n",
       "                       2.2090e-02,  3.8099e-02,  1.2510e-02, -5.0191e-02,  2.8271e-01,\n",
       "                       5.1412e-02,  6.2214e-02, -3.6143e-02,  3.1162e-02,  8.6363e-02,\n",
       "                       8.3945e-03,  7.2299e-02,  1.4980e-01,  1.0372e-01, -4.2573e-02,\n",
       "                      -3.8077e-02,  1.3591e-01,  2.4473e-02, -8.3224e-02, -2.3177e-02,\n",
       "                      -7.5403e-02, -8.2147e-02,  2.7005e-02, -6.2587e-02, -6.1758e-02,\n",
       "                      -6.5450e-02, -6.1310e-02,  5.2332e-02,  3.7341e-02, -6.2011e-02,\n",
       "                      -8.1123e-04, -1.3806e-02, -6.3829e-02, -5.5763e-02,  2.3297e-02,\n",
       "                      -5.7610e-02, -5.7013e-02,  9.2849e-02, -3.2891e-02,  1.8917e-01,\n",
       "                       2.2601e-01,  1.7295e-01], device='cuda:0'))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.0.weight',\n",
       "              tensor([[-0.1396,  0.0467, -0.0312,  ...,  0.0007, -0.2838,  0.3086],\n",
       "                      [ 0.0017, -0.0041, -0.0094,  ...,  0.0116,  0.1530, -0.0595],\n",
       "                      [ 0.0976,  0.0505,  0.0010,  ...,  0.1117, -0.0056, -0.0236],\n",
       "                      ...,\n",
       "                      [ 0.0509,  0.0253, -0.0583,  ...,  0.0244,  0.2125, -0.2639],\n",
       "                      [ 0.0624,  0.0081, -0.0249,  ...,  0.0044,  0.0583, -0.0241],\n",
       "                      [ 0.0240, -0.0344, -0.0038,  ...,  0.0660, -0.3947,  0.2876]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.0.bias',\n",
       "              tensor([-0.0289,  0.0653, -0.0274,  0.0476, -0.0014,  0.0434,  0.0270,  0.0249,\n",
       "                      -0.0039,  0.0494,  0.0095,  0.0098, -0.0311,  0.0158,  0.0211, -0.0311,\n",
       "                      -0.0307, -0.0588, -0.0579, -0.0377, -0.0281,  0.0350,  0.0059,  0.0590,\n",
       "                       0.0149,  0.0271,  0.0285, -0.0099, -0.0548,  0.0600,  0.0193,  0.0628],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.1.weight',\n",
       "              tensor([1.0626, 1.2037, 0.9204, 1.0411, 1.4316, 1.2731, 1.2978, 0.9720, 1.1429,\n",
       "                      1.1054, 1.3724, 1.5383, 1.0619, 1.0928, 0.8090, 1.2034, 1.0598, 1.3547,\n",
       "                      1.1372, 0.9635, 1.0841, 0.9085, 0.9770, 0.8138, 0.9653, 1.0998, 1.2223,\n",
       "                      0.8843, 1.0040, 1.0144, 1.0770, 1.1458], device='cuda:0')),\n",
       "             ('encoder.1.bias',\n",
       "              tensor([ 5.7665e-03,  1.3979e-01,  1.6285e-01,  9.2658e-02,  3.3826e-01,\n",
       "                       8.0893e-02,  2.7830e-01, -1.1338e-01,  7.5435e-02,  5.3593e-02,\n",
       "                       6.9571e-02,  9.3067e-03,  4.9135e-02,  3.9190e-01,  4.9306e-04,\n",
       "                       3.4275e-01,  6.1127e-02,  4.8600e-01, -5.9802e-01, -4.4879e-02,\n",
       "                       3.5488e-02, -4.0731e-01, -9.1112e-02, -3.3908e-01,  2.7485e-01,\n",
       "                       8.4087e-02,  1.5644e-01, -1.3318e-03, -5.5935e-02, -1.8767e-01,\n",
       "                       2.9655e-02,  3.7130e-02], device='cuda:0')),\n",
       "             ('encoder.1.running_mean',\n",
       "              tensor([ 0.0791,  0.0625, -0.1023,  0.4357, -0.3758,  0.2466, -0.2072, -0.2673,\n",
       "                      -0.0680,  0.2000, -0.0254,  0.1554,  0.3575, -0.1119, -0.4800, -0.3017,\n",
       "                       0.3927, -0.3993,  0.3151, -0.2563, -0.0581,  0.1431, -0.3472, -0.2832,\n",
       "                       0.1732,  0.1964,  0.1066, -0.4455,  0.3694, -0.1935, -0.1166, -0.3177],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.1.running_var',\n",
       "              tensor([0.1490, 0.0787, 0.0642, 0.1293, 0.1115, 0.2225, 0.0996, 0.1094, 0.1261,\n",
       "                      0.1142, 0.1623, 0.1277, 0.1249, 0.1695, 0.1252, 0.0939, 0.1232, 0.4147,\n",
       "                      0.1940, 0.1359, 0.0875, 0.1036, 0.1585, 0.1492, 0.1045, 0.1300, 0.1442,\n",
       "                      0.1621, 0.1451, 0.1848, 0.0877, 0.1332], device='cuda:0')),\n",
       "             ('encoder.1.num_batches_tracked', tensor(5760, device='cuda:0')),\n",
       "             ('encoder.2.weight',\n",
       "              tensor([[-8.8927e-02, -1.2431e-01,  2.7515e-02,  1.5683e-01, -1.0409e-01,\n",
       "                        7.5310e-02,  6.9601e-02,  8.8314e-02,  4.5385e-02, -1.5436e-01,\n",
       "                       -5.4526e-02,  1.5425e-01, -4.0708e-02, -2.8177e-02, -4.6669e-02,\n",
       "                        1.4565e-02, -7.9849e-02,  2.3996e-01,  8.9691e-02,  3.9420e-02,\n",
       "                       -1.0320e-01, -1.2972e-01,  4.8631e-02, -3.3028e-02, -1.2960e-01,\n",
       "                       -7.9077e-03,  9.3299e-02,  7.6786e-02,  4.0696e-02,  1.6242e-01,\n",
       "                       -2.0453e-01, -7.6377e-02],\n",
       "                      [-2.4128e-02,  2.7576e-01,  2.2893e-01, -1.7600e-01, -2.8435e-02,\n",
       "                       -1.5553e-03,  1.9705e-01, -1.5626e-01, -2.0973e-01, -2.9483e-02,\n",
       "                       -4.7123e-03, -1.3544e-02, -3.0205e-04,  1.1225e-01, -6.4975e-02,\n",
       "                        7.3269e-02,  9.1603e-02,  3.2646e-02, -1.6643e-01,  5.1918e-02,\n",
       "                       -5.1887e-02, -8.3088e-02,  1.5850e-01, -2.2873e-01,  1.6319e-01,\n",
       "                        1.2023e-01,  2.4621e-01,  1.4839e-01, -1.2775e-01,  1.0128e-01,\n",
       "                       -6.5777e-02, -9.5819e-03],\n",
       "                      [-3.2316e-01,  3.5127e-01, -2.2701e-01,  9.6158e-03, -4.4909e-01,\n",
       "                        3.9497e-01,  5.2396e-01,  1.6098e-02,  5.0392e-01,  3.0326e-01,\n",
       "                       -4.6966e-01,  4.6463e-01,  4.2087e-01,  3.5477e-01,  2.2831e-02,\n",
       "                       -6.6963e-03, -1.2203e-01, -4.3740e-01,  4.6359e-01, -2.7940e-01,\n",
       "                        3.6043e-01,  3.3244e-01,  1.2004e-01, -2.2255e-01,  2.1979e-01,\n",
       "                        3.1330e-01,  4.1441e-01, -1.0245e-01,  4.0614e-01, -1.4853e-01,\n",
       "                        3.7898e-01, -9.7099e-02],\n",
       "                      [-7.2979e-02,  2.2370e-01, -1.8206e-01, -2.4353e-02, -2.9756e-01,\n",
       "                        1.1137e-01,  2.6352e-01, -2.6120e-02,  1.1357e-01,  6.2308e-02,\n",
       "                       -2.4780e-01,  3.0581e-01, -1.0069e-01,  2.7326e-01, -6.8923e-03,\n",
       "                        6.8330e-02, -1.3769e-01, -2.1035e-01,  3.0569e-01, -9.9032e-02,\n",
       "                        2.3316e-01,  1.9300e-01, -6.6174e-02, -5.3439e-02,  3.6518e-02,\n",
       "                        2.8159e-01,  6.2721e-02,  2.9772e-02,  1.9629e-01, -1.0185e-01,\n",
       "                        1.0734e-01, -1.5194e-01],\n",
       "                      [-2.4895e-01,  2.7395e-02,  7.9423e-02, -1.3841e-01, -4.3761e-01,\n",
       "                        3.1482e-01, -2.0044e-02,  2.5828e-01,  1.1321e-01,  2.1395e-01,\n",
       "                       -3.7983e-01,  3.7388e-01,  5.9949e-02, -1.1724e-01,  1.0365e-01,\n",
       "                       -5.6051e-02, -3.6499e-01, -5.5869e-01,  4.9618e-01,  1.0198e-01,\n",
       "                        1.5674e-01, -6.4057e-02,  2.3684e-01, -1.9839e-02, -7.2154e-02,\n",
       "                       -2.1227e-01,  1.8598e-01,  9.1913e-02,  9.7347e-02,  2.6274e-01,\n",
       "                        1.9070e-01, -3.8913e-01],\n",
       "                      [-5.8527e-03,  1.1020e-01,  1.4702e-02,  4.2336e-02,  5.8370e-02,\n",
       "                        1.1466e-01,  1.7731e-01,  6.6129e-02,  1.4087e-01,  1.9098e-01,\n",
       "                       -1.4586e-01,  1.3243e-01,  6.6717e-02,  4.1631e-01, -1.6675e-02,\n",
       "                        9.0460e-02, -1.6382e-01,  8.7826e-02, -1.7986e-01, -6.2815e-02,\n",
       "                        1.4662e-01, -2.3302e-01, -1.2021e-01, -9.1921e-02,  1.1167e-01,\n",
       "                        6.1836e-02,  1.3800e-01, -3.5418e-03, -1.0547e-02, -1.3817e-01,\n",
       "                        1.6480e-01, -5.5434e-02],\n",
       "                      [-1.5742e-01, -1.8856e-01, -4.3305e-02,  9.7283e-03, -1.2246e-01,\n",
       "                        2.9118e-01, -2.0560e-01,  2.1766e-02,  1.2386e-01, -1.0502e-01,\n",
       "                       -6.9740e-02,  3.8420e-02,  1.0565e-01, -3.5674e-02,  1.8798e-01,\n",
       "                        1.6125e-02, -3.8172e-02, -3.7261e-01,  1.8998e-01, -3.5593e-02,\n",
       "                       -2.3200e-01,  1.3111e-01,  1.9368e-01, -2.7550e-01,  7.2626e-02,\n",
       "                        4.8978e-02, -1.1274e-01, -6.1627e-03,  1.0773e-01,  1.4484e-01,\n",
       "                       -1.3645e-01, -6.4111e-02],\n",
       "                      [ 3.5038e-02,  7.3537e-02, -9.3378e-02,  2.0526e-02,  1.1377e-02,\n",
       "                        2.6735e-02, -2.2548e-01, -1.2875e-02,  1.3657e-01,  4.8695e-02,\n",
       "                        1.1046e-01, -4.7034e-02,  1.0460e-01,  1.3818e-01,  5.8274e-02,\n",
       "                        1.7901e-02,  1.1846e-01, -7.5435e-02,  9.4030e-02, -1.7046e-01,\n",
       "                       -1.5997e-01,  1.1470e-01, -2.0115e-01, -1.6867e-01,  1.4435e-01,\n",
       "                        1.4653e-01,  8.0173e-02, -1.4283e-01,  1.4661e-01, -8.5264e-02,\n",
       "                       -2.8535e-01, -4.8354e-02],\n",
       "                      [ 1.5388e-01,  1.6467e-01, -9.5231e-02, -6.1568e-02,  9.2000e-02,\n",
       "                       -2.9310e-02,  9.8248e-02, -4.4077e-02,  1.1665e-01,  2.2777e-01,\n",
       "                       -3.4037e-02, -4.1058e-02,  1.7562e-01,  1.2179e-01, -2.3777e-03,\n",
       "                        1.5003e-01,  3.4075e-02, -1.2382e-01, -4.4499e-02, -2.4822e-01,\n",
       "                        1.1691e-01,  1.3547e-01, -1.7916e-01, -3.5433e-02,  2.1021e-01,\n",
       "                        2.2148e-01, -9.9672e-02, -4.7074e-02,  6.9263e-02, -6.6885e-02,\n",
       "                        4.4622e-02, -1.8343e-02],\n",
       "                      [-8.2406e-02,  1.9809e-02,  9.3970e-02,  2.2631e-01, -6.7449e-02,\n",
       "                        3.8773e-02,  4.8623e-02,  1.5650e-01, -5.0516e-02, -1.1726e-01,\n",
       "                        5.9093e-02,  2.6334e-01, -8.2158e-02, -1.6451e-02,  4.5297e-02,\n",
       "                       -1.4504e-01, -1.1223e-01,  9.8103e-02,  1.4770e-01,  1.7448e-01,\n",
       "                       -2.0850e-01, -6.1633e-02,  1.8413e-01,  9.9880e-04,  4.6305e-02,\n",
       "                       -2.2574e-01,  1.6685e-01,  3.2701e-02, -7.4830e-02,  1.3668e-01,\n",
       "                       -1.4548e-01, -1.2185e-01],\n",
       "                      [-2.0122e-01,  6.6695e-02, -1.0473e-01, -1.4337e-01, -3.7331e-01,\n",
       "                        3.1114e-01, -1.2967e-01,  2.2368e-01, -1.6883e-02,  1.4775e-01,\n",
       "                       -3.8305e-01,  1.4533e-01,  1.3530e-01, -4.2680e-02,  2.0376e-01,\n",
       "                       -1.0899e-01, -2.7915e-01, -4.1902e-01,  5.3484e-01, -3.5783e-02,\n",
       "                        9.4345e-02,  3.1231e-02,  1.7433e-01, -3.8550e-02, -1.6528e-01,\n",
       "                       -1.0870e-01,  2.5413e-01,  1.5222e-01,  4.7121e-02,  2.1335e-01,\n",
       "                        4.7492e-02, -1.7181e-01],\n",
       "                      [-3.0094e-02,  6.5570e-02,  1.8987e-01,  8.6820e-02, -7.2166e-02,\n",
       "                       -3.4338e-02,  1.9931e-01, -7.9412e-02,  9.8595e-02, -6.3496e-02,\n",
       "                        1.5824e-01,  7.2487e-03, -1.6381e-01, -1.6211e-01, -6.4291e-02,\n",
       "                        3.9262e-02,  7.7681e-02,  1.0542e-01, -1.6917e-01,  7.8353e-02,\n",
       "                        1.4323e-03, -1.1845e-01,  3.8737e-02, -1.5383e-02, -2.3431e-02,\n",
       "                       -2.5958e-01,  1.9886e-01,  1.0580e-01,  4.4184e-02,  1.0830e-01,\n",
       "                       -1.1241e-01,  1.9680e-02],\n",
       "                      [-9.6539e-02,  2.8888e-02, -1.5568e-01,  2.6025e-01, -3.3818e-02,\n",
       "                        1.0797e-01,  6.7323e-02,  1.9329e-01,  5.9426e-02,  1.2057e-01,\n",
       "                       -3.0234e-01,  3.6695e-01,  1.9020e-01,  1.9547e-01, -2.2137e-01,\n",
       "                       -1.7042e-02, -2.2468e-01,  2.6361e-01, -1.4995e-02,  2.8863e-02,\n",
       "                        1.1246e-01, -1.7561e-01, -1.6541e-01,  8.1955e-02,  1.0903e-01,\n",
       "                       -7.3937e-02,  2.7347e-01, -1.2407e-01,  5.8340e-02, -1.0089e-01,\n",
       "                        2.0591e-01,  8.3456e-02],\n",
       "                      [-1.8982e-01,  2.4618e-01,  1.0883e-01, -2.7209e-01, -1.5482e-01,\n",
       "                        2.6327e-01,  3.7109e-01, -1.8373e-01,  5.0681e-02,  1.1653e-01,\n",
       "                       -9.4930e-03, -1.1480e-02,  2.1743e-01,  1.2140e-01, -6.6392e-02,\n",
       "                        1.4977e-01,  2.2393e-01, -2.0652e-01, -1.8316e-01, -3.2304e-02,\n",
       "                        1.0248e-01,  6.1889e-02,  5.3504e-02, -1.2223e-01,  4.0113e-01,\n",
       "                        6.8572e-02, -3.1241e-02,  9.7589e-02, -2.7913e-02,  2.3025e-01,\n",
       "                        2.6910e-02, -1.2969e-01],\n",
       "                      [-5.5165e-02,  1.1971e-01,  8.0840e-02,  3.3391e-01, -4.7330e-01,\n",
       "                        8.2346e-02, -1.6671e-01, -2.1084e-01, -1.1052e-01,  2.0716e-01,\n",
       "                       -6.4010e-02,  4.1427e-01,  1.6361e-01, -6.5368e-02, -2.1427e-01,\n",
       "                       -6.0496e-01,  2.5912e-01, -2.4257e-01,  3.2786e-01, -1.3908e-01,\n",
       "                        1.8933e-01,  2.4251e-02, -8.9898e-02, -6.4283e-02,  3.7862e-02,\n",
       "                        2.0094e-01,  2.3457e-01, -1.8488e-01,  3.1840e-01,  1.2874e-02,\n",
       "                        3.3199e-02, -2.4417e-01],\n",
       "                      [-2.8410e-02, -3.7985e-01, -2.5680e-01,  4.9945e-02,  1.4781e-02,\n",
       "                       -1.6779e-01, -3.5330e-01,  2.9295e-01, -1.0728e-01, -1.5008e-01,\n",
       "                       -1.3511e-01, -9.2882e-02, -1.0356e-01, -4.7628e-02,  8.5674e-02,\n",
       "                       -2.9210e-02,  4.4653e-02,  1.4019e-01,  2.2886e-01,  1.5986e-01,\n",
       "                        4.9433e-02,  1.6913e-01,  1.8519e-02,  1.7504e-01, -2.1971e-01,\n",
       "                       -6.1355e-02,  3.9847e-02, -2.5246e-01, -3.6820e-03, -2.0411e-01,\n",
       "                        8.7835e-02,  2.3736e-01]], device='cuda:0')),\n",
       "             ('encoder.2.bias',\n",
       "              tensor([ 0.0256,  0.2405,  0.0340,  0.1483, -0.4355,  0.6744,  0.1338,  0.0800,\n",
       "                      -0.1821,  0.1681, -0.1440,  0.2882, -0.0880,  0.1412, -0.1981, -0.1074],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.4.weight',\n",
       "              tensor([[-0.1313, -0.1296,  0.3713,  0.1293, -0.4086,  0.3379, -0.0946,  0.0728,\n",
       "                       -0.1704, -0.2329, -0.2439, -0.0671, -0.1989,  0.1137,  0.4443,  0.0750],\n",
       "                      [ 0.2307, -0.1017, -0.1451,  0.0644,  0.1242, -0.1458, -0.0892, -0.2094,\n",
       "                       -0.1687,  0.2377, -0.0985,  0.2490, -0.0077, -0.1042, -0.2407, -0.0845]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.4.bias', tensor([0.1411, 0.1352], device='cuda:0')),\n",
       "             ('decoder.0.weight',\n",
       "              tensor([[-0.6732, -0.2072],\n",
       "                      [ 0.4279,  0.1056],\n",
       "                      [ 0.3284, -0.5242],\n",
       "                      [-0.6664, -0.4385],\n",
       "                      [-0.6706, -0.4833],\n",
       "                      [ 0.6697, -0.2643],\n",
       "                      [-0.6531,  0.2193],\n",
       "                      [-0.5553, -0.3542],\n",
       "                      [-0.4087,  0.6946],\n",
       "                      [ 0.5768,  0.5513],\n",
       "                      [-0.3609, -0.6389],\n",
       "                      [ 0.6510, -0.4413],\n",
       "                      [ 0.1792,  0.2962],\n",
       "                      [ 0.3074,  0.6459],\n",
       "                      [ 0.2798, -0.4055],\n",
       "                      [-0.5171,  0.1751]], device='cuda:0')),\n",
       "             ('decoder.0.bias',\n",
       "              tensor([ 0.0729, -0.1789,  0.5773, -0.5473, -0.3123,  0.1668, -0.3433, -0.5109,\n",
       "                      -0.1861, -0.3430,  0.6403,  0.6553,  0.0525, -0.2147,  0.0752,  0.5981],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.2.weight',\n",
       "              tensor([[-0.2126,  0.1882, -0.0257, -0.0621, -0.1430,  0.0445,  0.1324, -0.1009,\n",
       "                       -0.1750,  0.2305, -0.2354,  0.0379, -0.0515,  0.1799,  0.2271,  0.1325],\n",
       "                      [ 0.2266, -0.0494,  0.1751,  0.1846, -0.1210, -0.1249,  0.0219,  0.1537,\n",
       "                        0.1530, -0.0127, -0.1569,  0.2315, -0.0173, -0.1337, -0.2462,  0.1660],\n",
       "                      [ 0.0095,  0.2447, -0.0899,  0.2096, -0.1290, -0.0624, -0.1296, -0.0260,\n",
       "                       -0.1234, -0.1945,  0.0410,  0.2029,  0.0589,  0.1891,  0.2167, -0.2373],\n",
       "                      [-0.1471,  0.1318, -0.1495, -0.1543,  0.1539, -0.0921, -0.2197,  0.0212,\n",
       "                        0.0198,  0.0564,  0.1338, -0.1980, -0.0135, -0.1095,  0.0775,  0.1171],\n",
       "                      [-0.0409, -0.1069,  0.1319,  0.2135, -0.1667,  0.1481,  0.0535, -0.0364,\n",
       "                       -0.1214,  0.0138,  0.0967, -0.1205,  0.1820, -0.2494,  0.0915,  0.1211],\n",
       "                      [ 0.1086, -0.2470, -0.0179,  0.2187,  0.0797, -0.1636, -0.0225, -0.2498,\n",
       "                       -0.2008, -0.1266,  0.2246, -0.2158,  0.0112, -0.1651, -0.1869,  0.0995],\n",
       "                      [-0.0559, -0.0452, -0.1728, -0.0657, -0.1512, -0.0333, -0.0123, -0.2165,\n",
       "                       -0.0011, -0.1842,  0.0758,  0.1205, -0.0134, -0.2096, -0.0105, -0.0263],\n",
       "                      [ 0.0312, -0.0840, -0.2239,  0.1996,  0.0605, -0.1779, -0.1862,  0.0119,\n",
       "                       -0.1136,  0.0815, -0.2448,  0.1203,  0.1318,  0.1341, -0.0662, -0.0514],\n",
       "                      [ 0.0182, -0.0066, -0.0697,  0.0197,  0.1023, -0.0642, -0.1307,  0.1649,\n",
       "                        0.1784, -0.0477,  0.1135, -0.0236, -0.0511,  0.0026,  0.2243, -0.0727],\n",
       "                      [ 0.2379, -0.1160,  0.2126, -0.2455,  0.2109,  0.1306,  0.1311,  0.2072,\n",
       "                        0.0244,  0.0704, -0.0176,  0.2042,  0.1850, -0.2178,  0.0479,  0.0553],\n",
       "                      [ 0.2282, -0.1700,  0.0048,  0.2434, -0.1208,  0.0377, -0.1029,  0.1472,\n",
       "                        0.0395, -0.1664,  0.2252,  0.1163, -0.1313, -0.2060, -0.1745, -0.2457],\n",
       "                      [-0.2350,  0.1481,  0.0296, -0.0768,  0.0708, -0.0048, -0.1268,  0.1909,\n",
       "                        0.0872, -0.1303, -0.0656,  0.0561,  0.0296,  0.0600, -0.0264, -0.1319],\n",
       "                      [-0.1010,  0.2433,  0.2122,  0.2041,  0.1033, -0.2065, -0.1696, -0.0121,\n",
       "                       -0.0761, -0.0207, -0.0172,  0.1880, -0.0017,  0.1661,  0.0701,  0.0784],\n",
       "                      [-0.0364, -0.1376, -0.2223,  0.1183,  0.1946, -0.1794, -0.1584, -0.0710,\n",
       "                        0.0090, -0.0471,  0.2125, -0.0030, -0.1143, -0.2024,  0.1222,  0.1945],\n",
       "                      [-0.1250,  0.1279,  0.1964,  0.1070,  0.0982,  0.0856,  0.1788, -0.0247,\n",
       "                       -0.2289,  0.0681,  0.0985,  0.1646,  0.1038,  0.1051,  0.1465,  0.0119],\n",
       "                      [ 0.1170,  0.2243, -0.1601,  0.0160,  0.1192, -0.2211,  0.1328,  0.1731,\n",
       "                        0.1146, -0.0387, -0.2118, -0.1263,  0.1070,  0.0133,  0.0303, -0.1554],\n",
       "                      [-0.2354,  0.0511,  0.1837, -0.0604, -0.0530, -0.1741, -0.1857,  0.2097,\n",
       "                        0.2254, -0.1162,  0.1323, -0.2108,  0.1801,  0.2050, -0.1937, -0.1139],\n",
       "                      [-0.1415, -0.1538,  0.1537,  0.1331,  0.1126,  0.2494, -0.2096, -0.0856,\n",
       "                        0.2014,  0.1713, -0.0920,  0.2384, -0.2271, -0.1367,  0.1589,  0.0488],\n",
       "                      [-0.1351, -0.1257, -0.0872, -0.0600, -0.1418, -0.0155,  0.2213,  0.1225,\n",
       "                       -0.1743,  0.0918, -0.2478, -0.2105,  0.0371,  0.1599,  0.0222,  0.0533],\n",
       "                      [-0.1795,  0.2319, -0.1164,  0.2216,  0.2100, -0.2221, -0.0334,  0.0030,\n",
       "                       -0.1269,  0.2476, -0.0631,  0.0012,  0.1664, -0.1998, -0.1893, -0.0644],\n",
       "                      [ 0.0476, -0.1433, -0.2048, -0.2408,  0.1338, -0.1956,  0.1443, -0.2101,\n",
       "                        0.1569,  0.0359, -0.1644, -0.0817, -0.1640, -0.1753,  0.0714, -0.2321],\n",
       "                      [-0.2146,  0.1893, -0.0466,  0.1138,  0.0603,  0.0039,  0.1242,  0.1675,\n",
       "                       -0.0283, -0.0339, -0.0522,  0.1588, -0.2404, -0.2348, -0.2058, -0.1071],\n",
       "                      [ 0.1496, -0.0090,  0.1254, -0.0243, -0.0800, -0.1759, -0.1373, -0.1423,\n",
       "                       -0.0569, -0.0394,  0.0098,  0.1477,  0.2107, -0.0207, -0.2180,  0.2314],\n",
       "                      [ 0.2103,  0.0488, -0.1790,  0.2081,  0.2377,  0.1559,  0.2375, -0.0726,\n",
       "                        0.1862, -0.2093,  0.1645, -0.0540,  0.0158,  0.1249,  0.1758,  0.1453],\n",
       "                      [-0.1732, -0.1215, -0.2316, -0.1207,  0.1099,  0.1696, -0.1611, -0.0526,\n",
       "                        0.0683, -0.2269,  0.1605,  0.0538,  0.0567, -0.0030, -0.2440, -0.1016],\n",
       "                      [ 0.0571,  0.0250, -0.1096,  0.1003, -0.1554, -0.0485, -0.1697, -0.0153,\n",
       "                       -0.1475,  0.0781, -0.2213,  0.0295, -0.0389, -0.0501, -0.0081, -0.0927],\n",
       "                      [ 0.1784, -0.2038, -0.1427, -0.1818, -0.2055, -0.0930,  0.1609,  0.1870,\n",
       "                        0.0973,  0.0749,  0.0918, -0.0928, -0.1164,  0.1077, -0.0782,  0.1514],\n",
       "                      [-0.2141,  0.2343, -0.0158,  0.2461, -0.1213,  0.1495, -0.2173,  0.0224,\n",
       "                       -0.0710,  0.0278,  0.0696, -0.2137,  0.1212, -0.1515, -0.0546,  0.0989],\n",
       "                      [-0.1946, -0.1990,  0.2049, -0.0151,  0.0504, -0.0193,  0.0552, -0.0853,\n",
       "                        0.1885, -0.0424, -0.2494,  0.1600, -0.1355, -0.1358, -0.1092, -0.1962],\n",
       "                      [ 0.0839, -0.0207,  0.0823,  0.1031,  0.0651, -0.0296,  0.1067,  0.0699,\n",
       "                       -0.2435, -0.2322,  0.0626, -0.0976, -0.0726, -0.2011,  0.0281,  0.1938],\n",
       "                      [-0.1011,  0.2001,  0.1535, -0.0713, -0.0459, -0.0960,  0.1879, -0.0181,\n",
       "                       -0.0892,  0.1322, -0.1487,  0.1736, -0.1623, -0.0242,  0.1065,  0.0426],\n",
       "                      [ 0.1677,  0.1338, -0.1943,  0.0837,  0.0379, -0.0953, -0.2365,  0.0158,\n",
       "                       -0.0767, -0.1757,  0.1874, -0.1823, -0.1634, -0.1585,  0.1848, -0.1960]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.2.bias',\n",
       "              tensor([-1.0001e-01, -1.3793e-04, -5.2551e-02, -1.4296e-01,  3.3186e-02,\n",
       "                       2.0231e-01, -1.5790e-02, -2.1737e-01, -1.0817e-01, -7.4533e-02,\n",
       "                       1.0068e-01,  1.2978e-01, -2.4412e-01,  1.4384e-01, -8.3280e-02,\n",
       "                       1.9063e-01, -1.0590e-01, -9.7041e-02, -3.4981e-03,  1.6889e-01,\n",
       "                      -8.4105e-03,  1.7496e-02,  7.7986e-02, -1.3432e-01, -1.8348e-01,\n",
       "                      -1.1402e-01, -1.1356e-01,  2.7482e-02,  6.0905e-02, -2.4721e-01,\n",
       "                       1.5787e-01, -3.3149e-02], device='cuda:0')),\n",
       "             ('decoder.4.weight',\n",
       "              tensor([[ 0.1044,  0.1292,  0.0701,  ...,  0.0203,  0.0398,  0.0374],\n",
       "                      [ 0.0221, -0.0819,  0.1563,  ...,  0.1620,  0.1625,  0.1687],\n",
       "                      [ 0.1305,  0.1022,  0.0735,  ...,  0.0115,  0.0794, -0.0055],\n",
       "                      ...,\n",
       "                      [-0.0101,  0.0570, -0.1479,  ...,  0.0512, -0.1045,  0.0299],\n",
       "                      [ 0.0403, -0.1650,  0.0953,  ...,  0.0329,  0.0848, -0.1066],\n",
       "                      [ 0.0389, -0.1438, -0.1527,  ...,  0.0290, -0.0126,  0.1528]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.4.bias',\n",
       "              tensor([ 1.3079e-01, -3.7835e-02, -1.4920e-01, -5.5088e-02,  8.9347e-02,\n",
       "                      -1.4851e-02, -6.9178e-02,  1.3777e-01,  1.0244e-01,  1.6637e-01,\n",
       "                      -2.6106e-02,  4.8382e-02,  8.9980e-02,  1.3850e-01,  2.1832e-02,\n",
       "                       6.1290e-02,  7.7962e-03,  3.9267e-02, -8.6537e-02,  1.5612e-01,\n",
       "                      -8.5135e-02,  6.6712e-02, -1.3588e-01,  8.1908e-02,  1.4465e-01,\n",
       "                       1.7234e-01,  1.3142e-01,  1.5096e-01,  1.5405e-01,  1.9023e-02,\n",
       "                       5.6008e-04, -1.7000e-01,  1.0522e-01,  1.9009e-02, -6.8213e-02,\n",
       "                      -1.4217e-01,  1.3623e-02,  7.6481e-02, -4.1279e-02,  3.8026e-02,\n",
       "                      -9.5283e-03, -1.5964e-01,  1.1914e-01, -5.7226e-02, -1.7045e-01,\n",
       "                      -1.0877e-01,  1.4121e-01, -9.5533e-02, -9.2159e-02,  1.2445e-01,\n",
       "                       4.3346e-02,  6.9951e-02, -6.3961e-02, -1.1778e-01, -1.5620e-01,\n",
       "                       2.6285e-02, -9.3342e-02, -5.5884e-02,  1.5672e-01, -1.4339e-01,\n",
       "                      -6.9765e-02, -4.5939e-02,  1.3505e-01, -6.2190e-02,  5.3933e-02,\n",
       "                       1.6277e-01,  1.7250e-01,  1.3873e-01, -6.0800e-02, -1.5253e-01,\n",
       "                      -3.9350e-02,  1.4808e-01,  8.1245e-02,  1.6815e-01, -6.4244e-02,\n",
       "                      -6.9097e-02, -1.1596e-01, -2.3375e-02,  1.6594e-01, -1.1750e-01,\n",
       "                      -6.7670e-02, -1.4818e-01,  1.5689e-01,  9.7786e-02, -6.9458e-02,\n",
       "                      -1.4065e-01, -9.7488e-02, -6.7741e-03, -1.3637e-01, -1.7622e-01,\n",
       "                      -1.2061e-01, -1.3520e-01,  2.2939e-02, -4.6746e-02, -1.6659e-01,\n",
       "                       1.7471e-01, -1.0409e-01,  9.9570e-02, -3.6514e-02, -1.1997e-01,\n",
       "                       3.5355e-02, -1.3934e-02, -1.0497e-01,  1.2723e-01,  2.7507e-02,\n",
       "                       1.6334e-01, -1.0422e-01,  1.2438e-04,  9.3965e-02,  1.2044e-02,\n",
       "                      -9.2884e-02, -1.3983e-01, -1.6084e-01, -9.1819e-02,  9.1599e-02,\n",
       "                      -5.2991e-02, -1.3252e-01,  2.6494e-02, -1.1965e-01, -1.5669e-01,\n",
       "                       1.0299e-01,  4.1699e-02,  5.5804e-02,  1.5714e-01,  1.7486e-01,\n",
       "                      -4.1370e-02,  3.7895e-02, -6.5370e-02, -2.5202e-02,  1.1274e-01,\n",
       "                       9.6898e-02,  6.4204e-02, -1.4993e-01,  1.5767e-01,  1.4662e-02,\n",
       "                      -2.4510e-02, -3.4099e-02, -1.3606e-01,  1.5252e-01, -1.2493e-01,\n",
       "                       1.0968e-01, -1.5076e-01, -1.0662e-01, -7.0247e-02,  5.7451e-02,\n",
       "                       1.6838e-01,  9.2996e-02, -6.5089e-02, -2.2747e-02, -6.7446e-02,\n",
       "                       2.0966e-05, -1.6285e-01,  2.1301e-02,  1.4154e-01, -1.4230e-01,\n",
       "                       7.8180e-02,  8.6337e-04, -3.8804e-02,  1.2613e-01,  1.1573e-02,\n",
       "                       1.4744e-01, -6.0256e-02,  1.6751e-01, -1.6110e-03,  1.1673e-01,\n",
       "                      -1.3430e-01,  1.2601e-01, -1.5734e-01,  3.4581e-02,  1.0772e-01,\n",
       "                      -3.3694e-03, -8.5992e-02,  1.2097e-01,  5.1163e-02, -7.5682e-02,\n",
       "                      -1.6198e-01, -1.4988e-02, -1.4065e-01, -1.5243e-01, -1.7524e-02,\n",
       "                      -2.9733e-03, -1.3628e-01,  1.2866e-01, -1.0320e-01, -5.6191e-02,\n",
       "                       4.8403e-02,  1.7554e-01, -1.4720e-01, -4.4579e-02,  1.0703e-01,\n",
       "                      -1.4702e-01,  9.0154e-02, -8.1066e-02,  1.1013e-01,  1.1605e-02,\n",
       "                       4.7210e-02,  1.0390e-01,  4.7083e-02, -1.7636e-01, -1.5938e-01,\n",
       "                       3.0554e-02, -1.5541e-01, -1.4269e-01, -6.4301e-02, -1.0451e-01,\n",
       "                      -3.4612e-02,  4.9578e-02,  1.7344e-01,  1.4205e-01,  2.1323e-02,\n",
       "                       1.4223e-01, -4.9339e-02, -5.0152e-02, -8.2510e-02,  6.2972e-02,\n",
       "                       1.2348e-01, -1.4980e-01,  5.6621e-02, -6.6503e-02, -8.5567e-02,\n",
       "                      -9.9697e-03,  5.9915e-02,  1.0445e-02,  1.0756e-01, -1.3168e-01,\n",
       "                       1.2235e-01, -2.7203e-02], device='cuda:0'))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.weight matches\n",
      "encoder.0.bias matches\n",
      "encoder.1.weight matches\n",
      "encoder.1.bias matches\n",
      "encoder.1.running_mean different\n",
      "encoder.1.running_var different\n",
      "encoder.1.num_batches_tracked different\n",
      "encoder.2.weight matches\n",
      "encoder.2.bias matches\n",
      "encoder.4.weight different\n",
      "encoder.4.bias different\n",
      "decoder.0.weight different\n",
      "decoder.0.bias different\n",
      "decoder.2.weight different\n",
      "decoder.2.bias different\n",
      "decoder.4.weight different\n",
      "decoder.4.bias different\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    if (a[i] == b[i]).all():\n",
    "        print(f\"{i} matches\")\n",
    "    else:\n",
    "       print(f\"{i} different\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a[\"encoder.0.bias\"] == b[\"encoder.0.bias\"]).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9990, -0.9761, -0.0871, -0.8815,  0.6756,  0.6615,  0.7377,  0.2528,\n",
       "        -0.6931,  0.6946,  0.7773,  0.8097,  0.5125,  0.9604,  0.4772,  0.3078],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"decoder.0.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0729, -0.1789,  0.5773, -0.5473, -0.3123,  0.1668, -0.3433, -0.5109,\n",
       "        -0.1861, -0.3430,  0.6403,  0.6553,  0.0525, -0.2147,  0.0752,  0.5981],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"decoder.0.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.weight\n",
      "encoder.0.bias\n",
      "encoder.1.weight\n",
      "encoder.1.bias\n",
      "encoder.1.running_mean\n",
      "encoder.1.running_var\n",
      "encoder.1.num_batches_tracked\n",
      "encoder.2.weight\n",
      "encoder.2.bias\n",
      "encoder.4.weight\n",
      "encoder.4.bias\n",
      "decoder.0.weight\n",
      "decoder.0.bias\n",
      "decoder.2.weight\n",
      "decoder.2.bias\n",
      "decoder.4.weight\n",
      "decoder.4.bias\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
