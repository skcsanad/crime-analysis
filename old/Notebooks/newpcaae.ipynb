{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "#from tensorboardX import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAAutoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, last_hidden_shape):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.last_hidden_shape = last_hidden_shape\n",
    "        self.bottleneck = nn.Linear(in_features=self.last_hidden_shape, out_features=1)\n",
    "        self.bottleneck_2 = None\n",
    "\n",
    "\n",
    "    def increase_latentdim(self):\n",
    "        # Combining bottleneck and bottleneck expansion layer and turning off gradients\n",
    "        if self.bottleneck_2:\n",
    "            new_bottleneck = nn.Linear(in_features=self.last_hidden_shape, out_features=self.bottleneck.out_features + 1)\n",
    "            new_bottleneck.weight = nn.Parameter(torch.cat((self.bottleneck.weight, self.bottleneck_2.weight)), requires_grad=False)\n",
    "            new_bottleneck.bias = nn.Parameter(torch.cat((self.bottleneck.bias, self.bottleneck_2.bias)), requires_grad=False)\n",
    "            self.bottleneck = new_bottleneck\n",
    "        # Creating new bottleneck expansion layer\n",
    "        self.bottleneck_2 = nn.Linear(self.last_hidden_shape, 1)\n",
    "        self.bottleneck_2.requires_grad_(requires_grad=True)\n",
    "        # Turning off gradients for all layers in encoder (just in case)\n",
    "        for layer in self.encoder:\n",
    "            layer.requires_grad_(requires_grad=False)\n",
    "        self._recreate_decoder()\n",
    "\n",
    "\n",
    "    def _recreate_decoder(self):\n",
    "        # Copying old decoder to new\n",
    "        new_decoder = nn.ModuleList()\n",
    "        # Increasing first dimension in first layer\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            if i == 0 and isinstance(layer, nn.Linear):\n",
    "                new_decoder.append(nn.Linear(in_features=layer.in_features + 1, out_features=layer.out_features))\n",
    "            else:\n",
    "                new_decoder.append(layer)\n",
    "        # Reinitializing weights\n",
    "        for new_layer in new_decoder:\n",
    "            self.init_weights(new_layer)\n",
    "        \n",
    "        self.decoder = new_decoder\n",
    "\n",
    "\n",
    "    # Reinitialize weights\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):  # Check layer type\n",
    "            nn.init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)  # Bias initialized to 0\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        if not self.bottleneck_2:\n",
    "            x = self.bottleneck(x)\n",
    "            return x\n",
    "        else:\n",
    "            print('yes')\n",
    "            self.current_bn_weight = torch.cat((self.bottleneck.weight, self.bottleneck_2.weight))\n",
    "            self.current_bn_bias = torch.cat((self.bottleneck.bias, self.bottleneck_2.bias))\n",
    "            x = x@self.current_bn_weight.T + self.current_bn_bias\n",
    "            return x\n",
    "\n",
    "\n",
    "    def decode(self, x):\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self.decode(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sizes for the layers\n",
    "layer_sizes = [8, 4]  # Example decreasing sizes for the encoder\n",
    "\n",
    "# Create the encoder ModuleList\n",
    "encoder = nn.ModuleList()\n",
    "for i in range(len(layer_sizes) - 1):\n",
    "    encoder.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "    encoder.append(nn.ReLU())\n",
    "\n",
    "# Create the decoder ModuleList (mirror of the encoder)\n",
    "decoder = nn.ModuleList()\n",
    "for i in range(len(layer_sizes) - 1, 0, -1):\n",
    "    decoder.append(nn.Linear(layer_sizes[i], layer_sizes[i - 1]))\n",
    "    decoder.append(nn.ReLU())\n",
    "\n",
    "# Remove the last ReLU from the decoder (optional, depending on use case)\n",
    "decoder = decoder[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO look into why requires grads are turned on/off weirdly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCAAutoencoder(encoder, decoder, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "encoder\n",
      "\n",
      "True\n",
      "True\n",
      "\n",
      "bottleneck\n",
      "\n",
      "True\n",
      "True\n",
      "\n",
      "decoder\n",
      "\n",
      "True\n",
      "True\n",
      "\n",
      "encoder\n",
      "\n",
      "False\n",
      "False\n",
      "\n",
      "bottleneck\n",
      "\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "\n",
      "decoder\n",
      "\n",
      "True\n",
      "True\n",
      "\n",
      "encoder\n",
      "\n",
      "False\n",
      "False\n",
      "\n",
      "bottleneck\n",
      "\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "\n",
      "decoder\n",
      "\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model = PCAAutoencoder(encoder, decoder, 4)\n",
    "for i in range(3):\n",
    "    print(\"\\nencoder\\n\")\n",
    "    for layer in model.encoder.parameters():\n",
    "        print(layer.requires_grad)\n",
    "    print(\"\\nbottleneck\\n\")\n",
    "    for layer in model.bottleneck.parameters():\n",
    "        print(layer.requires_grad)\n",
    "    if model.bottleneck_2:\n",
    "        for layer in model.bottleneck_2.parameters():\n",
    "            print(layer.requires_grad)\n",
    "    print(\"\\ndecoder\\n\")\n",
    "    for layer in model.decoder.parameters():\n",
    "        print(layer.requires_grad)\n",
    "    model.increase_latentdim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0605,  0.0390, -0.0118,  0.2159],\n",
      "        [-0.3686, -0.2754,  0.0876, -0.2395],\n",
      "        [-0.4354,  0.1305,  0.2570,  0.1123]])\n",
      "False\n",
      "Parameter containing:\n",
      "tensor([-0.3499, -0.0935,  0.3131])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in model.bottleneck.parameters():\n",
    "    print(i)\n",
    "    print(i.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for i in model.encoder.parameters():\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4127,  0.2841, -0.4800, -0.1114],\n",
       "        [-0.4127,  0.2841, -0.4800, -0.1114],\n",
       "        [-0.4127,  0.2841, -0.4800, -0.1114],\n",
       "        [-0.4127,  0.2841, -0.4800, -0.1114]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode(torch.ones(4, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.current_bn_weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(model.current_bn_weight.shape[1]):\n",
    "    print(model.current_bn_weight[:, i].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create two separate tensors\n",
    "tensor_requires_grad = torch.randn(3, requires_grad=True)  # This will track gradients\n",
    "tensor_no_grad = torch.randn(2).detach()  # This will not track gradients\n",
    "\n",
    "# Concatenate them into a single tensor\n",
    "combined_tensor = torch.cat([tensor_requires_grad, tensor_no_grad])\n",
    "\n",
    "# Check requires_grad property\n",
    "print(combined_tensor.requires_grad)  # Output: True, but only parts of it contribute to gradient updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in combined_tensor:\n",
    "    print(i.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.requires_grad: True\n",
      "Gradients of x:\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "\n",
      "Manually checking gradients:\n",
      "x[0] gradient: 1.0 (requires_grad=True)\n",
      "x[1] gradient: 1.0 (requires_grad=True)\n",
      "x[2] gradient: 1.0 (requires_grad=True)\n",
      "x[3] gradient: 1.0 (requires_grad=True)\n",
      "x[4] gradient: 1.0 (requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Create tensors where some elements require gradients and some don't\n",
    "x_requires_grad = torch.randn(3, requires_grad=True)  # Will track gradients\n",
    "x_no_grad = torch.randn(2).detach()  # This part should NOT track gradients\n",
    "\n",
    "# Concatenate into a single tensor\n",
    "x = torch.cat([x_requires_grad, x_no_grad])\n",
    "\n",
    "# **Fix: Explicitly detach the non-gradients part**\n",
    "x[3:].detach()  # Now the last two elements should not receive gradients\n",
    "\n",
    "# Retain grad for debugging\n",
    "x.retain_grad()\n",
    "\n",
    "print(\"x.requires_grad:\", x.requires_grad)  # True, but only some elements should contribute\n",
    "\n",
    "# Step 2: Define a simple loss function (sum of all elements)\n",
    "loss = x.sum()\n",
    "\n",
    "# Step 3: Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Step 4: Display gradients for each element\n",
    "print(\"Gradients of x:\")\n",
    "print(x.grad)  # Now only first 3 elements should have gradients\n",
    "\n",
    "# Step 5: Manually check gradients for each element\n",
    "print(\"\\nManually checking gradients:\")\n",
    "for i, g in enumerate(x.grad):\n",
    "    print(f\"x[{i}] gradient: {g.item()} (requires_grad={x[i].requires_grad})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
