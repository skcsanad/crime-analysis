{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "#from tensorboardX import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PCAAutoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, last_hidden_shape):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList(encoder)  # Ensure encoder is a ModuleList\n",
    "        self.decoder = nn.ModuleList(decoder)  # Ensure decoder is a ModuleList\n",
    "        self.last_hidden_shape = last_hidden_shape\n",
    "        self.bottleneck = nn.ModuleList([nn.Linear(in_features=self.last_hidden_shape, out_features=1),\n",
    "                                         nn.BatchNorm1d(num_features=1, affine=False)])\n",
    "\n",
    "    def increase_latentdim(self):\n",
    "        # Create new bottleneck expansion layer\n",
    "        new_bottleneck = nn.ModuleList([nn.Linear(in_features=self.last_hidden_shape, out_features=self.bottleneck[0].out_features + 1),\n",
    "                                        nn.BatchNorm1d(num_features=self.bottleneck[0].out_features + 1, affine=False)])\n",
    "        # Copying weights while freezing old neurons\n",
    "        with torch.no_grad():\n",
    "            new_bottleneck[0].weight[: self.bottleneck[0].out_features] = self.bottleneck[0].weight\n",
    "            new_bottleneck[0].bias[: self.bottleneck[0].out_features] = self.bottleneck[0].bias\n",
    "\n",
    "        self.bottleneck = new_bottleneck  # Replace the layer\n",
    "        self.bottleneck[0].requires_grad_(True)  # Allow gradients\n",
    "\n",
    "        # Freeze the old neurons using a hook\n",
    "        self.bottleneck[0].weight.register_hook(self._freeze_old_neurons_hook)\n",
    "        self.bottleneck[0].bias.register_hook(self._freeze_old_neurons_hook)\n",
    "\n",
    "        # Turn off gradients for all layers in the encoder (just in case)\n",
    "        for layer in self.encoder:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self._recreate_decoder()\n",
    "\n",
    "    def _freeze_old_neurons_hook(self, grad):\n",
    "        \"\"\"Backward hook: Freeze gradients for old neurons, allowing updates only for new ones\"\"\"\n",
    "        grad[: -1] = 0  # Zero out gradients for old neurons\n",
    "        return grad\n",
    "\n",
    "    def _recreate_decoder(self):\n",
    "        # Copying old decoder to new\n",
    "        new_decoder = nn.ModuleList()\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            if i == 0 and isinstance(layer, nn.Linear):\n",
    "                new_layer = nn.Linear(layer.in_features + 1, layer.out_features)\n",
    "                nn.init.xavier_uniform_(new_layer.weight)\n",
    "                if new_layer.bias is not None:\n",
    "                    nn.init.zeros_(new_layer.bias)\n",
    "                new_decoder.append(new_layer)\n",
    "            else:\n",
    "                new_decoder.append(layer)\n",
    "        \n",
    "        self.decoder = new_decoder  # Ensure it's still a ModuleList\n",
    "\n",
    "    def encode(self, x):\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        for layer in self.bottleneck:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "        return x  # Return the output\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encode(x)\n",
    "        out = self.decode(enc)\n",
    "        return out, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAAE_Loss(nn.Module):\n",
    "    def __init__(self, loss_func, lambda_cov=0.01):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        self.lambda_cov = lambda_cov\n",
    "\n",
    "    \n",
    "    def forward(self, y_hat, y, z):\n",
    "        recon_loss = self.loss_func(y_hat, y)\n",
    "\n",
    "        batch_size, latent_dim = z.shape\n",
    "        z_mean = torch.mean(z, dim=0, keepdim=True)\n",
    "        z_centered = z - z_mean\n",
    "\n",
    "        covariance_matrix = (z_centered.T @ z_centered) / batch_size\n",
    "        covariance_loss = torch.sum(covariance_matrix**2) - torch.sum(torch.diagonal(covariance_matrix)**2)\n",
    "\n",
    "        total_loss = recon_loss + self.lambda_cov * covariance_loss\n",
    "        return total_loss, recon_loss, covariance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, goal_hidden_dim, optimizer, loss_func, epochs, trainloader, testloader, print_every):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    outer_steps = 0\n",
    "    total_steps = 0\n",
    "    total_train_losses, total_test_losses = [], []\n",
    "    if isinstance(loss_func, PCAAE_Loss):\n",
    "        total_train_recon_losses, total_test_recon_losses = [], []\n",
    "        total_train_cov_losses, total_test_cov_losses = [], []\n",
    "    total_min_testloss = np.Inf\n",
    "    hidden_dim = 1\n",
    "    \n",
    "    while hidden_dim != goal_hidden_dim:\n",
    "        if not outer_steps == 0:\n",
    "            # Increasing latent space\n",
    "            model.increase_latentdim()\n",
    "            hidden_dim += 1\n",
    "            model.to(device)\n",
    "\n",
    "        outer_steps += 1\n",
    "        print(f\"Training with hidden dim: {hidden_dim}\")\n",
    "        steps = 0\n",
    "        train_losses, test_losses = [], []\n",
    "        if isinstance(loss_func, PCAAE_Loss):\n",
    "            train_recon_losses, test_recon_losses = [], []\n",
    "            train_cov_losses, test_cov_losses = [], []\n",
    "        min_test_loss = np.Inf\n",
    "\n",
    "        # Training loop\n",
    "        for e in range(epochs):\n",
    "            running_loss = 0\n",
    "            # Only for printing it\n",
    "            running_loss_ = 0\n",
    "            if isinstance(loss_func, PCAAE_Loss):\n",
    "                running_recon_loss = 0\n",
    "                running_cov_loss = 0\n",
    "                running_recon_loss_ = 0\n",
    "                running_cov_loss_ = 0\n",
    "\n",
    "            for X, y in trainloader:\n",
    "                steps += 1\n",
    "                total_steps += 1\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                y_hat, hidden = model(X)\n",
    "                if isinstance(loss_func, PCAAE_Loss):\n",
    "                    loss, recon_loss, cov_loss = loss_func(y_hat, y, hidden)\n",
    "                    running_recon_loss += recon_loss.item()*X.size(0)\n",
    "                    running_cov_loss += cov_loss.item()*X.size(0)\n",
    "                    running_recon_loss_ += recon_loss.item()\n",
    "                    running_cov_loss_ += cov_loss.item()\n",
    "                else:\n",
    "                    loss = loss_func(y_hat, y)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()*X.size(0)\n",
    "                running_loss_ += loss.item()\n",
    "\n",
    "                if steps % print_every == 0:\n",
    "                    if isinstance(loss_func, PCAAE_Loss):\n",
    "                        print(f\"Epoch: {e + 1}/{epochs}, Step {steps}, Train loss: {running_loss_/print_every:.3f} \" \n",
    "                              f\"Train reconstruction loss: {running_recon_loss_/print_every:.3f} \"\n",
    "                              f\"Train covariance loss: {running_cov_loss_/print_every:.3f}\")\n",
    "                    else:\n",
    "                        print(f\"Epoch: {e + 1}/{epochs}, Step {steps}, Train loss: {running_loss_/print_every:.3f}\")\n",
    "                    running_loss_ = 0\n",
    "                    running_recon_loss_ = 0\n",
    "                    running_cov_loss_ = 0\n",
    "\n",
    "            # Running model on the test data  \n",
    "            else:\n",
    "                running_testloss = 0\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    for X, y in testloader:\n",
    "                        X, y = X.to(device), y.to(device)\n",
    "                        y_hat, hidden = model(X)\n",
    "                        test_loss = loss_func(y_hat, y, hidden) if isinstance(loss_func, PCAAE_Loss) else loss_func(y_hat, y)\n",
    "                        running_testloss += test_loss.item()*X.size(0)\n",
    "                model.train()\n",
    "\n",
    "                train_losses.append(running_loss/len(trainloader.dataset))\n",
    "                total_train_losses.append(running_loss/len(trainloader.dataset))\n",
    "                test_losses.append(running_testloss/len(testloader.dataset))\n",
    "                total_test_losses.append(running_testloss/len(testloader.dataset))\n",
    "\n",
    "                # Saving model when test loss improved\n",
    "                if test_losses[-1] <= min_test_loss:\n",
    "                    print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(min_test_loss,test_losses[-1]))\n",
    "                    torch.save(model.state_dict(), f'PCAAE_hidden_dim{hidden_dim}.pt')\n",
    "                    min_test_loss = test_losses[-1]\n",
    "                \n",
    "                print(f'Epoch {e+1}/{epochs}, Train Loss: {running_loss/len(trainloader.dataset):.3f}, Test Loss: {running_testloss/len(testloader.dataset):.3f}')\n",
    "\n",
    "    return total_train_losses, total_test_losses, total_steps              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for tabular data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "inputFeature = pd.read_csv('../Data/NIBRS_ND_2021/processed/input.csv', index_col='Unnamed: 0')\n",
    "# Separating numerical and categorical features\n",
    "numerical_features=['population','victim_seq_num','age_num_victim','incident_hour','incident_month','incident_day','incident_dayofmonth','incident_weekofyear']\n",
    "categorical_features = ['resident_status_code','race_desc_victim',\n",
    "'ethnicity_name_victim','pub_agency_name','offense_name','location_name','weapon_name'\n",
    ",'injury_name','relationship_name','incident_isweekend']\n",
    "# Onehot-encoding categorical features\n",
    "inputFeature_1h = pd.get_dummies(inputFeature, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object columns to numeric if they represent categories\n",
    "for column in inputFeature_1h.select_dtypes(include=['object']):\n",
    "    inputFeature_1h[column] = inputFeature_1h[column].astype('category').cat.codes\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(inputFeature_1h, test_size=0.1, random_state=42)\n",
    "\n",
    "# Normalizing numerical features\n",
    "for feature in numerical_features:\n",
    "  train[feature] = (train[feature] - train[feature].min()) / (train[feature].max() - train[feature].min())\n",
    "  test[feature] = (test[feature] - test[feature].min()) / (test[feature].max() - test[feature].min())\n",
    "\n",
    "# Converting data to tensors\n",
    "X_train = torch.nan_to_num(torch.Tensor(train.values.astype(np.float32)))\n",
    "y_train = torch.nan_to_num(torch.Tensor(train.values.astype(np.float32)))\n",
    "\n",
    "X_test = torch.nan_to_num(torch.Tensor(test.values.astype(np.float32)))\n",
    "y_test = torch.nan_to_num(torch.Tensor(test.values.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "trainset = MyDataset(X_train, y_train)\n",
    "testset = MyDataset(X_test, y_test)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_loss(z):\n",
    "    batch_size, latent_dim = z.shape\n",
    "    z_mean = torch.mean(z, dim=0, keepdim=True)\n",
    "    z_centered = z - z_mean\n",
    "\n",
    "    cov_matrix = (z_centered.T @ z_centered) / batch_size\n",
    "    cov_loss = torch.sum(cov_matrix**2) - torch.sum(torch.diagonal(cov_matrix)**2)\n",
    "    return cov_loss\n",
    "\n",
    "def complete_loss_func(y_hat, y, z, lambda_cov=0.01):\n",
    "    loss = nn.MSELoss()\n",
    "    reconstruction_loss = loss(y_hat, y)\n",
    "    cov_loss = covariance_loss(z)\n",
    "    total_loss = reconstruction_loss + lambda_cov * cov_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sizes for the layers\n",
    "layer_sizes = [227, 64, 32]  # Example decreasing sizes for the encoder\n",
    "\n",
    "# Create the encoder ModuleList\n",
    "encoder = nn.ModuleList()\n",
    "for i in range(len(layer_sizes) - 1):\n",
    "    encoder.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "    encoder.append(nn.ReLU())\n",
    "\n",
    "# Create the decoder ModuleList (mirror of the encoder)\n",
    "decoder = nn.ModuleList()\n",
    "decoder.append(nn.Linear(1, layer_sizes[len(layer_sizes) - 1]))\n",
    "for i in range(len(layer_sizes) - 1, 0, -1):\n",
    "    decoder.append(nn.Linear(layer_sizes[i], layer_sizes[i - 1]))\n",
    "    decoder.append(nn.ReLU())\n",
    "\n",
    "# Remove the last ReLU from the decoder (optional, depending on use case)\n",
    "decoder = decoder[:-1]\n",
    "criterion = complete_loss_func\n",
    "model = PCAAutoencoder(encoder, decoder, 32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 25\n",
    "print_every = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden dim: 1\n",
      "Epoch: 1/25, Step 40, Train loss: 0.044\n",
      "Epoch: 1/25, Step 80, Train loss: 0.024\n",
      "Epoch: 1/25, Step 120, Train loss: 0.023\n",
      "Epoch: 1/25, Step 160, Train loss: 0.023\n",
      "Epoch: 1/25, Step 200, Train loss: 0.023\n",
      "Epoch: 1/25, Step 240, Train loss: 0.023\n",
      "Epoch: 1/25, Step 280, Train loss: 0.023\n",
      "Epoch: 1/25, Step 320, Train loss: 0.023\n",
      "Epoch: 1/25, Step 360, Train loss: 0.023\n",
      "Epoch: 1/25, Step 400, Train loss: 0.023\n",
      "Epoch: 1/25, Step 440, Train loss: 0.023\n",
      "Epoch: 1/25, Step 480, Train loss: 0.023\n",
      "Test loss decreased (inf --> 0.022674).  Saving model ...\n",
      "Epoch 1/25, Train Loss: 0.025, Test Loss: 0.023\n",
      "Epoch: 2/25, Step 520, Train loss: 0.023\n",
      "Epoch: 2/25, Step 560, Train loss: 0.023\n",
      "Epoch: 2/25, Step 600, Train loss: 0.023\n",
      "Epoch: 2/25, Step 640, Train loss: 0.023\n",
      "Epoch: 2/25, Step 680, Train loss: 0.023\n",
      "Epoch: 2/25, Step 720, Train loss: 0.023\n",
      "Epoch: 2/25, Step 760, Train loss: 0.023\n",
      "Epoch: 2/25, Step 800, Train loss: 0.023\n",
      "Epoch: 2/25, Step 840, Train loss: 0.023\n",
      "Epoch: 2/25, Step 880, Train loss: 0.023\n",
      "Epoch: 2/25, Step 920, Train loss: 0.023\n",
      "Epoch: 2/25, Step 960, Train loss: 0.022\n",
      "Test loss decreased (0.022674 --> 0.022492).  Saving model ...\n",
      "Epoch 2/25, Train Loss: 0.023, Test Loss: 0.022\n",
      "Epoch: 3/25, Step 1000, Train loss: 0.023\n",
      "Epoch: 3/25, Step 1040, Train loss: 0.023\n",
      "Epoch: 3/25, Step 1080, Train loss: 0.023\n",
      "Epoch: 3/25, Step 1120, Train loss: 0.022\n",
      "Epoch: 3/25, Step 1160, Train loss: 0.022\n",
      "Epoch: 3/25, Step 1200, Train loss: 0.022\n",
      "Epoch: 3/25, Step 1240, Train loss: 0.023\n",
      "Epoch: 3/25, Step 1280, Train loss: 0.023\n",
      "Epoch: 3/25, Step 1320, Train loss: 0.023\n",
      "Epoch: 3/25, Step 1360, Train loss: 0.023\n",
      "Epoch: 3/25, Step 1400, Train loss: 0.023\n",
      "Epoch: 3/25, Step 1440, Train loss: 0.023\n",
      "Test loss decreased (0.022492 --> 0.022349).  Saving model ...\n",
      "Epoch 3/25, Train Loss: 0.023, Test Loss: 0.022\n",
      "Epoch: 4/25, Step 1480, Train loss: 0.023\n",
      "Epoch: 4/25, Step 1520, Train loss: 0.023\n",
      "Epoch: 4/25, Step 1560, Train loss: 0.023\n",
      "Epoch: 4/25, Step 1600, Train loss: 0.022\n",
      "Epoch: 4/25, Step 1640, Train loss: 0.022\n",
      "Epoch: 4/25, Step 1680, Train loss: 0.023\n",
      "Epoch: 4/25, Step 1720, Train loss: 0.022\n",
      "Epoch: 4/25, Step 1760, Train loss: 0.022\n",
      "Epoch: 4/25, Step 1800, Train loss: 0.023\n",
      "Epoch: 4/25, Step 1840, Train loss: 0.022\n",
      "Epoch: 4/25, Step 1880, Train loss: 0.022\n",
      "Epoch: 4/25, Step 1920, Train loss: 0.022\n",
      "Test loss decreased (0.022349 --> 0.021985).  Saving model ...\n",
      "Epoch 4/25, Train Loss: 0.022, Test Loss: 0.022\n",
      "Epoch: 5/25, Step 1960, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2000, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2040, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2080, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2120, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2160, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2200, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2240, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2280, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2320, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2360, Train loss: 0.022\n",
      "Epoch: 5/25, Step 2400, Train loss: 0.022\n",
      "Test loss decreased (0.021985 --> 0.021568).  Saving model ...\n",
      "Epoch 5/25, Train Loss: 0.022, Test Loss: 0.022\n",
      "Epoch: 6/25, Step 2440, Train loss: 0.022\n",
      "Epoch: 6/25, Step 2480, Train loss: 0.022\n",
      "Epoch: 6/25, Step 2520, Train loss: 0.022\n",
      "Epoch: 6/25, Step 2560, Train loss: 0.022\n",
      "Epoch: 6/25, Step 2600, Train loss: 0.022\n",
      "Epoch: 6/25, Step 2640, Train loss: 0.022\n",
      "Epoch: 6/25, Step 2680, Train loss: 0.022\n",
      "Epoch: 6/25, Step 2720, Train loss: 0.022\n",
      "Epoch: 6/25, Step 2760, Train loss: 0.022\n",
      "Epoch: 6/25, Step 2800, Train loss: 0.022\n",
      "Epoch: 6/25, Step 2840, Train loss: 0.021\n",
      "Epoch: 6/25, Step 2880, Train loss: 0.022\n",
      "Test loss decreased (0.021568 --> 0.021499).  Saving model ...\n",
      "Epoch 6/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 7/25, Step 2920, Train loss: 0.022\n",
      "Epoch: 7/25, Step 2960, Train loss: 0.022\n",
      "Epoch: 7/25, Step 3000, Train loss: 0.022\n",
      "Epoch: 7/25, Step 3040, Train loss: 0.022\n",
      "Epoch: 7/25, Step 3080, Train loss: 0.022\n",
      "Epoch: 7/25, Step 3120, Train loss: 0.022\n",
      "Epoch: 7/25, Step 3160, Train loss: 0.022\n",
      "Epoch: 7/25, Step 3200, Train loss: 0.022\n",
      "Epoch: 7/25, Step 3240, Train loss: 0.022\n",
      "Epoch: 7/25, Step 3280, Train loss: 0.022\n",
      "Epoch: 7/25, Step 3320, Train loss: 0.022\n",
      "Epoch: 7/25, Step 3360, Train loss: 0.022\n",
      "Epoch 7/25, Train Loss: 0.022, Test Loss: 0.022\n",
      "Epoch: 8/25, Step 3400, Train loss: 0.022\n",
      "Epoch: 8/25, Step 3440, Train loss: 0.022\n",
      "Epoch: 8/25, Step 3480, Train loss: 0.022\n",
      "Epoch: 8/25, Step 3520, Train loss: 0.022\n",
      "Epoch: 8/25, Step 3560, Train loss: 0.022\n",
      "Epoch: 8/25, Step 3600, Train loss: 0.022\n",
      "Epoch: 8/25, Step 3640, Train loss: 0.022\n",
      "Epoch: 8/25, Step 3680, Train loss: 0.022\n",
      "Epoch: 8/25, Step 3720, Train loss: 0.022\n",
      "Epoch: 8/25, Step 3760, Train loss: 0.022\n",
      "Epoch: 8/25, Step 3800, Train loss: 0.021\n",
      "Epoch: 8/25, Step 3840, Train loss: 0.022\n",
      "Test loss decreased (0.021499 --> 0.021355).  Saving model ...\n",
      "Epoch 8/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 9/25, Step 3880, Train loss: 0.022\n",
      "Epoch: 9/25, Step 3920, Train loss: 0.022\n",
      "Epoch: 9/25, Step 3960, Train loss: 0.022\n",
      "Epoch: 9/25, Step 4000, Train loss: 0.022\n",
      "Epoch: 9/25, Step 4040, Train loss: 0.022\n",
      "Epoch: 9/25, Step 4080, Train loss: 0.021\n",
      "Epoch: 9/25, Step 4120, Train loss: 0.021\n",
      "Epoch: 9/25, Step 4160, Train loss: 0.022\n",
      "Epoch: 9/25, Step 4200, Train loss: 0.022\n",
      "Epoch: 9/25, Step 4240, Train loss: 0.022\n",
      "Epoch: 9/25, Step 4280, Train loss: 0.022\n",
      "Epoch: 9/25, Step 4320, Train loss: 0.021\n",
      "Epoch 9/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 10/25, Step 4360, Train loss: 0.022\n",
      "Epoch: 10/25, Step 4400, Train loss: 0.022\n",
      "Epoch: 10/25, Step 4440, Train loss: 0.021\n",
      "Epoch: 10/25, Step 4480, Train loss: 0.021\n",
      "Epoch: 10/25, Step 4520, Train loss: 0.022\n",
      "Epoch: 10/25, Step 4560, Train loss: 0.022\n",
      "Epoch: 10/25, Step 4600, Train loss: 0.022\n",
      "Epoch: 10/25, Step 4640, Train loss: 0.022\n",
      "Epoch: 10/25, Step 4680, Train loss: 0.022\n",
      "Epoch: 10/25, Step 4720, Train loss: 0.022\n",
      "Epoch: 10/25, Step 4760, Train loss: 0.022\n",
      "Epoch: 10/25, Step 4800, Train loss: 0.022\n",
      "Epoch 10/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 11/25, Step 4840, Train loss: 0.022\n",
      "Epoch: 11/25, Step 4880, Train loss: 0.021\n",
      "Epoch: 11/25, Step 4920, Train loss: 0.022\n",
      "Epoch: 11/25, Step 4960, Train loss: 0.022\n",
      "Epoch: 11/25, Step 5000, Train loss: 0.022\n",
      "Epoch: 11/25, Step 5040, Train loss: 0.021\n",
      "Epoch: 11/25, Step 5080, Train loss: 0.022\n",
      "Epoch: 11/25, Step 5120, Train loss: 0.021\n",
      "Epoch: 11/25, Step 5160, Train loss: 0.022\n",
      "Epoch: 11/25, Step 5200, Train loss: 0.022\n",
      "Epoch: 11/25, Step 5240, Train loss: 0.022\n",
      "Epoch: 11/25, Step 5280, Train loss: 0.022\n",
      "Test loss decreased (0.021355 --> 0.021311).  Saving model ...\n",
      "Epoch 11/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 12/25, Step 5320, Train loss: 0.021\n",
      "Epoch: 12/25, Step 5360, Train loss: 0.022\n",
      "Epoch: 12/25, Step 5400, Train loss: 0.022\n",
      "Epoch: 12/25, Step 5440, Train loss: 0.022\n",
      "Epoch: 12/25, Step 5480, Train loss: 0.022\n",
      "Epoch: 12/25, Step 5520, Train loss: 0.022\n",
      "Epoch: 12/25, Step 5560, Train loss: 0.022\n",
      "Epoch: 12/25, Step 5600, Train loss: 0.022\n",
      "Epoch: 12/25, Step 5640, Train loss: 0.022\n",
      "Epoch: 12/25, Step 5680, Train loss: 0.022\n",
      "Epoch: 12/25, Step 5720, Train loss: 0.021\n",
      "Epoch: 12/25, Step 5760, Train loss: 0.021\n",
      "Test loss decreased (0.021311 --> 0.021299).  Saving model ...\n",
      "Epoch 12/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 13/25, Step 5800, Train loss: 0.022\n",
      "Epoch: 13/25, Step 5840, Train loss: 0.021\n",
      "Epoch: 13/25, Step 5880, Train loss: 0.022\n",
      "Epoch: 13/25, Step 5920, Train loss: 0.022\n",
      "Epoch: 13/25, Step 5960, Train loss: 0.022\n",
      "Epoch: 13/25, Step 6000, Train loss: 0.022\n",
      "Epoch: 13/25, Step 6040, Train loss: 0.022\n",
      "Epoch: 13/25, Step 6080, Train loss: 0.022\n",
      "Epoch: 13/25, Step 6120, Train loss: 0.022\n",
      "Epoch: 13/25, Step 6160, Train loss: 0.021\n",
      "Epoch: 13/25, Step 6200, Train loss: 0.022\n",
      "Epoch: 13/25, Step 6240, Train loss: 0.021\n",
      "Test loss decreased (0.021299 --> 0.021275).  Saving model ...\n",
      "Epoch 13/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 14/25, Step 6280, Train loss: 0.022\n",
      "Epoch: 14/25, Step 6320, Train loss: 0.022\n",
      "Epoch: 14/25, Step 6360, Train loss: 0.022\n",
      "Epoch: 14/25, Step 6400, Train loss: 0.022\n",
      "Epoch: 14/25, Step 6440, Train loss: 0.022\n",
      "Epoch: 14/25, Step 6480, Train loss: 0.022\n",
      "Epoch: 14/25, Step 6520, Train loss: 0.022\n",
      "Epoch: 14/25, Step 6560, Train loss: 0.021\n",
      "Epoch: 14/25, Step 6600, Train loss: 0.022\n",
      "Epoch: 14/25, Step 6640, Train loss: 0.022\n",
      "Epoch: 14/25, Step 6680, Train loss: 0.021\n",
      "Epoch: 14/25, Step 6720, Train loss: 0.021\n",
      "Epoch 14/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 15/25, Step 6760, Train loss: 0.022\n",
      "Epoch: 15/25, Step 6800, Train loss: 0.022\n",
      "Epoch: 15/25, Step 6840, Train loss: 0.022\n",
      "Epoch: 15/25, Step 6880, Train loss: 0.022\n",
      "Epoch: 15/25, Step 6920, Train loss: 0.021\n",
      "Epoch: 15/25, Step 6960, Train loss: 0.021\n",
      "Epoch: 15/25, Step 7000, Train loss: 0.022\n",
      "Epoch: 15/25, Step 7040, Train loss: 0.021\n",
      "Epoch: 15/25, Step 7080, Train loss: 0.022\n",
      "Epoch: 15/25, Step 7120, Train loss: 0.022\n",
      "Epoch: 15/25, Step 7160, Train loss: 0.022\n",
      "Epoch: 15/25, Step 7200, Train loss: 0.021\n",
      "Test loss decreased (0.021275 --> 0.021207).  Saving model ...\n",
      "Epoch 15/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 16/25, Step 7240, Train loss: 0.022\n",
      "Epoch: 16/25, Step 7280, Train loss: 0.021\n",
      "Epoch: 16/25, Step 7320, Train loss: 0.022\n",
      "Epoch: 16/25, Step 7360, Train loss: 0.022\n",
      "Epoch: 16/25, Step 7400, Train loss: 0.021\n",
      "Epoch: 16/25, Step 7440, Train loss: 0.022\n",
      "Epoch: 16/25, Step 7480, Train loss: 0.022\n",
      "Epoch: 16/25, Step 7520, Train loss: 0.021\n",
      "Epoch: 16/25, Step 7560, Train loss: 0.021\n",
      "Epoch: 16/25, Step 7600, Train loss: 0.022\n",
      "Epoch: 16/25, Step 7640, Train loss: 0.022\n",
      "Epoch: 16/25, Step 7680, Train loss: 0.021\n",
      "Epoch 16/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 17/25, Step 7720, Train loss: 0.022\n",
      "Epoch: 17/25, Step 7760, Train loss: 0.022\n",
      "Epoch: 17/25, Step 7800, Train loss: 0.022\n",
      "Epoch: 17/25, Step 7840, Train loss: 0.022\n",
      "Epoch: 17/25, Step 7880, Train loss: 0.022\n",
      "Epoch: 17/25, Step 7920, Train loss: 0.021\n",
      "Epoch: 17/25, Step 7960, Train loss: 0.022\n",
      "Epoch: 17/25, Step 8000, Train loss: 0.022\n",
      "Epoch: 17/25, Step 8040, Train loss: 0.021\n",
      "Epoch: 17/25, Step 8080, Train loss: 0.022\n",
      "Epoch: 17/25, Step 8120, Train loss: 0.022\n",
      "Epoch: 17/25, Step 8160, Train loss: 0.022\n",
      "Epoch 17/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 18/25, Step 8200, Train loss: 0.021\n",
      "Epoch: 18/25, Step 8240, Train loss: 0.022\n",
      "Epoch: 18/25, Step 8280, Train loss: 0.021\n",
      "Epoch: 18/25, Step 8320, Train loss: 0.022\n",
      "Epoch: 18/25, Step 8360, Train loss: 0.021\n",
      "Epoch: 18/25, Step 8400, Train loss: 0.022\n",
      "Epoch: 18/25, Step 8440, Train loss: 0.021\n",
      "Epoch: 18/25, Step 8480, Train loss: 0.021\n",
      "Epoch: 18/25, Step 8520, Train loss: 0.021\n",
      "Epoch: 18/25, Step 8560, Train loss: 0.022\n",
      "Epoch: 18/25, Step 8600, Train loss: 0.022\n",
      "Epoch: 18/25, Step 8640, Train loss: 0.022\n",
      "Test loss decreased (0.021207 --> 0.021033).  Saving model ...\n",
      "Epoch 18/25, Train Loss: 0.022, Test Loss: 0.021\n",
      "Epoch: 19/25, Step 8680, Train loss: 0.022\n",
      "Epoch: 19/25, Step 8720, Train loss: 0.022\n",
      "Epoch: 19/25, Step 8760, Train loss: 0.022\n",
      "Epoch: 19/25, Step 8800, Train loss: 0.021\n",
      "Epoch: 19/25, Step 8840, Train loss: 0.022\n",
      "Epoch: 19/25, Step 8880, Train loss: 0.022\n",
      "Epoch: 19/25, Step 8920, Train loss: 0.021\n",
      "Epoch: 19/25, Step 8960, Train loss: 0.021\n",
      "Epoch: 19/25, Step 9000, Train loss: 0.021\n",
      "Epoch: 19/25, Step 9040, Train loss: 0.022\n",
      "Epoch: 19/25, Step 9080, Train loss: 0.022\n",
      "Epoch: 19/25, Step 9120, Train loss: 0.021\n",
      "Test loss decreased (0.021033 --> 0.020987).  Saving model ...\n",
      "Epoch 19/25, Train Loss: 0.021, Test Loss: 0.021\n",
      "Epoch: 20/25, Step 9160, Train loss: 0.021\n",
      "Epoch: 20/25, Step 9200, Train loss: 0.021\n",
      "Epoch: 20/25, Step 9240, Train loss: 0.021\n",
      "Epoch: 20/25, Step 9280, Train loss: 0.021\n",
      "Epoch: 20/25, Step 9320, Train loss: 0.021\n",
      "Epoch: 20/25, Step 9360, Train loss: 0.021\n",
      "Epoch: 20/25, Step 9400, Train loss: 0.021\n",
      "Epoch: 20/25, Step 9440, Train loss: 0.022\n",
      "Epoch: 20/25, Step 9480, Train loss: 0.021\n",
      "Epoch: 20/25, Step 9520, Train loss: 0.022\n",
      "Epoch: 20/25, Step 9560, Train loss: 0.021\n",
      "Epoch: 20/25, Step 9600, Train loss: 0.021\n",
      "Test loss decreased (0.020987 --> 0.020962).  Saving model ...\n",
      "Epoch 20/25, Train Loss: 0.021, Test Loss: 0.021\n",
      "Epoch: 21/25, Step 9640, Train loss: 0.021\n",
      "Epoch: 21/25, Step 9680, Train loss: 0.022\n",
      "Epoch: 21/25, Step 9720, Train loss: 0.021\n",
      "Epoch: 21/25, Step 9760, Train loss: 0.021\n",
      "Epoch: 21/25, Step 9800, Train loss: 0.022\n",
      "Epoch: 21/25, Step 9840, Train loss: 0.021\n",
      "Epoch: 21/25, Step 9880, Train loss: 0.021\n",
      "Epoch: 21/25, Step 9920, Train loss: 0.021\n",
      "Epoch: 21/25, Step 9960, Train loss: 0.021\n",
      "Epoch: 21/25, Step 10000, Train loss: 0.021\n",
      "Epoch: 21/25, Step 10040, Train loss: 0.022\n",
      "Epoch: 21/25, Step 10080, Train loss: 0.021\n",
      "Test loss decreased (0.020962 --> 0.020867).  Saving model ...\n",
      "Epoch 21/25, Train Loss: 0.021, Test Loss: 0.021\n",
      "Epoch: 22/25, Step 10120, Train loss: 0.021\n",
      "Epoch: 22/25, Step 10160, Train loss: 0.021\n",
      "Epoch: 22/25, Step 10200, Train loss: 0.022\n",
      "Epoch: 22/25, Step 10240, Train loss: 0.022\n",
      "Epoch: 22/25, Step 10280, Train loss: 0.021\n",
      "Epoch: 22/25, Step 10320, Train loss: 0.022\n",
      "Epoch: 22/25, Step 10360, Train loss: 0.022\n",
      "Epoch: 22/25, Step 10400, Train loss: 0.021\n",
      "Epoch: 22/25, Step 10440, Train loss: 0.021\n",
      "Epoch: 22/25, Step 10480, Train loss: 0.022\n",
      "Epoch: 22/25, Step 10520, Train loss: 0.021\n",
      "Epoch: 22/25, Step 10560, Train loss: 0.021\n",
      "Test loss decreased (0.020867 --> 0.020835).  Saving model ...\n",
      "Epoch 22/25, Train Loss: 0.021, Test Loss: 0.021\n",
      "Epoch: 23/25, Step 10600, Train loss: 0.021\n",
      "Epoch: 23/25, Step 10640, Train loss: 0.021\n",
      "Epoch: 23/25, Step 10680, Train loss: 0.022\n",
      "Epoch: 23/25, Step 10720, Train loss: 0.021\n",
      "Epoch: 23/25, Step 10760, Train loss: 0.022\n",
      "Epoch: 23/25, Step 10800, Train loss: 0.021\n",
      "Epoch: 23/25, Step 10840, Train loss: 0.021\n",
      "Epoch: 23/25, Step 10880, Train loss: 0.021\n",
      "Epoch: 23/25, Step 10920, Train loss: 0.022\n",
      "Epoch: 23/25, Step 10960, Train loss: 0.021\n",
      "Epoch: 23/25, Step 11000, Train loss: 0.021\n",
      "Epoch: 23/25, Step 11040, Train loss: 0.022\n",
      "Epoch 23/25, Train Loss: 0.021, Test Loss: 0.021\n",
      "Epoch: 24/25, Step 11080, Train loss: 0.021\n",
      "Epoch: 24/25, Step 11120, Train loss: 0.021\n",
      "Epoch: 24/25, Step 11160, Train loss: 0.021\n",
      "Epoch: 24/25, Step 11200, Train loss: 0.021\n",
      "Epoch: 24/25, Step 11240, Train loss: 0.021\n",
      "Epoch: 24/25, Step 11280, Train loss: 0.021\n",
      "Epoch: 24/25, Step 11320, Train loss: 0.021\n",
      "Epoch: 24/25, Step 11360, Train loss: 0.021\n",
      "Epoch: 24/25, Step 11400, Train loss: 0.021\n",
      "Epoch: 24/25, Step 11440, Train loss: 0.021\n",
      "Epoch: 24/25, Step 11480, Train loss: 0.022\n",
      "Epoch: 24/25, Step 11520, Train loss: 0.021\n",
      "Test loss decreased (0.020835 --> 0.020751).  Saving model ...\n",
      "Epoch 24/25, Train Loss: 0.021, Test Loss: 0.021\n",
      "Epoch: 25/25, Step 11560, Train loss: 0.021\n",
      "Epoch: 25/25, Step 11600, Train loss: 0.021\n",
      "Epoch: 25/25, Step 11640, Train loss: 0.021\n",
      "Epoch: 25/25, Step 11680, Train loss: 0.021\n",
      "Epoch: 25/25, Step 11720, Train loss: 0.021\n",
      "Epoch: 25/25, Step 11760, Train loss: 0.021\n",
      "Epoch: 25/25, Step 11800, Train loss: 0.021\n",
      "Epoch: 25/25, Step 11840, Train loss: 0.021\n",
      "Epoch: 25/25, Step 11880, Train loss: 0.021\n",
      "Epoch: 25/25, Step 11920, Train loss: 0.021\n",
      "Epoch: 25/25, Step 11960, Train loss: 0.022\n",
      "Epoch: 25/25, Step 12000, Train loss: 0.021\n",
      "Epoch 25/25, Train Loss: 0.021, Test Loss: 0.021\n",
      "Training with hidden dim: 2\n",
      "Epoch: 1/25, Step 40, Train loss: 0.031\n",
      "Epoch: 1/25, Step 80, Train loss: 0.028\n",
      "Epoch: 1/25, Step 120, Train loss: 0.027\n",
      "Epoch: 1/25, Step 160, Train loss: 0.027\n",
      "Epoch: 1/25, Step 200, Train loss: 0.026\n",
      "Epoch: 1/25, Step 240, Train loss: 0.026\n",
      "Epoch: 1/25, Step 280, Train loss: 0.026\n",
      "Epoch: 1/25, Step 320, Train loss: 0.026\n",
      "Epoch: 1/25, Step 360, Train loss: 0.026\n",
      "Epoch: 1/25, Step 400, Train loss: 0.025\n",
      "Epoch: 1/25, Step 440, Train loss: 0.026\n",
      "Epoch: 1/25, Step 480, Train loss: 0.027\n",
      "Test loss decreased (inf --> 0.024744).  Saving model ...\n",
      "Epoch 1/25, Train Loss: 0.027, Test Loss: 0.025\n",
      "Epoch: 2/25, Step 520, Train loss: 0.026\n",
      "Epoch: 2/25, Step 560, Train loss: 0.026\n",
      "Epoch: 2/25, Step 600, Train loss: 0.026\n",
      "Epoch: 2/25, Step 640, Train loss: 0.026\n",
      "Epoch: 2/25, Step 680, Train loss: 0.026\n",
      "Epoch: 2/25, Step 720, Train loss: 0.026\n",
      "Epoch: 2/25, Step 760, Train loss: 0.025\n",
      "Epoch: 2/25, Step 800, Train loss: 0.026\n",
      "Epoch: 2/25, Step 840, Train loss: 0.025\n",
      "Epoch: 2/25, Step 880, Train loss: 0.025\n",
      "Epoch: 2/25, Step 920, Train loss: 0.025\n",
      "Epoch: 2/25, Step 960, Train loss: 0.026\n",
      "Test loss decreased (0.024744 --> 0.024244).  Saving model ...\n",
      "Epoch 2/25, Train Loss: 0.026, Test Loss: 0.024\n",
      "Epoch: 3/25, Step 1000, Train loss: 0.026\n",
      "Epoch: 3/25, Step 1040, Train loss: 0.026\n",
      "Epoch: 3/25, Step 1080, Train loss: 0.026\n",
      "Epoch: 3/25, Step 1120, Train loss: 0.026\n",
      "Epoch: 3/25, Step 1160, Train loss: 0.026\n",
      "Epoch: 3/25, Step 1200, Train loss: 0.025\n",
      "Epoch: 3/25, Step 1240, Train loss: 0.025\n",
      "Epoch: 3/25, Step 1280, Train loss: 0.025\n",
      "Epoch: 3/25, Step 1320, Train loss: 0.027\n",
      "Epoch: 3/25, Step 1360, Train loss: 0.025\n",
      "Epoch: 3/25, Step 1400, Train loss: 0.025\n",
      "Epoch: 3/25, Step 1440, Train loss: 0.025\n",
      "Test loss decreased (0.024244 --> 0.024068).  Saving model ...\n",
      "Epoch 3/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 4/25, Step 1480, Train loss: 0.025\n",
      "Epoch: 4/25, Step 1520, Train loss: 0.025\n",
      "Epoch: 4/25, Step 1560, Train loss: 0.025\n",
      "Epoch: 4/25, Step 1600, Train loss: 0.026\n",
      "Epoch: 4/25, Step 1640, Train loss: 0.025\n",
      "Epoch: 4/25, Step 1680, Train loss: 0.026\n",
      "Epoch: 4/25, Step 1720, Train loss: 0.025\n",
      "Epoch: 4/25, Step 1760, Train loss: 0.025\n",
      "Epoch: 4/25, Step 1800, Train loss: 0.025\n",
      "Epoch: 4/25, Step 1840, Train loss: 0.025\n",
      "Epoch: 4/25, Step 1880, Train loss: 0.026\n",
      "Epoch: 4/25, Step 1920, Train loss: 0.025\n",
      "Epoch 4/25, Train Loss: 0.025, Test Loss: 0.025\n",
      "Epoch: 5/25, Step 1960, Train loss: 0.025\n",
      "Epoch: 5/25, Step 2000, Train loss: 0.025\n",
      "Epoch: 5/25, Step 2040, Train loss: 0.025\n",
      "Epoch: 5/25, Step 2080, Train loss: 0.025\n",
      "Epoch: 5/25, Step 2120, Train loss: 0.026\n",
      "Epoch: 5/25, Step 2160, Train loss: 0.025\n",
      "Epoch: 5/25, Step 2200, Train loss: 0.025\n",
      "Epoch: 5/25, Step 2240, Train loss: 0.026\n",
      "Epoch: 5/25, Step 2280, Train loss: 0.024\n",
      "Epoch: 5/25, Step 2320, Train loss: 0.026\n",
      "Epoch: 5/25, Step 2360, Train loss: 0.026\n",
      "Epoch: 5/25, Step 2400, Train loss: 0.026\n",
      "Epoch 5/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 6/25, Step 2440, Train loss: 0.026\n",
      "Epoch: 6/25, Step 2480, Train loss: 0.026\n",
      "Epoch: 6/25, Step 2520, Train loss: 0.025\n",
      "Epoch: 6/25, Step 2560, Train loss: 0.026\n",
      "Epoch: 6/25, Step 2600, Train loss: 0.025\n",
      "Epoch: 6/25, Step 2640, Train loss: 0.025\n",
      "Epoch: 6/25, Step 2680, Train loss: 0.025\n",
      "Epoch: 6/25, Step 2720, Train loss: 0.025\n",
      "Epoch: 6/25, Step 2760, Train loss: 0.025\n",
      "Epoch: 6/25, Step 2800, Train loss: 0.026\n",
      "Epoch: 6/25, Step 2840, Train loss: 0.025\n",
      "Epoch: 6/25, Step 2880, Train loss: 0.025\n",
      "Epoch 6/25, Train Loss: 0.025, Test Loss: 0.025\n",
      "Epoch: 7/25, Step 2920, Train loss: 0.026\n",
      "Epoch: 7/25, Step 2960, Train loss: 0.025\n",
      "Epoch: 7/25, Step 3000, Train loss: 0.026\n",
      "Epoch: 7/25, Step 3040, Train loss: 0.025\n",
      "Epoch: 7/25, Step 3080, Train loss: 0.025\n",
      "Epoch: 7/25, Step 3120, Train loss: 0.025\n",
      "Epoch: 7/25, Step 3160, Train loss: 0.025\n",
      "Epoch: 7/25, Step 3200, Train loss: 0.025\n",
      "Epoch: 7/25, Step 3240, Train loss: 0.026\n",
      "Epoch: 7/25, Step 3280, Train loss: 0.025\n",
      "Epoch: 7/25, Step 3320, Train loss: 0.025\n",
      "Epoch: 7/25, Step 3360, Train loss: 0.025\n",
      "Epoch 7/25, Train Loss: 0.025, Test Loss: 0.025\n",
      "Epoch: 8/25, Step 3400, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3440, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3480, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3520, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3560, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3600, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3640, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3680, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3720, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3760, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3800, Train loss: 0.025\n",
      "Epoch: 8/25, Step 3840, Train loss: 0.026\n",
      "Epoch 8/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 9/25, Step 3880, Train loss: 0.025\n",
      "Epoch: 9/25, Step 3920, Train loss: 0.025\n",
      "Epoch: 9/25, Step 3960, Train loss: 0.024\n",
      "Epoch: 9/25, Step 4000, Train loss: 0.025\n",
      "Epoch: 9/25, Step 4040, Train loss: 0.025\n",
      "Epoch: 9/25, Step 4080, Train loss: 0.025\n",
      "Epoch: 9/25, Step 4120, Train loss: 0.025\n",
      "Epoch: 9/25, Step 4160, Train loss: 0.025\n",
      "Epoch: 9/25, Step 4200, Train loss: 0.025\n",
      "Epoch: 9/25, Step 4240, Train loss: 0.025\n",
      "Epoch: 9/25, Step 4280, Train loss: 0.026\n",
      "Epoch: 9/25, Step 4320, Train loss: 0.025\n",
      "Test loss decreased (0.024068 --> 0.024043).  Saving model ...\n",
      "Epoch 9/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 10/25, Step 4360, Train loss: 0.025\n",
      "Epoch: 10/25, Step 4400, Train loss: 0.026\n",
      "Epoch: 10/25, Step 4440, Train loss: 0.026\n",
      "Epoch: 10/25, Step 4480, Train loss: 0.025\n",
      "Epoch: 10/25, Step 4520, Train loss: 0.025\n",
      "Epoch: 10/25, Step 4560, Train loss: 0.024\n",
      "Epoch: 10/25, Step 4600, Train loss: 0.025\n",
      "Epoch: 10/25, Step 4640, Train loss: 0.026\n",
      "Epoch: 10/25, Step 4680, Train loss: 0.025\n",
      "Epoch: 10/25, Step 4720, Train loss: 0.025\n",
      "Epoch: 10/25, Step 4760, Train loss: 0.025\n",
      "Epoch: 10/25, Step 4800, Train loss: 0.026\n",
      "Test loss decreased (0.024043 --> 0.023914).  Saving model ...\n",
      "Epoch 10/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 11/25, Step 4840, Train loss: 0.025\n",
      "Epoch: 11/25, Step 4880, Train loss: 0.025\n",
      "Epoch: 11/25, Step 4920, Train loss: 0.025\n",
      "Epoch: 11/25, Step 4960, Train loss: 0.025\n",
      "Epoch: 11/25, Step 5000, Train loss: 0.025\n",
      "Epoch: 11/25, Step 5040, Train loss: 0.026\n",
      "Epoch: 11/25, Step 5080, Train loss: 0.024\n",
      "Epoch: 11/25, Step 5120, Train loss: 0.026\n",
      "Epoch: 11/25, Step 5160, Train loss: 0.025\n",
      "Epoch: 11/25, Step 5200, Train loss: 0.026\n",
      "Epoch: 11/25, Step 5240, Train loss: 0.025\n",
      "Epoch: 11/25, Step 5280, Train loss: 0.025\n",
      "Epoch 11/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 12/25, Step 5320, Train loss: 0.025\n",
      "Epoch: 12/25, Step 5360, Train loss: 0.025\n",
      "Epoch: 12/25, Step 5400, Train loss: 0.026\n",
      "Epoch: 12/25, Step 5440, Train loss: 0.025\n",
      "Epoch: 12/25, Step 5480, Train loss: 0.025\n",
      "Epoch: 12/25, Step 5520, Train loss: 0.025\n",
      "Epoch: 12/25, Step 5560, Train loss: 0.025\n",
      "Epoch: 12/25, Step 5600, Train loss: 0.025\n",
      "Epoch: 12/25, Step 5640, Train loss: 0.025\n",
      "Epoch: 12/25, Step 5680, Train loss: 0.025\n",
      "Epoch: 12/25, Step 5720, Train loss: 0.025\n",
      "Epoch: 12/25, Step 5760, Train loss: 0.025\n",
      "Epoch 12/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 13/25, Step 5800, Train loss: 0.025\n",
      "Epoch: 13/25, Step 5840, Train loss: 0.025\n",
      "Epoch: 13/25, Step 5880, Train loss: 0.025\n",
      "Epoch: 13/25, Step 5920, Train loss: 0.026\n",
      "Epoch: 13/25, Step 5960, Train loss: 0.025\n",
      "Epoch: 13/25, Step 6000, Train loss: 0.024\n",
      "Epoch: 13/25, Step 6040, Train loss: 0.025\n",
      "Epoch: 13/25, Step 6080, Train loss: 0.025\n",
      "Epoch: 13/25, Step 6120, Train loss: 0.025\n",
      "Epoch: 13/25, Step 6160, Train loss: 0.025\n",
      "Epoch: 13/25, Step 6200, Train loss: 0.025\n",
      "Epoch: 13/25, Step 6240, Train loss: 0.025\n",
      "Epoch 13/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 14/25, Step 6280, Train loss: 0.025\n",
      "Epoch: 14/25, Step 6320, Train loss: 0.025\n",
      "Epoch: 14/25, Step 6360, Train loss: 0.025\n",
      "Epoch: 14/25, Step 6400, Train loss: 0.025\n",
      "Epoch: 14/25, Step 6440, Train loss: 0.025\n",
      "Epoch: 14/25, Step 6480, Train loss: 0.026\n",
      "Epoch: 14/25, Step 6520, Train loss: 0.026\n",
      "Epoch: 14/25, Step 6560, Train loss: 0.025\n",
      "Epoch: 14/25, Step 6600, Train loss: 0.024\n",
      "Epoch: 14/25, Step 6640, Train loss: 0.025\n",
      "Epoch: 14/25, Step 6680, Train loss: 0.025\n",
      "Epoch: 14/25, Step 6720, Train loss: 0.025\n",
      "Epoch 14/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 15/25, Step 6760, Train loss: 0.025\n",
      "Epoch: 15/25, Step 6800, Train loss: 0.024\n",
      "Epoch: 15/25, Step 6840, Train loss: 0.025\n",
      "Epoch: 15/25, Step 6880, Train loss: 0.025\n",
      "Epoch: 15/25, Step 6920, Train loss: 0.025\n",
      "Epoch: 15/25, Step 6960, Train loss: 0.025\n",
      "Epoch: 15/25, Step 7000, Train loss: 0.025\n",
      "Epoch: 15/25, Step 7040, Train loss: 0.026\n",
      "Epoch: 15/25, Step 7080, Train loss: 0.024\n",
      "Epoch: 15/25, Step 7120, Train loss: 0.026\n",
      "Epoch: 15/25, Step 7160, Train loss: 0.025\n",
      "Epoch: 15/25, Step 7200, Train loss: 0.025\n",
      "Test loss decreased (0.023914 --> 0.023902).  Saving model ...\n",
      "Epoch 15/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 16/25, Step 7240, Train loss: 0.025\n",
      "Epoch: 16/25, Step 7280, Train loss: 0.024\n",
      "Epoch: 16/25, Step 7320, Train loss: 0.025\n",
      "Epoch: 16/25, Step 7360, Train loss: 0.025\n",
      "Epoch: 16/25, Step 7400, Train loss: 0.025\n",
      "Epoch: 16/25, Step 7440, Train loss: 0.025\n",
      "Epoch: 16/25, Step 7480, Train loss: 0.025\n",
      "Epoch: 16/25, Step 7520, Train loss: 0.025\n",
      "Epoch: 16/25, Step 7560, Train loss: 0.025\n",
      "Epoch: 16/25, Step 7600, Train loss: 0.025\n",
      "Epoch: 16/25, Step 7640, Train loss: 0.024\n",
      "Epoch: 16/25, Step 7680, Train loss: 0.025\n",
      "Test loss decreased (0.023902 --> 0.023640).  Saving model ...\n",
      "Epoch 16/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 17/25, Step 7720, Train loss: 0.025\n",
      "Epoch: 17/25, Step 7760, Train loss: 0.025\n",
      "Epoch: 17/25, Step 7800, Train loss: 0.025\n",
      "Epoch: 17/25, Step 7840, Train loss: 0.026\n",
      "Epoch: 17/25, Step 7880, Train loss: 0.025\n",
      "Epoch: 17/25, Step 7920, Train loss: 0.024\n",
      "Epoch: 17/25, Step 7960, Train loss: 0.024\n",
      "Epoch: 17/25, Step 8000, Train loss: 0.025\n",
      "Epoch: 17/25, Step 8040, Train loss: 0.025\n",
      "Epoch: 17/25, Step 8080, Train loss: 0.025\n",
      "Epoch: 17/25, Step 8120, Train loss: 0.025\n",
      "Epoch: 17/25, Step 8160, Train loss: 0.025\n",
      "Epoch 17/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 18/25, Step 8200, Train loss: 0.025\n",
      "Epoch: 18/25, Step 8240, Train loss: 0.025\n",
      "Epoch: 18/25, Step 8280, Train loss: 0.025\n",
      "Epoch: 18/25, Step 8320, Train loss: 0.025\n",
      "Epoch: 18/25, Step 8360, Train loss: 0.025\n",
      "Epoch: 18/25, Step 8400, Train loss: 0.025\n",
      "Epoch: 18/25, Step 8440, Train loss: 0.025\n",
      "Epoch: 18/25, Step 8480, Train loss: 0.026\n",
      "Epoch: 18/25, Step 8520, Train loss: 0.025\n",
      "Epoch: 18/25, Step 8560, Train loss: 0.025\n",
      "Epoch: 18/25, Step 8600, Train loss: 0.025\n",
      "Epoch: 18/25, Step 8640, Train loss: 0.025\n",
      "Epoch 18/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 19/25, Step 8680, Train loss: 0.025\n",
      "Epoch: 19/25, Step 8720, Train loss: 0.025\n",
      "Epoch: 19/25, Step 8760, Train loss: 0.025\n",
      "Epoch: 19/25, Step 8800, Train loss: 0.024\n",
      "Epoch: 19/25, Step 8840, Train loss: 0.025\n",
      "Epoch: 19/25, Step 8880, Train loss: 0.025\n",
      "Epoch: 19/25, Step 8920, Train loss: 0.025\n",
      "Epoch: 19/25, Step 8960, Train loss: 0.025\n",
      "Epoch: 19/25, Step 9000, Train loss: 0.026\n",
      "Epoch: 19/25, Step 9040, Train loss: 0.025\n",
      "Epoch: 19/25, Step 9080, Train loss: 0.024\n",
      "Epoch: 19/25, Step 9120, Train loss: 0.025\n",
      "Epoch 19/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 20/25, Step 9160, Train loss: 0.024\n",
      "Epoch: 20/25, Step 9200, Train loss: 0.025\n",
      "Epoch: 20/25, Step 9240, Train loss: 0.025\n",
      "Epoch: 20/25, Step 9280, Train loss: 0.024\n",
      "Epoch: 20/25, Step 9320, Train loss: 0.025\n",
      "Epoch: 20/25, Step 9360, Train loss: 0.025\n",
      "Epoch: 20/25, Step 9400, Train loss: 0.027\n",
      "Epoch: 20/25, Step 9440, Train loss: 0.025\n",
      "Epoch: 20/25, Step 9480, Train loss: 0.025\n",
      "Epoch: 20/25, Step 9520, Train loss: 0.025\n",
      "Epoch: 20/25, Step 9560, Train loss: 0.024\n",
      "Epoch: 20/25, Step 9600, Train loss: 0.025\n",
      "Epoch 20/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 21/25, Step 9640, Train loss: 0.025\n",
      "Epoch: 21/25, Step 9680, Train loss: 0.025\n",
      "Epoch: 21/25, Step 9720, Train loss: 0.025\n",
      "Epoch: 21/25, Step 9760, Train loss: 0.024\n",
      "Epoch: 21/25, Step 9800, Train loss: 0.026\n",
      "Epoch: 21/25, Step 9840, Train loss: 0.025\n",
      "Epoch: 21/25, Step 9880, Train loss: 0.025\n",
      "Epoch: 21/25, Step 9920, Train loss: 0.025\n",
      "Epoch: 21/25, Step 9960, Train loss: 0.025\n",
      "Epoch: 21/25, Step 10000, Train loss: 0.025\n",
      "Epoch: 21/25, Step 10040, Train loss: 0.025\n",
      "Epoch: 21/25, Step 10080, Train loss: 0.025\n",
      "Epoch 21/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 22/25, Step 10120, Train loss: 0.024\n",
      "Epoch: 22/25, Step 10160, Train loss: 0.026\n",
      "Epoch: 22/25, Step 10200, Train loss: 0.025\n",
      "Epoch: 22/25, Step 10240, Train loss: 0.025\n",
      "Epoch: 22/25, Step 10280, Train loss: 0.025\n",
      "Epoch: 22/25, Step 10320, Train loss: 0.024\n",
      "Epoch: 22/25, Step 10360, Train loss: 0.025\n",
      "Epoch: 22/25, Step 10400, Train loss: 0.025\n",
      "Epoch: 22/25, Step 10440, Train loss: 0.025\n",
      "Epoch: 22/25, Step 10480, Train loss: 0.025\n",
      "Epoch: 22/25, Step 10520, Train loss: 0.025\n",
      "Epoch: 22/25, Step 10560, Train loss: 0.025\n",
      "Epoch 22/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 23/25, Step 10600, Train loss: 0.025\n",
      "Epoch: 23/25, Step 10640, Train loss: 0.025\n",
      "Epoch: 23/25, Step 10680, Train loss: 0.025\n",
      "Epoch: 23/25, Step 10720, Train loss: 0.025\n",
      "Epoch: 23/25, Step 10760, Train loss: 0.025\n",
      "Epoch: 23/25, Step 10800, Train loss: 0.025\n",
      "Epoch: 23/25, Step 10840, Train loss: 0.025\n",
      "Epoch: 23/25, Step 10880, Train loss: 0.025\n",
      "Epoch: 23/25, Step 10920, Train loss: 0.025\n",
      "Epoch: 23/25, Step 10960, Train loss: 0.025\n",
      "Epoch: 23/25, Step 11000, Train loss: 0.025\n",
      "Epoch: 23/25, Step 11040, Train loss: 0.025\n",
      "Epoch 23/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Epoch: 24/25, Step 11080, Train loss: 0.025\n",
      "Epoch: 24/25, Step 11120, Train loss: 0.025\n",
      "Epoch: 24/25, Step 11160, Train loss: 0.024\n",
      "Epoch: 24/25, Step 11200, Train loss: 0.025\n",
      "Epoch: 24/25, Step 11240, Train loss: 0.025\n",
      "Epoch: 24/25, Step 11280, Train loss: 0.025\n",
      "Epoch: 24/25, Step 11320, Train loss: 0.025\n",
      "Epoch: 24/25, Step 11360, Train loss: 0.025\n",
      "Epoch: 24/25, Step 11400, Train loss: 0.025\n",
      "Epoch: 24/25, Step 11440, Train loss: 0.025\n",
      "Epoch: 24/25, Step 11480, Train loss: 0.025\n",
      "Epoch: 24/25, Step 11520, Train loss: 0.025\n",
      "Epoch 24/25, Train Loss: 0.025, Test Loss: 0.025\n",
      "Epoch: 25/25, Step 11560, Train loss: 0.025\n",
      "Epoch: 25/25, Step 11600, Train loss: 0.025\n",
      "Epoch: 25/25, Step 11640, Train loss: 0.025\n",
      "Epoch: 25/25, Step 11680, Train loss: 0.025\n",
      "Epoch: 25/25, Step 11720, Train loss: 0.025\n",
      "Epoch: 25/25, Step 11760, Train loss: 0.025\n",
      "Epoch: 25/25, Step 11800, Train loss: 0.025\n",
      "Epoch: 25/25, Step 11840, Train loss: 0.025\n",
      "Epoch: 25/25, Step 11880, Train loss: 0.025\n",
      "Epoch: 25/25, Step 11920, Train loss: 0.024\n",
      "Epoch: 25/25, Step 11960, Train loss: 0.026\n",
      "Epoch: 25/25, Step 12000, Train loss: 0.024\n",
      "Epoch 25/25, Train Loss: 0.025, Test Loss: 0.024\n",
      "Training with hidden dim: 3\n",
      "Epoch: 1/25, Step 40, Train loss: 0.049\n",
      "Epoch: 1/25, Step 80, Train loss: 0.048\n",
      "Epoch: 1/25, Step 120, Train loss: 0.045\n",
      "Epoch: 1/25, Step 160, Train loss: 0.046\n",
      "Epoch: 1/25, Step 200, Train loss: 0.044\n",
      "Epoch: 1/25, Step 240, Train loss: 0.046\n",
      "Epoch: 1/25, Step 280, Train loss: 0.045\n",
      "Epoch: 1/25, Step 320, Train loss: 0.045\n",
      "Epoch: 1/25, Step 360, Train loss: 0.045\n",
      "Epoch: 1/25, Step 400, Train loss: 0.045\n",
      "Epoch: 1/25, Step 440, Train loss: 0.045\n",
      "Epoch: 1/25, Step 480, Train loss: 0.045\n",
      "Test loss decreased (inf --> 0.046150).  Saving model ...\n",
      "Epoch 1/25, Train Loss: 0.046, Test Loss: 0.046\n",
      "Epoch: 2/25, Step 520, Train loss: 0.045\n",
      "Epoch: 2/25, Step 560, Train loss: 0.045\n",
      "Epoch: 2/25, Step 600, Train loss: 0.045\n",
      "Epoch: 2/25, Step 640, Train loss: 0.046\n",
      "Epoch: 2/25, Step 680, Train loss: 0.045\n",
      "Epoch: 2/25, Step 720, Train loss: 0.045\n",
      "Epoch: 2/25, Step 760, Train loss: 0.044\n",
      "Epoch: 2/25, Step 800, Train loss: 0.044\n",
      "Epoch: 2/25, Step 840, Train loss: 0.044\n",
      "Epoch: 2/25, Step 880, Train loss: 0.046\n",
      "Epoch: 2/25, Step 920, Train loss: 0.045\n",
      "Epoch: 2/25, Step 960, Train loss: 0.046\n",
      "Test loss decreased (0.046150 --> 0.042246).  Saving model ...\n",
      "Epoch 2/25, Train Loss: 0.045, Test Loss: 0.042\n",
      "Epoch: 3/25, Step 1000, Train loss: 0.044\n",
      "Epoch: 3/25, Step 1040, Train loss: 0.045\n",
      "Epoch: 3/25, Step 1080, Train loss: 0.046\n",
      "Epoch: 3/25, Step 1120, Train loss: 0.045\n",
      "Epoch: 3/25, Step 1160, Train loss: 0.044\n",
      "Epoch: 3/25, Step 1200, Train loss: 0.045\n",
      "Epoch: 3/25, Step 1240, Train loss: 0.044\n",
      "Epoch: 3/25, Step 1280, Train loss: 0.043\n",
      "Epoch: 3/25, Step 1320, Train loss: 0.045\n",
      "Epoch: 3/25, Step 1360, Train loss: 0.045\n",
      "Epoch: 3/25, Step 1400, Train loss: 0.046\n",
      "Epoch: 3/25, Step 1440, Train loss: 0.045\n",
      "Test loss decreased (0.042246 --> 0.042065).  Saving model ...\n",
      "Epoch 3/25, Train Loss: 0.045, Test Loss: 0.042\n",
      "Epoch: 4/25, Step 1480, Train loss: 0.044\n",
      "Epoch: 4/25, Step 1520, Train loss: 0.045\n",
      "Epoch: 4/25, Step 1560, Train loss: 0.043\n",
      "Epoch: 4/25, Step 1600, Train loss: 0.045\n",
      "Epoch: 4/25, Step 1640, Train loss: 0.047\n",
      "Epoch: 4/25, Step 1680, Train loss: 0.044\n",
      "Epoch: 4/25, Step 1720, Train loss: 0.044\n",
      "Epoch: 4/25, Step 1760, Train loss: 0.047\n",
      "Epoch: 4/25, Step 1800, Train loss: 0.043\n",
      "Epoch: 4/25, Step 1840, Train loss: 0.045\n",
      "Epoch: 4/25, Step 1880, Train loss: 0.044\n",
      "Epoch: 4/25, Step 1920, Train loss: 0.044\n",
      "Epoch 4/25, Train Loss: 0.045, Test Loss: 0.043\n",
      "Epoch: 5/25, Step 1960, Train loss: 0.045\n",
      "Epoch: 5/25, Step 2000, Train loss: 0.045\n",
      "Epoch: 5/25, Step 2040, Train loss: 0.045\n",
      "Epoch: 5/25, Step 2080, Train loss: 0.045\n",
      "Epoch: 5/25, Step 2120, Train loss: 0.044\n",
      "Epoch: 5/25, Step 2160, Train loss: 0.045\n",
      "Epoch: 5/25, Step 2200, Train loss: 0.044\n",
      "Epoch: 5/25, Step 2240, Train loss: 0.043\n",
      "Epoch: 5/25, Step 2280, Train loss: 0.045\n",
      "Epoch: 5/25, Step 2320, Train loss: 0.044\n",
      "Epoch: 5/25, Step 2360, Train loss: 0.045\n",
      "Epoch: 5/25, Step 2400, Train loss: 0.046\n",
      "Test loss decreased (0.042065 --> 0.041708).  Saving model ...\n",
      "Epoch 5/25, Train Loss: 0.045, Test Loss: 0.042\n",
      "Epoch: 6/25, Step 2440, Train loss: 0.046\n",
      "Epoch: 6/25, Step 2480, Train loss: 0.044\n",
      "Epoch: 6/25, Step 2520, Train loss: 0.046\n",
      "Epoch: 6/25, Step 2560, Train loss: 0.045\n",
      "Epoch: 6/25, Step 2600, Train loss: 0.043\n",
      "Epoch: 6/25, Step 2640, Train loss: 0.044\n",
      "Epoch: 6/25, Step 2680, Train loss: 0.043\n",
      "Epoch: 6/25, Step 2720, Train loss: 0.046\n",
      "Epoch: 6/25, Step 2760, Train loss: 0.044\n",
      "Epoch: 6/25, Step 2800, Train loss: 0.045\n",
      "Epoch: 6/25, Step 2840, Train loss: 0.045\n",
      "Epoch: 6/25, Step 2880, Train loss: 0.045\n",
      "Epoch 6/25, Train Loss: 0.045, Test Loss: 0.043\n",
      "Epoch: 7/25, Step 2920, Train loss: 0.046\n",
      "Epoch: 7/25, Step 2960, Train loss: 0.044\n",
      "Epoch: 7/25, Step 3000, Train loss: 0.045\n",
      "Epoch: 7/25, Step 3040, Train loss: 0.045\n",
      "Epoch: 7/25, Step 3080, Train loss: 0.047\n",
      "Epoch: 7/25, Step 3120, Train loss: 0.043\n",
      "Epoch: 7/25, Step 3160, Train loss: 0.044\n",
      "Epoch: 7/25, Step 3200, Train loss: 0.044\n",
      "Epoch: 7/25, Step 3240, Train loss: 0.045\n",
      "Epoch: 7/25, Step 3280, Train loss: 0.044\n",
      "Epoch: 7/25, Step 3320, Train loss: 0.044\n",
      "Epoch: 7/25, Step 3360, Train loss: 0.044\n",
      "Epoch 7/25, Train Loss: 0.045, Test Loss: 0.043\n",
      "Epoch: 8/25, Step 3400, Train loss: 0.045\n",
      "Epoch: 8/25, Step 3440, Train loss: 0.044\n",
      "Epoch: 8/25, Step 3480, Train loss: 0.046\n",
      "Epoch: 8/25, Step 3520, Train loss: 0.044\n",
      "Epoch: 8/25, Step 3560, Train loss: 0.044\n",
      "Epoch: 8/25, Step 3600, Train loss: 0.045\n",
      "Epoch: 8/25, Step 3640, Train loss: 0.045\n",
      "Epoch: 8/25, Step 3680, Train loss: 0.044\n",
      "Epoch: 8/25, Step 3720, Train loss: 0.043\n",
      "Epoch: 8/25, Step 3760, Train loss: 0.045\n",
      "Epoch: 8/25, Step 3800, Train loss: 0.046\n",
      "Epoch: 8/25, Step 3840, Train loss: 0.045\n",
      "Epoch 8/25, Train Loss: 0.045, Test Loss: 0.042\n",
      "Epoch: 9/25, Step 3880, Train loss: 0.044\n",
      "Epoch: 9/25, Step 3920, Train loss: 0.045\n",
      "Epoch: 9/25, Step 3960, Train loss: 0.045\n",
      "Epoch: 9/25, Step 4000, Train loss: 0.045\n",
      "Epoch: 9/25, Step 4040, Train loss: 0.044\n",
      "Epoch: 9/25, Step 4080, Train loss: 0.045\n",
      "Epoch: 9/25, Step 4120, Train loss: 0.045\n",
      "Epoch: 9/25, Step 4160, Train loss: 0.044\n",
      "Epoch: 9/25, Step 4200, Train loss: 0.043\n",
      "Epoch: 9/25, Step 4240, Train loss: 0.045\n",
      "Epoch: 9/25, Step 4280, Train loss: 0.046\n",
      "Epoch: 9/25, Step 4320, Train loss: 0.046\n",
      "Epoch 9/25, Train Loss: 0.045, Test Loss: 0.045\n",
      "Epoch: 10/25, Step 4360, Train loss: 0.044\n",
      "Epoch: 10/25, Step 4400, Train loss: 0.045\n",
      "Epoch: 10/25, Step 4440, Train loss: 0.044\n",
      "Epoch: 10/25, Step 4480, Train loss: 0.046\n",
      "Epoch: 10/25, Step 4520, Train loss: 0.043\n",
      "Epoch: 10/25, Step 4560, Train loss: 0.044\n",
      "Epoch: 10/25, Step 4600, Train loss: 0.045\n",
      "Epoch: 10/25, Step 4640, Train loss: 0.045\n",
      "Epoch: 10/25, Step 4680, Train loss: 0.044\n",
      "Epoch: 10/25, Step 4720, Train loss: 0.044\n",
      "Epoch: 10/25, Step 4760, Train loss: 0.045\n",
      "Epoch: 10/25, Step 4800, Train loss: 0.045\n",
      "Test loss decreased (0.041708 --> 0.041660).  Saving model ...\n",
      "Epoch 10/25, Train Loss: 0.044, Test Loss: 0.042\n",
      "Epoch: 11/25, Step 4840, Train loss: 0.046\n",
      "Epoch: 11/25, Step 4880, Train loss: 0.044\n",
      "Epoch: 11/25, Step 4920, Train loss: 0.044\n",
      "Epoch: 11/25, Step 4960, Train loss: 0.046\n",
      "Epoch: 11/25, Step 5000, Train loss: 0.045\n",
      "Epoch: 11/25, Step 5040, Train loss: 0.042\n",
      "Epoch: 11/25, Step 5080, Train loss: 0.045\n",
      "Epoch: 11/25, Step 5120, Train loss: 0.044\n",
      "Epoch: 11/25, Step 5160, Train loss: 0.044\n",
      "Epoch: 11/25, Step 5200, Train loss: 0.045\n",
      "Epoch: 11/25, Step 5240, Train loss: 0.044\n",
      "Epoch: 11/25, Step 5280, Train loss: 0.045\n",
      "Epoch 11/25, Train Loss: 0.044, Test Loss: 0.044\n",
      "Epoch: 12/25, Step 5320, Train loss: 0.044\n",
      "Epoch: 12/25, Step 5360, Train loss: 0.045\n",
      "Epoch: 12/25, Step 5400, Train loss: 0.043\n",
      "Epoch: 12/25, Step 5440, Train loss: 0.044\n",
      "Epoch: 12/25, Step 5480, Train loss: 0.046\n",
      "Epoch: 12/25, Step 5520, Train loss: 0.043\n",
      "Epoch: 12/25, Step 5560, Train loss: 0.046\n",
      "Epoch: 12/25, Step 5600, Train loss: 0.045\n",
      "Epoch: 12/25, Step 5640, Train loss: 0.045\n",
      "Epoch: 12/25, Step 5680, Train loss: 0.043\n",
      "Epoch: 12/25, Step 5720, Train loss: 0.045\n",
      "Epoch: 12/25, Step 5760, Train loss: 0.044\n",
      "Epoch 12/25, Train Loss: 0.044, Test Loss: 0.044\n",
      "Epoch: 13/25, Step 5800, Train loss: 0.044\n",
      "Epoch: 13/25, Step 5840, Train loss: 0.045\n",
      "Epoch: 13/25, Step 5880, Train loss: 0.046\n",
      "Epoch: 13/25, Step 5920, Train loss: 0.045\n",
      "Epoch: 13/25, Step 5960, Train loss: 0.045\n",
      "Epoch: 13/25, Step 6000, Train loss: 0.044\n",
      "Epoch: 13/25, Step 6040, Train loss: 0.045\n",
      "Epoch: 13/25, Step 6080, Train loss: 0.045\n",
      "Epoch: 13/25, Step 6120, Train loss: 0.045\n",
      "Epoch: 13/25, Step 6160, Train loss: 0.042\n",
      "Epoch: 13/25, Step 6200, Train loss: 0.044\n",
      "Epoch: 13/25, Step 6240, Train loss: 0.045\n",
      "Epoch 13/25, Train Loss: 0.044, Test Loss: 0.042\n",
      "Epoch: 14/25, Step 6280, Train loss: 0.044\n",
      "Epoch: 14/25, Step 6320, Train loss: 0.043\n",
      "Epoch: 14/25, Step 6360, Train loss: 0.045\n",
      "Epoch: 14/25, Step 6400, Train loss: 0.044\n",
      "Epoch: 14/25, Step 6440, Train loss: 0.045\n",
      "Epoch: 14/25, Step 6480, Train loss: 0.045\n",
      "Epoch: 14/25, Step 6520, Train loss: 0.044\n",
      "Epoch: 14/25, Step 6560, Train loss: 0.045\n",
      "Epoch: 14/25, Step 6600, Train loss: 0.045\n",
      "Epoch: 14/25, Step 6640, Train loss: 0.046\n",
      "Epoch: 14/25, Step 6680, Train loss: 0.044\n",
      "Epoch: 14/25, Step 6720, Train loss: 0.045\n",
      "Epoch 14/25, Train Loss: 0.045, Test Loss: 0.042\n",
      "Epoch: 15/25, Step 6760, Train loss: 0.044\n",
      "Epoch: 15/25, Step 6800, Train loss: 0.045\n",
      "Epoch: 15/25, Step 6840, Train loss: 0.045\n",
      "Epoch: 15/25, Step 6880, Train loss: 0.045\n",
      "Epoch: 15/25, Step 6920, Train loss: 0.045\n",
      "Epoch: 15/25, Step 6960, Train loss: 0.044\n",
      "Epoch: 15/25, Step 7000, Train loss: 0.044\n",
      "Epoch: 15/25, Step 7040, Train loss: 0.044\n",
      "Epoch: 15/25, Step 7080, Train loss: 0.044\n",
      "Epoch: 15/25, Step 7120, Train loss: 0.045\n",
      "Epoch: 15/25, Step 7160, Train loss: 0.045\n",
      "Epoch: 15/25, Step 7200, Train loss: 0.045\n",
      "Test loss decreased (0.041660 --> 0.041523).  Saving model ...\n",
      "Epoch 15/25, Train Loss: 0.044, Test Loss: 0.042\n",
      "Epoch: 16/25, Step 7240, Train loss: 0.044\n",
      "Epoch: 16/25, Step 7280, Train loss: 0.046\n",
      "Epoch: 16/25, Step 7320, Train loss: 0.043\n",
      "Epoch: 16/25, Step 7360, Train loss: 0.045\n",
      "Epoch: 16/25, Step 7400, Train loss: 0.045\n",
      "Epoch: 16/25, Step 7440, Train loss: 0.043\n",
      "Epoch: 16/25, Step 7480, Train loss: 0.045\n",
      "Epoch: 16/25, Step 7520, Train loss: 0.045\n",
      "Epoch: 16/25, Step 7560, Train loss: 0.044\n",
      "Epoch: 16/25, Step 7600, Train loss: 0.044\n",
      "Epoch: 16/25, Step 7640, Train loss: 0.045\n",
      "Epoch: 16/25, Step 7680, Train loss: 0.044\n",
      "Epoch 16/25, Train Loss: 0.044, Test Loss: 0.046\n",
      "Epoch: 17/25, Step 7720, Train loss: 0.044\n",
      "Epoch: 17/25, Step 7760, Train loss: 0.045\n",
      "Epoch: 17/25, Step 7800, Train loss: 0.046\n",
      "Epoch: 17/25, Step 7840, Train loss: 0.044\n",
      "Epoch: 17/25, Step 7880, Train loss: 0.042\n",
      "Epoch: 17/25, Step 7920, Train loss: 0.043\n",
      "Epoch: 17/25, Step 7960, Train loss: 0.045\n",
      "Epoch: 17/25, Step 8000, Train loss: 0.044\n",
      "Epoch: 17/25, Step 8040, Train loss: 0.045\n",
      "Epoch: 17/25, Step 8080, Train loss: 0.047\n",
      "Epoch: 17/25, Step 8120, Train loss: 0.044\n",
      "Epoch: 17/25, Step 8160, Train loss: 0.045\n",
      "Epoch 17/25, Train Loss: 0.044, Test Loss: 0.044\n",
      "Epoch: 18/25, Step 8200, Train loss: 0.045\n",
      "Epoch: 18/25, Step 8240, Train loss: 0.044\n",
      "Epoch: 18/25, Step 8280, Train loss: 0.044\n",
      "Epoch: 18/25, Step 8320, Train loss: 0.046\n",
      "Epoch: 18/25, Step 8360, Train loss: 0.044\n",
      "Epoch: 18/25, Step 8400, Train loss: 0.046\n",
      "Epoch: 18/25, Step 8440, Train loss: 0.044\n",
      "Epoch: 18/25, Step 8480, Train loss: 0.044\n",
      "Epoch: 18/25, Step 8520, Train loss: 0.043\n",
      "Epoch: 18/25, Step 8560, Train loss: 0.045\n",
      "Epoch: 18/25, Step 8600, Train loss: 0.044\n",
      "Epoch: 18/25, Step 8640, Train loss: 0.044\n",
      "Epoch 18/25, Train Loss: 0.044, Test Loss: 0.046\n",
      "Epoch: 19/25, Step 8680, Train loss: 0.045\n",
      "Epoch: 19/25, Step 8720, Train loss: 0.046\n",
      "Epoch: 19/25, Step 8760, Train loss: 0.043\n",
      "Epoch: 19/25, Step 8800, Train loss: 0.045\n",
      "Epoch: 19/25, Step 8840, Train loss: 0.044\n",
      "Epoch: 19/25, Step 8880, Train loss: 0.045\n",
      "Epoch: 19/25, Step 8920, Train loss: 0.045\n",
      "Epoch: 19/25, Step 8960, Train loss: 0.042\n",
      "Epoch: 19/25, Step 9000, Train loss: 0.046\n",
      "Epoch: 19/25, Step 9040, Train loss: 0.045\n",
      "Epoch: 19/25, Step 9080, Train loss: 0.045\n",
      "Epoch: 19/25, Step 9120, Train loss: 0.042\n",
      "Epoch 19/25, Train Loss: 0.044, Test Loss: 0.044\n",
      "Epoch: 20/25, Step 9160, Train loss: 0.044\n",
      "Epoch: 20/25, Step 9200, Train loss: 0.045\n",
      "Epoch: 20/25, Step 9240, Train loss: 0.044\n",
      "Epoch: 20/25, Step 9280, Train loss: 0.044\n",
      "Epoch: 20/25, Step 9320, Train loss: 0.045\n",
      "Epoch: 20/25, Step 9360, Train loss: 0.043\n",
      "Epoch: 20/25, Step 9400, Train loss: 0.045\n",
      "Epoch: 20/25, Step 9440, Train loss: 0.044\n",
      "Epoch: 20/25, Step 9480, Train loss: 0.043\n",
      "Epoch: 20/25, Step 9520, Train loss: 0.044\n",
      "Epoch: 20/25, Step 9560, Train loss: 0.046\n",
      "Epoch: 20/25, Step 9600, Train loss: 0.045\n",
      "Epoch 20/25, Train Loss: 0.044, Test Loss: 0.042\n",
      "Epoch: 21/25, Step 9640, Train loss: 0.044\n",
      "Epoch: 21/25, Step 9680, Train loss: 0.045\n",
      "Epoch: 21/25, Step 9720, Train loss: 0.045\n",
      "Epoch: 21/25, Step 9760, Train loss: 0.044\n",
      "Epoch: 21/25, Step 9800, Train loss: 0.046\n",
      "Epoch: 21/25, Step 9840, Train loss: 0.045\n",
      "Epoch: 21/25, Step 9880, Train loss: 0.045\n",
      "Epoch: 21/25, Step 9920, Train loss: 0.045\n",
      "Epoch: 21/25, Step 9960, Train loss: 0.043\n",
      "Epoch: 21/25, Step 10000, Train loss: 0.044\n",
      "Epoch: 21/25, Step 10040, Train loss: 0.043\n",
      "Epoch: 21/25, Step 10080, Train loss: 0.044\n",
      "Epoch 21/25, Train Loss: 0.044, Test Loss: 0.042\n",
      "Epoch: 22/25, Step 10120, Train loss: 0.044\n",
      "Epoch: 22/25, Step 10160, Train loss: 0.043\n",
      "Epoch: 22/25, Step 10200, Train loss: 0.045\n",
      "Epoch: 22/25, Step 10240, Train loss: 0.046\n",
      "Epoch: 22/25, Step 10280, Train loss: 0.044\n",
      "Epoch: 22/25, Step 10320, Train loss: 0.046\n",
      "Epoch: 22/25, Step 10360, Train loss: 0.044\n",
      "Epoch: 22/25, Step 10400, Train loss: 0.044\n",
      "Epoch: 22/25, Step 10440, Train loss: 0.044\n",
      "Epoch: 22/25, Step 10480, Train loss: 0.044\n",
      "Epoch: 22/25, Step 10520, Train loss: 0.044\n",
      "Epoch: 22/25, Step 10560, Train loss: 0.046\n",
      "Epoch 22/25, Train Loss: 0.044, Test Loss: 0.043\n",
      "Epoch: 23/25, Step 10600, Train loss: 0.045\n",
      "Epoch: 23/25, Step 10640, Train loss: 0.045\n",
      "Epoch: 23/25, Step 10680, Train loss: 0.043\n",
      "Epoch: 23/25, Step 10720, Train loss: 0.045\n",
      "Epoch: 23/25, Step 10760, Train loss: 0.044\n",
      "Epoch: 23/25, Step 10800, Train loss: 0.045\n",
      "Epoch: 23/25, Step 10840, Train loss: 0.046\n",
      "Epoch: 23/25, Step 10880, Train loss: 0.045\n",
      "Epoch: 23/25, Step 10920, Train loss: 0.044\n",
      "Epoch: 23/25, Step 10960, Train loss: 0.044\n",
      "Epoch: 23/25, Step 11000, Train loss: 0.044\n",
      "Epoch: 23/25, Step 11040, Train loss: 0.045\n",
      "Epoch 23/25, Train Loss: 0.044, Test Loss: 0.042\n",
      "Epoch: 24/25, Step 11080, Train loss: 0.045\n",
      "Epoch: 24/25, Step 11120, Train loss: 0.044\n",
      "Epoch: 24/25, Step 11160, Train loss: 0.044\n",
      "Epoch: 24/25, Step 11200, Train loss: 0.045\n",
      "Epoch: 24/25, Step 11240, Train loss: 0.045\n",
      "Epoch: 24/25, Step 11280, Train loss: 0.044\n",
      "Epoch: 24/25, Step 11320, Train loss: 0.046\n",
      "Epoch: 24/25, Step 11360, Train loss: 0.045\n",
      "Epoch: 24/25, Step 11400, Train loss: 0.042\n",
      "Epoch: 24/25, Step 11440, Train loss: 0.045\n",
      "Epoch: 24/25, Step 11480, Train loss: 0.044\n",
      "Epoch: 24/25, Step 11520, Train loss: 0.045\n",
      "Epoch 24/25, Train Loss: 0.044, Test Loss: 0.042\n",
      "Epoch: 25/25, Step 11560, Train loss: 0.046\n",
      "Epoch: 25/25, Step 11600, Train loss: 0.044\n",
      "Epoch: 25/25, Step 11640, Train loss: 0.044\n",
      "Epoch: 25/25, Step 11680, Train loss: 0.043\n",
      "Epoch: 25/25, Step 11720, Train loss: 0.045\n",
      "Epoch: 25/25, Step 11760, Train loss: 0.045\n",
      "Epoch: 25/25, Step 11800, Train loss: 0.045\n",
      "Epoch: 25/25, Step 11840, Train loss: 0.045\n",
      "Epoch: 25/25, Step 11880, Train loss: 0.043\n",
      "Epoch: 25/25, Step 11920, Train loss: 0.044\n",
      "Epoch: 25/25, Step 11960, Train loss: 0.045\n",
      "Epoch: 25/25, Step 12000, Train loss: 0.044\n",
      "Epoch 25/25, Train Loss: 0.044, Test Loss: 0.043\n",
      "Training with hidden dim: 4\n",
      "Epoch: 1/25, Step 40, Train loss: 0.070\n",
      "Epoch: 1/25, Step 80, Train loss: 0.065\n",
      "Epoch: 1/25, Step 120, Train loss: 0.070\n",
      "Epoch: 1/25, Step 160, Train loss: 0.068\n",
      "Epoch: 1/25, Step 200, Train loss: 0.068\n",
      "Epoch: 1/25, Step 240, Train loss: 0.068\n",
      "Epoch: 1/25, Step 280, Train loss: 0.065\n",
      "Epoch: 1/25, Step 320, Train loss: 0.069\n",
      "Epoch: 1/25, Step 360, Train loss: 0.067\n",
      "Epoch: 1/25, Step 400, Train loss: 0.068\n",
      "Epoch: 1/25, Step 440, Train loss: 0.068\n",
      "Epoch: 1/25, Step 480, Train loss: 0.063\n",
      "Test loss decreased (inf --> 0.066994).  Saving model ...\n",
      "Epoch 1/25, Train Loss: 0.067, Test Loss: 0.067\n",
      "Epoch: 2/25, Step 520, Train loss: 0.068\n",
      "Epoch: 2/25, Step 560, Train loss: 0.066\n",
      "Epoch: 2/25, Step 600, Train loss: 0.066\n",
      "Epoch: 2/25, Step 640, Train loss: 0.065\n",
      "Epoch: 2/25, Step 680, Train loss: 0.065\n",
      "Epoch: 2/25, Step 720, Train loss: 0.069\n",
      "Epoch: 2/25, Step 760, Train loss: 0.068\n",
      "Epoch: 2/25, Step 800, Train loss: 0.066\n",
      "Epoch: 2/25, Step 840, Train loss: 0.069\n",
      "Epoch: 2/25, Step 880, Train loss: 0.066\n",
      "Epoch: 2/25, Step 920, Train loss: 0.066\n",
      "Epoch: 2/25, Step 960, Train loss: 0.067\n",
      "Epoch 2/25, Train Loss: 0.067, Test Loss: 0.067\n",
      "Epoch: 3/25, Step 1000, Train loss: 0.066\n",
      "Epoch: 3/25, Step 1040, Train loss: 0.069\n",
      "Epoch: 3/25, Step 1080, Train loss: 0.068\n",
      "Epoch: 3/25, Step 1120, Train loss: 0.066\n",
      "Epoch: 3/25, Step 1160, Train loss: 0.069\n",
      "Epoch: 3/25, Step 1200, Train loss: 0.063\n",
      "Epoch: 3/25, Step 1240, Train loss: 0.066\n",
      "Epoch: 3/25, Step 1280, Train loss: 0.068\n",
      "Epoch: 3/25, Step 1320, Train loss: 0.068\n",
      "Epoch: 3/25, Step 1360, Train loss: 0.066\n",
      "Epoch: 3/25, Step 1400, Train loss: 0.064\n",
      "Epoch: 3/25, Step 1440, Train loss: 0.063\n",
      "Test loss decreased (0.066994 --> 0.066124).  Saving model ...\n",
      "Epoch 3/25, Train Loss: 0.066, Test Loss: 0.066\n",
      "Epoch: 4/25, Step 1480, Train loss: 0.066\n",
      "Epoch: 4/25, Step 1520, Train loss: 0.065\n",
      "Epoch: 4/25, Step 1560, Train loss: 0.066\n",
      "Epoch: 4/25, Step 1600, Train loss: 0.070\n",
      "Epoch: 4/25, Step 1640, Train loss: 0.062\n",
      "Epoch: 4/25, Step 1680, Train loss: 0.064\n",
      "Epoch: 4/25, Step 1720, Train loss: 0.065\n",
      "Epoch: 4/25, Step 1760, Train loss: 0.066\n",
      "Epoch: 4/25, Step 1800, Train loss: 0.067\n",
      "Epoch: 4/25, Step 1840, Train loss: 0.065\n",
      "Epoch: 4/25, Step 1880, Train loss: 0.069\n",
      "Epoch: 4/25, Step 1920, Train loss: 0.067\n",
      "Test loss decreased (0.066124 --> 0.065578).  Saving model ...\n",
      "Epoch 4/25, Train Loss: 0.066, Test Loss: 0.066\n",
      "Epoch: 5/25, Step 1960, Train loss: 0.065\n",
      "Epoch: 5/25, Step 2000, Train loss: 0.067\n",
      "Epoch: 5/25, Step 2040, Train loss: 0.068\n",
      "Epoch: 5/25, Step 2080, Train loss: 0.065\n",
      "Epoch: 5/25, Step 2120, Train loss: 0.065\n",
      "Epoch: 5/25, Step 2160, Train loss: 0.069\n",
      "Epoch: 5/25, Step 2200, Train loss: 0.065\n",
      "Epoch: 5/25, Step 2240, Train loss: 0.070\n",
      "Epoch: 5/25, Step 2280, Train loss: 0.066\n",
      "Epoch: 5/25, Step 2320, Train loss: 0.066\n",
      "Epoch: 5/25, Step 2360, Train loss: 0.066\n",
      "Epoch: 5/25, Step 2400, Train loss: 0.065\n",
      "Test loss decreased (0.065578 --> 0.062809).  Saving model ...\n",
      "Epoch 5/25, Train Loss: 0.066, Test Loss: 0.063\n",
      "Epoch: 6/25, Step 2440, Train loss: 0.067\n",
      "Epoch: 6/25, Step 2480, Train loss: 0.068\n",
      "Epoch: 6/25, Step 2520, Train loss: 0.064\n",
      "Epoch: 6/25, Step 2560, Train loss: 0.065\n",
      "Epoch: 6/25, Step 2600, Train loss: 0.068\n",
      "Epoch: 6/25, Step 2640, Train loss: 0.067\n",
      "Epoch: 6/25, Step 2680, Train loss: 0.067\n",
      "Epoch: 6/25, Step 2720, Train loss: 0.064\n",
      "Epoch: 6/25, Step 2760, Train loss: 0.067\n",
      "Epoch: 6/25, Step 2800, Train loss: 0.067\n",
      "Epoch: 6/25, Step 2840, Train loss: 0.068\n",
      "Epoch: 6/25, Step 2880, Train loss: 0.064\n",
      "Epoch 6/25, Train Loss: 0.066, Test Loss: 0.064\n",
      "Epoch: 7/25, Step 2920, Train loss: 0.063\n",
      "Epoch: 7/25, Step 2960, Train loss: 0.064\n",
      "Epoch: 7/25, Step 3000, Train loss: 0.065\n",
      "Epoch: 7/25, Step 3040, Train loss: 0.069\n",
      "Epoch: 7/25, Step 3080, Train loss: 0.065\n",
      "Epoch: 7/25, Step 3120, Train loss: 0.065\n",
      "Epoch: 7/25, Step 3160, Train loss: 0.067\n",
      "Epoch: 7/25, Step 3200, Train loss: 0.066\n",
      "Epoch: 7/25, Step 3240, Train loss: 0.069\n",
      "Epoch: 7/25, Step 3280, Train loss: 0.064\n",
      "Epoch: 7/25, Step 3320, Train loss: 0.068\n",
      "Epoch: 7/25, Step 3360, Train loss: 0.068\n",
      "Test loss decreased (0.062809 --> 0.059438).  Saving model ...\n",
      "Epoch 7/25, Train Loss: 0.066, Test Loss: 0.059\n",
      "Epoch: 8/25, Step 3400, Train loss: 0.070\n",
      "Epoch: 8/25, Step 3440, Train loss: 0.064\n",
      "Epoch: 8/25, Step 3480, Train loss: 0.062\n",
      "Epoch: 8/25, Step 3520, Train loss: 0.062\n",
      "Epoch: 8/25, Step 3560, Train loss: 0.071\n",
      "Epoch: 8/25, Step 3600, Train loss: 0.064\n",
      "Epoch: 8/25, Step 3640, Train loss: 0.068\n",
      "Epoch: 8/25, Step 3680, Train loss: 0.068\n",
      "Epoch: 8/25, Step 3720, Train loss: 0.064\n",
      "Epoch: 8/25, Step 3760, Train loss: 0.068\n",
      "Epoch: 8/25, Step 3800, Train loss: 0.065\n",
      "Epoch: 8/25, Step 3840, Train loss: 0.065\n",
      "Epoch 8/25, Train Loss: 0.066, Test Loss: 0.062\n",
      "Epoch: 9/25, Step 3880, Train loss: 0.067\n",
      "Epoch: 9/25, Step 3920, Train loss: 0.068\n",
      "Epoch: 9/25, Step 3960, Train loss: 0.065\n",
      "Epoch: 9/25, Step 4000, Train loss: 0.065\n",
      "Epoch: 9/25, Step 4040, Train loss: 0.066\n",
      "Epoch: 9/25, Step 4080, Train loss: 0.066\n",
      "Epoch: 9/25, Step 4120, Train loss: 0.065\n",
      "Epoch: 9/25, Step 4160, Train loss: 0.067\n",
      "Epoch: 9/25, Step 4200, Train loss: 0.067\n",
      "Epoch: 9/25, Step 4240, Train loss: 0.060\n",
      "Epoch: 9/25, Step 4280, Train loss: 0.067\n",
      "Epoch: 9/25, Step 4320, Train loss: 0.067\n",
      "Epoch 9/25, Train Loss: 0.066, Test Loss: 0.066\n",
      "Epoch: 10/25, Step 4360, Train loss: 0.067\n",
      "Epoch: 10/25, Step 4400, Train loss: 0.066\n",
      "Epoch: 10/25, Step 4440, Train loss: 0.063\n",
      "Epoch: 10/25, Step 4480, Train loss: 0.070\n",
      "Epoch: 10/25, Step 4520, Train loss: 0.064\n",
      "Epoch: 10/25, Step 4560, Train loss: 0.068\n",
      "Epoch: 10/25, Step 4600, Train loss: 0.066\n",
      "Epoch: 10/25, Step 4640, Train loss: 0.066\n",
      "Epoch: 10/25, Step 4680, Train loss: 0.066\n",
      "Epoch: 10/25, Step 4720, Train loss: 0.066\n",
      "Epoch: 10/25, Step 4760, Train loss: 0.063\n",
      "Epoch: 10/25, Step 4800, Train loss: 0.066\n",
      "Test loss decreased (0.059438 --> 0.057918).  Saving model ...\n",
      "Epoch 10/25, Train Loss: 0.066, Test Loss: 0.058\n",
      "Epoch: 11/25, Step 4840, Train loss: 0.067\n",
      "Epoch: 11/25, Step 4880, Train loss: 0.065\n",
      "Epoch: 11/25, Step 4920, Train loss: 0.066\n",
      "Epoch: 11/25, Step 4960, Train loss: 0.068\n",
      "Epoch: 11/25, Step 5000, Train loss: 0.067\n",
      "Epoch: 11/25, Step 5040, Train loss: 0.065\n",
      "Epoch: 11/25, Step 5080, Train loss: 0.061\n",
      "Epoch: 11/25, Step 5120, Train loss: 0.067\n",
      "Epoch: 11/25, Step 5160, Train loss: 0.067\n",
      "Epoch: 11/25, Step 5200, Train loss: 0.066\n",
      "Epoch: 11/25, Step 5240, Train loss: 0.064\n",
      "Epoch: 11/25, Step 5280, Train loss: 0.068\n",
      "Epoch 11/25, Train Loss: 0.066, Test Loss: 0.060\n",
      "Epoch: 12/25, Step 5320, Train loss: 0.065\n",
      "Epoch: 12/25, Step 5360, Train loss: 0.066\n",
      "Epoch: 12/25, Step 5400, Train loss: 0.068\n",
      "Epoch: 12/25, Step 5440, Train loss: 0.066\n",
      "Epoch: 12/25, Step 5480, Train loss: 0.067\n",
      "Epoch: 12/25, Step 5520, Train loss: 0.065\n",
      "Epoch: 12/25, Step 5560, Train loss: 0.065\n",
      "Epoch: 12/25, Step 5600, Train loss: 0.065\n",
      "Epoch: 12/25, Step 5640, Train loss: 0.068\n",
      "Epoch: 12/25, Step 5680, Train loss: 0.065\n",
      "Epoch: 12/25, Step 5720, Train loss: 0.067\n",
      "Epoch: 12/25, Step 5760, Train loss: 0.063\n",
      "Epoch 12/25, Train Loss: 0.066, Test Loss: 0.060\n",
      "Epoch: 13/25, Step 5800, Train loss: 0.063\n",
      "Epoch: 13/25, Step 5840, Train loss: 0.066\n",
      "Epoch: 13/25, Step 5880, Train loss: 0.066\n",
      "Epoch: 13/25, Step 5920, Train loss: 0.067\n",
      "Epoch: 13/25, Step 5960, Train loss: 0.068\n",
      "Epoch: 13/25, Step 6000, Train loss: 0.064\n",
      "Epoch: 13/25, Step 6040, Train loss: 0.066\n",
      "Epoch: 13/25, Step 6080, Train loss: 0.065\n",
      "Epoch: 13/25, Step 6120, Train loss: 0.067\n",
      "Epoch: 13/25, Step 6160, Train loss: 0.065\n",
      "Epoch: 13/25, Step 6200, Train loss: 0.067\n",
      "Epoch: 13/25, Step 6240, Train loss: 0.065\n",
      "Epoch 13/25, Train Loss: 0.066, Test Loss: 0.070\n",
      "Epoch: 14/25, Step 6280, Train loss: 0.064\n",
      "Epoch: 14/25, Step 6320, Train loss: 0.068\n",
      "Epoch: 14/25, Step 6360, Train loss: 0.065\n",
      "Epoch: 14/25, Step 6400, Train loss: 0.063\n",
      "Epoch: 14/25, Step 6440, Train loss: 0.069\n",
      "Epoch: 14/25, Step 6480, Train loss: 0.065\n",
      "Epoch: 14/25, Step 6520, Train loss: 0.067\n",
      "Epoch: 14/25, Step 6560, Train loss: 0.063\n",
      "Epoch: 14/25, Step 6600, Train loss: 0.069\n",
      "Epoch: 14/25, Step 6640, Train loss: 0.065\n",
      "Epoch: 14/25, Step 6680, Train loss: 0.067\n",
      "Epoch: 14/25, Step 6720, Train loss: 0.067\n",
      "Epoch 14/25, Train Loss: 0.066, Test Loss: 0.060\n",
      "Epoch: 15/25, Step 6760, Train loss: 0.070\n",
      "Epoch: 15/25, Step 6800, Train loss: 0.066\n",
      "Epoch: 15/25, Step 6840, Train loss: 0.066\n",
      "Epoch: 15/25, Step 6880, Train loss: 0.069\n",
      "Epoch: 15/25, Step 6920, Train loss: 0.068\n",
      "Epoch: 15/25, Step 6960, Train loss: 0.066\n",
      "Epoch: 15/25, Step 7000, Train loss: 0.062\n",
      "Epoch: 15/25, Step 7040, Train loss: 0.066\n",
      "Epoch: 15/25, Step 7080, Train loss: 0.064\n",
      "Epoch: 15/25, Step 7120, Train loss: 0.063\n",
      "Epoch: 15/25, Step 7160, Train loss: 0.064\n",
      "Epoch: 15/25, Step 7200, Train loss: 0.064\n",
      "Epoch 15/25, Train Loss: 0.066, Test Loss: 0.062\n",
      "Epoch: 16/25, Step 7240, Train loss: 0.065\n",
      "Epoch: 16/25, Step 7280, Train loss: 0.066\n",
      "Epoch: 16/25, Step 7320, Train loss: 0.067\n",
      "Epoch: 16/25, Step 7360, Train loss: 0.064\n",
      "Epoch: 16/25, Step 7400, Train loss: 0.067\n",
      "Epoch: 16/25, Step 7440, Train loss: 0.065\n",
      "Epoch: 16/25, Step 7480, Train loss: 0.067\n",
      "Epoch: 16/25, Step 7520, Train loss: 0.065\n",
      "Epoch: 16/25, Step 7560, Train loss: 0.066\n",
      "Epoch: 16/25, Step 7600, Train loss: 0.067\n",
      "Epoch: 16/25, Step 7640, Train loss: 0.067\n",
      "Epoch: 16/25, Step 7680, Train loss: 0.066\n",
      "Epoch 16/25, Train Loss: 0.066, Test Loss: 0.063\n",
      "Epoch: 17/25, Step 7720, Train loss: 0.069\n",
      "Epoch: 17/25, Step 7760, Train loss: 0.067\n",
      "Epoch: 17/25, Step 7800, Train loss: 0.067\n",
      "Epoch: 17/25, Step 7840, Train loss: 0.062\n",
      "Epoch: 17/25, Step 7880, Train loss: 0.064\n",
      "Epoch: 17/25, Step 7920, Train loss: 0.066\n",
      "Epoch: 17/25, Step 7960, Train loss: 0.063\n",
      "Epoch: 17/25, Step 8000, Train loss: 0.069\n",
      "Epoch: 17/25, Step 8040, Train loss: 0.063\n",
      "Epoch: 17/25, Step 8080, Train loss: 0.064\n",
      "Epoch: 17/25, Step 8120, Train loss: 0.064\n",
      "Epoch: 17/25, Step 8160, Train loss: 0.070\n",
      "Epoch 17/25, Train Loss: 0.066, Test Loss: 0.062\n",
      "Epoch: 18/25, Step 8200, Train loss: 0.066\n",
      "Epoch: 18/25, Step 8240, Train loss: 0.064\n",
      "Epoch: 18/25, Step 8280, Train loss: 0.066\n",
      "Epoch: 18/25, Step 8320, Train loss: 0.064\n",
      "Epoch: 18/25, Step 8360, Train loss: 0.070\n",
      "Epoch: 18/25, Step 8400, Train loss: 0.064\n",
      "Epoch: 18/25, Step 8440, Train loss: 0.065\n",
      "Epoch: 18/25, Step 8480, Train loss: 0.066\n",
      "Epoch: 18/25, Step 8520, Train loss: 0.067\n",
      "Epoch: 18/25, Step 8560, Train loss: 0.067\n",
      "Epoch: 18/25, Step 8600, Train loss: 0.064\n",
      "Epoch: 18/25, Step 8640, Train loss: 0.064\n",
      "Epoch 18/25, Train Loss: 0.066, Test Loss: 0.063\n",
      "Epoch: 19/25, Step 8680, Train loss: 0.069\n",
      "Epoch: 19/25, Step 8720, Train loss: 0.066\n",
      "Epoch: 19/25, Step 8760, Train loss: 0.066\n",
      "Epoch: 19/25, Step 8800, Train loss: 0.070\n",
      "Epoch: 19/25, Step 8840, Train loss: 0.063\n",
      "Epoch: 19/25, Step 8880, Train loss: 0.065\n",
      "Epoch: 19/25, Step 8920, Train loss: 0.065\n",
      "Epoch: 19/25, Step 8960, Train loss: 0.066\n",
      "Epoch: 19/25, Step 9000, Train loss: 0.064\n",
      "Epoch: 19/25, Step 9040, Train loss: 0.067\n",
      "Epoch: 19/25, Step 9080, Train loss: 0.066\n",
      "Epoch: 19/25, Step 9120, Train loss: 0.066\n",
      "Epoch 19/25, Train Loss: 0.066, Test Loss: 0.060\n",
      "Epoch: 20/25, Step 9160, Train loss: 0.064\n",
      "Epoch: 20/25, Step 9200, Train loss: 0.068\n",
      "Epoch: 20/25, Step 9240, Train loss: 0.065\n",
      "Epoch: 20/25, Step 9280, Train loss: 0.062\n",
      "Epoch: 20/25, Step 9320, Train loss: 0.064\n",
      "Epoch: 20/25, Step 9360, Train loss: 0.066\n",
      "Epoch: 20/25, Step 9400, Train loss: 0.064\n",
      "Epoch: 20/25, Step 9440, Train loss: 0.063\n",
      "Epoch: 20/25, Step 9480, Train loss: 0.069\n",
      "Epoch: 20/25, Step 9520, Train loss: 0.063\n",
      "Epoch: 20/25, Step 9560, Train loss: 0.069\n",
      "Epoch: 20/25, Step 9600, Train loss: 0.068\n",
      "Epoch 20/25, Train Loss: 0.066, Test Loss: 0.062\n",
      "Epoch: 21/25, Step 9640, Train loss: 0.067\n",
      "Epoch: 21/25, Step 9680, Train loss: 0.066\n",
      "Epoch: 21/25, Step 9720, Train loss: 0.064\n",
      "Epoch: 21/25, Step 9760, Train loss: 0.069\n",
      "Epoch: 21/25, Step 9800, Train loss: 0.066\n",
      "Epoch: 21/25, Step 9840, Train loss: 0.064\n",
      "Epoch: 21/25, Step 9880, Train loss: 0.067\n",
      "Epoch: 21/25, Step 9920, Train loss: 0.064\n",
      "Epoch: 21/25, Step 9960, Train loss: 0.065\n",
      "Epoch: 21/25, Step 10000, Train loss: 0.067\n",
      "Epoch: 21/25, Step 10040, Train loss: 0.064\n",
      "Epoch: 21/25, Step 10080, Train loss: 0.067\n",
      "Epoch 21/25, Train Loss: 0.066, Test Loss: 0.061\n",
      "Epoch: 22/25, Step 10120, Train loss: 0.066\n",
      "Epoch: 22/25, Step 10160, Train loss: 0.067\n",
      "Epoch: 22/25, Step 10200, Train loss: 0.066\n",
      "Epoch: 22/25, Step 10240, Train loss: 0.060\n",
      "Epoch: 22/25, Step 10280, Train loss: 0.066\n",
      "Epoch: 22/25, Step 10320, Train loss: 0.067\n",
      "Epoch: 22/25, Step 10360, Train loss: 0.066\n",
      "Epoch: 22/25, Step 10400, Train loss: 0.069\n",
      "Epoch: 22/25, Step 10440, Train loss: 0.066\n",
      "Epoch: 22/25, Step 10480, Train loss: 0.063\n",
      "Epoch: 22/25, Step 10520, Train loss: 0.066\n",
      "Epoch: 22/25, Step 10560, Train loss: 0.068\n",
      "Epoch 22/25, Train Loss: 0.066, Test Loss: 0.063\n",
      "Epoch: 23/25, Step 10600, Train loss: 0.066\n",
      "Epoch: 23/25, Step 10640, Train loss: 0.066\n",
      "Epoch: 23/25, Step 10680, Train loss: 0.065\n",
      "Epoch: 23/25, Step 10720, Train loss: 0.062\n",
      "Epoch: 23/25, Step 10760, Train loss: 0.069\n",
      "Epoch: 23/25, Step 10800, Train loss: 0.065\n",
      "Epoch: 23/25, Step 10840, Train loss: 0.064\n",
      "Epoch: 23/25, Step 10880, Train loss: 0.065\n",
      "Epoch: 23/25, Step 10920, Train loss: 0.065\n",
      "Epoch: 23/25, Step 10960, Train loss: 0.067\n",
      "Epoch: 23/25, Step 11000, Train loss: 0.067\n",
      "Epoch: 23/25, Step 11040, Train loss: 0.067\n",
      "Epoch 23/25, Train Loss: 0.066, Test Loss: 0.062\n",
      "Epoch: 24/25, Step 11080, Train loss: 0.067\n",
      "Epoch: 24/25, Step 11120, Train loss: 0.065\n",
      "Epoch: 24/25, Step 11160, Train loss: 0.065\n",
      "Epoch: 24/25, Step 11200, Train loss: 0.064\n",
      "Epoch: 24/25, Step 11240, Train loss: 0.065\n",
      "Epoch: 24/25, Step 11280, Train loss: 0.065\n",
      "Epoch: 24/25, Step 11320, Train loss: 0.068\n",
      "Epoch: 24/25, Step 11360, Train loss: 0.064\n",
      "Epoch: 24/25, Step 11400, Train loss: 0.066\n",
      "Epoch: 24/25, Step 11440, Train loss: 0.065\n",
      "Epoch: 24/25, Step 11480, Train loss: 0.066\n",
      "Epoch: 24/25, Step 11520, Train loss: 0.067\n",
      "Epoch 24/25, Train Loss: 0.066, Test Loss: 0.073\n",
      "Epoch: 25/25, Step 11560, Train loss: 0.067\n",
      "Epoch: 25/25, Step 11600, Train loss: 0.066\n",
      "Epoch: 25/25, Step 11640, Train loss: 0.064\n",
      "Epoch: 25/25, Step 11680, Train loss: 0.065\n",
      "Epoch: 25/25, Step 11720, Train loss: 0.066\n",
      "Epoch: 25/25, Step 11760, Train loss: 0.065\n",
      "Epoch: 25/25, Step 11800, Train loss: 0.068\n",
      "Epoch: 25/25, Step 11840, Train loss: 0.068\n",
      "Epoch: 25/25, Step 11880, Train loss: 0.067\n",
      "Epoch: 25/25, Step 11920, Train loss: 0.065\n",
      "Epoch: 25/25, Step 11960, Train loss: 0.065\n",
      "Epoch: 25/25, Step 12000, Train loss: 0.064\n",
      "Epoch 25/25, Train Loss: 0.066, Test Loss: 0.064\n",
      "Training with hidden dim: 5\n",
      "Epoch: 1/25, Step 40, Train loss: 0.120\n",
      "Epoch: 1/25, Step 80, Train loss: 0.116\n",
      "Epoch: 1/25, Step 120, Train loss: 0.111\n",
      "Epoch: 1/25, Step 160, Train loss: 0.115\n",
      "Epoch: 1/25, Step 200, Train loss: 0.113\n",
      "Epoch: 1/25, Step 240, Train loss: 0.116\n",
      "Epoch: 1/25, Step 280, Train loss: 0.115\n",
      "Epoch: 1/25, Step 320, Train loss: 0.114\n",
      "Epoch: 1/25, Step 360, Train loss: 0.113\n",
      "Epoch: 1/25, Step 400, Train loss: 0.117\n",
      "Epoch: 1/25, Step 440, Train loss: 0.113\n",
      "Epoch: 1/25, Step 480, Train loss: 0.117\n",
      "Test loss decreased (inf --> 0.113925).  Saving model ...\n",
      "Epoch 1/25, Train Loss: 0.115, Test Loss: 0.114\n",
      "Epoch: 2/25, Step 520, Train loss: 0.118\n",
      "Epoch: 2/25, Step 560, Train loss: 0.113\n",
      "Epoch: 2/25, Step 600, Train loss: 0.113\n",
      "Epoch: 2/25, Step 640, Train loss: 0.114\n",
      "Epoch: 2/25, Step 680, Train loss: 0.112\n",
      "Epoch: 2/25, Step 720, Train loss: 0.113\n",
      "Epoch: 2/25, Step 760, Train loss: 0.112\n",
      "Epoch: 2/25, Step 800, Train loss: 0.119\n",
      "Epoch: 2/25, Step 840, Train loss: 0.115\n",
      "Epoch: 2/25, Step 880, Train loss: 0.117\n",
      "Epoch: 2/25, Step 920, Train loss: 0.109\n",
      "Epoch: 2/25, Step 960, Train loss: 0.112\n",
      "Test loss decreased (0.113925 --> 0.103121).  Saving model ...\n",
      "Epoch 2/25, Train Loss: 0.114, Test Loss: 0.103\n",
      "Epoch: 3/25, Step 1000, Train loss: 0.113\n",
      "Epoch: 3/25, Step 1040, Train loss: 0.114\n",
      "Epoch: 3/25, Step 1080, Train loss: 0.112\n",
      "Epoch: 3/25, Step 1120, Train loss: 0.119\n",
      "Epoch: 3/25, Step 1160, Train loss: 0.113\n",
      "Epoch: 3/25, Step 1200, Train loss: 0.115\n",
      "Epoch: 3/25, Step 1240, Train loss: 0.119\n",
      "Epoch: 3/25, Step 1280, Train loss: 0.113\n",
      "Epoch: 3/25, Step 1320, Train loss: 0.112\n",
      "Epoch: 3/25, Step 1360, Train loss: 0.117\n",
      "Epoch: 3/25, Step 1400, Train loss: 0.113\n",
      "Epoch: 3/25, Step 1440, Train loss: 0.112\n",
      "Epoch 3/25, Train Loss: 0.114, Test Loss: 0.115\n",
      "Epoch: 4/25, Step 1480, Train loss: 0.117\n",
      "Epoch: 4/25, Step 1520, Train loss: 0.118\n",
      "Epoch: 4/25, Step 1560, Train loss: 0.115\n",
      "Epoch: 4/25, Step 1600, Train loss: 0.113\n",
      "Epoch: 4/25, Step 1640, Train loss: 0.111\n",
      "Epoch: 4/25, Step 1680, Train loss: 0.112\n",
      "Epoch: 4/25, Step 1720, Train loss: 0.112\n",
      "Epoch: 4/25, Step 1760, Train loss: 0.114\n",
      "Epoch: 4/25, Step 1800, Train loss: 0.112\n",
      "Epoch: 4/25, Step 1840, Train loss: 0.111\n",
      "Epoch: 4/25, Step 1880, Train loss: 0.113\n",
      "Epoch: 4/25, Step 1920, Train loss: 0.112\n",
      "Epoch 4/25, Train Loss: 0.113, Test Loss: 0.103\n",
      "Epoch: 5/25, Step 1960, Train loss: 0.112\n",
      "Epoch: 5/25, Step 2000, Train loss: 0.113\n",
      "Epoch: 5/25, Step 2040, Train loss: 0.112\n",
      "Epoch: 5/25, Step 2080, Train loss: 0.112\n",
      "Epoch: 5/25, Step 2120, Train loss: 0.111\n",
      "Epoch: 5/25, Step 2160, Train loss: 0.120\n",
      "Epoch: 5/25, Step 2200, Train loss: 0.115\n",
      "Epoch: 5/25, Step 2240, Train loss: 0.115\n",
      "Epoch: 5/25, Step 2280, Train loss: 0.114\n",
      "Epoch: 5/25, Step 2320, Train loss: 0.111\n",
      "Epoch: 5/25, Step 2360, Train loss: 0.115\n",
      "Epoch: 5/25, Step 2400, Train loss: 0.115\n",
      "Epoch 5/25, Train Loss: 0.114, Test Loss: 0.115\n",
      "Epoch: 6/25, Step 2440, Train loss: 0.121\n",
      "Epoch: 6/25, Step 2480, Train loss: 0.119\n",
      "Epoch: 6/25, Step 2520, Train loss: 0.109\n",
      "Epoch: 6/25, Step 2560, Train loss: 0.114\n",
      "Epoch: 6/25, Step 2600, Train loss: 0.114\n",
      "Epoch: 6/25, Step 2640, Train loss: 0.112\n",
      "Epoch: 6/25, Step 2680, Train loss: 0.113\n",
      "Epoch: 6/25, Step 2720, Train loss: 0.113\n",
      "Epoch: 6/25, Step 2760, Train loss: 0.110\n",
      "Epoch: 6/25, Step 2800, Train loss: 0.110\n",
      "Epoch: 6/25, Step 2840, Train loss: 0.114\n",
      "Epoch: 6/25, Step 2880, Train loss: 0.117\n",
      "Test loss decreased (0.103121 --> 0.098223).  Saving model ...\n",
      "Epoch 6/25, Train Loss: 0.114, Test Loss: 0.098\n",
      "Epoch: 7/25, Step 2920, Train loss: 0.113\n",
      "Epoch: 7/25, Step 2960, Train loss: 0.114\n",
      "Epoch: 7/25, Step 3000, Train loss: 0.113\n",
      "Epoch: 7/25, Step 3040, Train loss: 0.118\n",
      "Epoch: 7/25, Step 3080, Train loss: 0.113\n",
      "Epoch: 7/25, Step 3120, Train loss: 0.115\n",
      "Epoch: 7/25, Step 3160, Train loss: 0.117\n",
      "Epoch: 7/25, Step 3200, Train loss: 0.115\n",
      "Epoch: 7/25, Step 3240, Train loss: 0.107\n",
      "Epoch: 7/25, Step 3280, Train loss: 0.115\n",
      "Epoch: 7/25, Step 3320, Train loss: 0.116\n",
      "Epoch: 7/25, Step 3360, Train loss: 0.115\n",
      "Epoch 7/25, Train Loss: 0.114, Test Loss: 0.109\n",
      "Epoch: 8/25, Step 3400, Train loss: 0.117\n",
      "Epoch: 8/25, Step 3440, Train loss: 0.112\n",
      "Epoch: 8/25, Step 3480, Train loss: 0.111\n",
      "Epoch: 8/25, Step 3520, Train loss: 0.113\n",
      "Epoch: 8/25, Step 3560, Train loss: 0.116\n",
      "Epoch: 8/25, Step 3600, Train loss: 0.116\n",
      "Epoch: 8/25, Step 3640, Train loss: 0.109\n",
      "Epoch: 8/25, Step 3680, Train loss: 0.116\n",
      "Epoch: 8/25, Step 3720, Train loss: 0.118\n",
      "Epoch: 8/25, Step 3760, Train loss: 0.113\n",
      "Epoch: 8/25, Step 3800, Train loss: 0.107\n",
      "Epoch: 8/25, Step 3840, Train loss: 0.117\n",
      "Epoch 8/25, Train Loss: 0.114, Test Loss: 0.116\n",
      "Epoch: 9/25, Step 3880, Train loss: 0.108\n",
      "Epoch: 9/25, Step 3920, Train loss: 0.116\n",
      "Epoch: 9/25, Step 3960, Train loss: 0.115\n",
      "Epoch: 9/25, Step 4000, Train loss: 0.112\n",
      "Epoch: 9/25, Step 4040, Train loss: 0.111\n",
      "Epoch: 9/25, Step 4080, Train loss: 0.115\n",
      "Epoch: 9/25, Step 4120, Train loss: 0.120\n",
      "Epoch: 9/25, Step 4160, Train loss: 0.115\n",
      "Epoch: 9/25, Step 4200, Train loss: 0.116\n",
      "Epoch: 9/25, Step 4240, Train loss: 0.114\n",
      "Epoch: 9/25, Step 4280, Train loss: 0.109\n",
      "Epoch: 9/25, Step 4320, Train loss: 0.115\n",
      "Epoch 9/25, Train Loss: 0.114, Test Loss: 0.107\n",
      "Epoch: 10/25, Step 4360, Train loss: 0.112\n",
      "Epoch: 10/25, Step 4400, Train loss: 0.117\n",
      "Epoch: 10/25, Step 4440, Train loss: 0.118\n",
      "Epoch: 10/25, Step 4480, Train loss: 0.112\n",
      "Epoch: 10/25, Step 4520, Train loss: 0.110\n",
      "Epoch: 10/25, Step 4560, Train loss: 0.108\n",
      "Epoch: 10/25, Step 4600, Train loss: 0.116\n",
      "Epoch: 10/25, Step 4640, Train loss: 0.109\n",
      "Epoch: 10/25, Step 4680, Train loss: 0.115\n",
      "Epoch: 10/25, Step 4720, Train loss: 0.113\n",
      "Epoch: 10/25, Step 4760, Train loss: 0.110\n",
      "Epoch: 10/25, Step 4800, Train loss: 0.119\n",
      "Epoch 10/25, Train Loss: 0.113, Test Loss: 0.117\n",
      "Epoch: 11/25, Step 4840, Train loss: 0.108\n",
      "Epoch: 11/25, Step 4880, Train loss: 0.111\n",
      "Epoch: 11/25, Step 4920, Train loss: 0.111\n",
      "Epoch: 11/25, Step 4960, Train loss: 0.114\n",
      "Epoch: 11/25, Step 5000, Train loss: 0.116\n",
      "Epoch: 11/25, Step 5040, Train loss: 0.113\n",
      "Epoch: 11/25, Step 5080, Train loss: 0.121\n",
      "Epoch: 11/25, Step 5120, Train loss: 0.117\n",
      "Epoch: 11/25, Step 5160, Train loss: 0.113\n",
      "Epoch: 11/25, Step 5200, Train loss: 0.113\n",
      "Epoch: 11/25, Step 5240, Train loss: 0.115\n",
      "Epoch: 11/25, Step 5280, Train loss: 0.115\n",
      "Epoch 11/25, Train Loss: 0.114, Test Loss: 0.106\n",
      "Epoch: 12/25, Step 5320, Train loss: 0.114\n",
      "Epoch: 12/25, Step 5360, Train loss: 0.112\n",
      "Epoch: 12/25, Step 5400, Train loss: 0.110\n",
      "Epoch: 12/25, Step 5440, Train loss: 0.112\n",
      "Epoch: 12/25, Step 5480, Train loss: 0.118\n",
      "Epoch: 12/25, Step 5520, Train loss: 0.115\n",
      "Epoch: 12/25, Step 5560, Train loss: 0.116\n",
      "Epoch: 12/25, Step 5600, Train loss: 0.115\n",
      "Epoch: 12/25, Step 5640, Train loss: 0.112\n",
      "Epoch: 12/25, Step 5680, Train loss: 0.116\n",
      "Epoch: 12/25, Step 5720, Train loss: 0.114\n",
      "Epoch: 12/25, Step 5760, Train loss: 0.112\n",
      "Epoch 12/25, Train Loss: 0.114, Test Loss: 0.110\n",
      "Epoch: 13/25, Step 5800, Train loss: 0.111\n",
      "Epoch: 13/25, Step 5840, Train loss: 0.112\n",
      "Epoch: 13/25, Step 5880, Train loss: 0.116\n",
      "Epoch: 13/25, Step 5920, Train loss: 0.114\n",
      "Epoch: 13/25, Step 5960, Train loss: 0.113\n",
      "Epoch: 13/25, Step 6000, Train loss: 0.113\n",
      "Epoch: 13/25, Step 6040, Train loss: 0.112\n",
      "Epoch: 13/25, Step 6080, Train loss: 0.113\n",
      "Epoch: 13/25, Step 6120, Train loss: 0.113\n",
      "Epoch: 13/25, Step 6160, Train loss: 0.118\n",
      "Epoch: 13/25, Step 6200, Train loss: 0.116\n",
      "Epoch: 13/25, Step 6240, Train loss: 0.112\n",
      "Epoch 13/25, Train Loss: 0.114, Test Loss: 0.117\n",
      "Epoch: 14/25, Step 6280, Train loss: 0.114\n",
      "Epoch: 14/25, Step 6320, Train loss: 0.113\n",
      "Epoch: 14/25, Step 6360, Train loss: 0.106\n",
      "Epoch: 14/25, Step 6400, Train loss: 0.116\n",
      "Epoch: 14/25, Step 6440, Train loss: 0.119\n",
      "Epoch: 14/25, Step 6480, Train loss: 0.112\n",
      "Epoch: 14/25, Step 6520, Train loss: 0.107\n",
      "Epoch: 14/25, Step 6560, Train loss: 0.118\n",
      "Epoch: 14/25, Step 6600, Train loss: 0.115\n",
      "Epoch: 14/25, Step 6640, Train loss: 0.117\n",
      "Epoch: 14/25, Step 6680, Train loss: 0.115\n",
      "Epoch: 14/25, Step 6720, Train loss: 0.112\n",
      "Epoch 14/25, Train Loss: 0.114, Test Loss: 0.109\n",
      "Epoch: 15/25, Step 6760, Train loss: 0.118\n",
      "Epoch: 15/25, Step 6800, Train loss: 0.113\n",
      "Epoch: 15/25, Step 6840, Train loss: 0.115\n",
      "Epoch: 15/25, Step 6880, Train loss: 0.116\n",
      "Epoch: 15/25, Step 6920, Train loss: 0.118\n",
      "Epoch: 15/25, Step 6960, Train loss: 0.113\n",
      "Epoch: 15/25, Step 7000, Train loss: 0.114\n",
      "Epoch: 15/25, Step 7040, Train loss: 0.112\n",
      "Epoch: 15/25, Step 7080, Train loss: 0.116\n",
      "Epoch: 15/25, Step 7120, Train loss: 0.107\n",
      "Epoch: 15/25, Step 7160, Train loss: 0.109\n",
      "Epoch: 15/25, Step 7200, Train loss: 0.108\n",
      "Epoch 15/25, Train Loss: 0.113, Test Loss: 0.113\n",
      "Epoch: 16/25, Step 7240, Train loss: 0.119\n",
      "Epoch: 16/25, Step 7280, Train loss: 0.108\n",
      "Epoch: 16/25, Step 7320, Train loss: 0.109\n",
      "Epoch: 16/25, Step 7360, Train loss: 0.115\n",
      "Epoch: 16/25, Step 7400, Train loss: 0.111\n",
      "Epoch: 16/25, Step 7440, Train loss: 0.117\n",
      "Epoch: 16/25, Step 7480, Train loss: 0.110\n",
      "Epoch: 16/25, Step 7520, Train loss: 0.112\n",
      "Epoch: 16/25, Step 7560, Train loss: 0.116\n",
      "Epoch: 16/25, Step 7600, Train loss: 0.120\n",
      "Epoch: 16/25, Step 7640, Train loss: 0.111\n",
      "Epoch: 16/25, Step 7680, Train loss: 0.118\n",
      "Epoch 16/25, Train Loss: 0.114, Test Loss: 0.103\n",
      "Epoch: 17/25, Step 7720, Train loss: 0.112\n",
      "Epoch: 17/25, Step 7760, Train loss: 0.113\n",
      "Epoch: 17/25, Step 7800, Train loss: 0.112\n",
      "Epoch: 17/25, Step 7840, Train loss: 0.114\n",
      "Epoch: 17/25, Step 7880, Train loss: 0.115\n",
      "Epoch: 17/25, Step 7920, Train loss: 0.111\n",
      "Epoch: 17/25, Step 7960, Train loss: 0.111\n",
      "Epoch: 17/25, Step 8000, Train loss: 0.113\n",
      "Epoch: 17/25, Step 8040, Train loss: 0.110\n",
      "Epoch: 17/25, Step 8080, Train loss: 0.117\n",
      "Epoch: 17/25, Step 8120, Train loss: 0.116\n",
      "Epoch: 17/25, Step 8160, Train loss: 0.117\n",
      "Epoch 17/25, Train Loss: 0.113, Test Loss: 0.107\n",
      "Epoch: 18/25, Step 8200, Train loss: 0.118\n",
      "Epoch: 18/25, Step 8240, Train loss: 0.113\n",
      "Epoch: 18/25, Step 8280, Train loss: 0.111\n",
      "Epoch: 18/25, Step 8320, Train loss: 0.111\n",
      "Epoch: 18/25, Step 8360, Train loss: 0.117\n",
      "Epoch: 18/25, Step 8400, Train loss: 0.114\n",
      "Epoch: 18/25, Step 8440, Train loss: 0.116\n",
      "Epoch: 18/25, Step 8480, Train loss: 0.116\n",
      "Epoch: 18/25, Step 8520, Train loss: 0.114\n",
      "Epoch: 18/25, Step 8560, Train loss: 0.110\n",
      "Epoch: 18/25, Step 8600, Train loss: 0.113\n",
      "Epoch: 18/25, Step 8640, Train loss: 0.112\n",
      "Epoch 18/25, Train Loss: 0.114, Test Loss: 0.105\n",
      "Epoch: 19/25, Step 8680, Train loss: 0.119\n",
      "Epoch: 19/25, Step 8720, Train loss: 0.119\n",
      "Epoch: 19/25, Step 8760, Train loss: 0.115\n",
      "Epoch: 19/25, Step 8800, Train loss: 0.113\n",
      "Epoch: 19/25, Step 8840, Train loss: 0.115\n",
      "Epoch: 19/25, Step 8880, Train loss: 0.114\n",
      "Epoch: 19/25, Step 8920, Train loss: 0.110\n",
      "Epoch: 19/25, Step 8960, Train loss: 0.112\n",
      "Epoch: 19/25, Step 9000, Train loss: 0.106\n",
      "Epoch: 19/25, Step 9040, Train loss: 0.115\n",
      "Epoch: 19/25, Step 9080, Train loss: 0.107\n",
      "Epoch: 19/25, Step 9120, Train loss: 0.114\n",
      "Epoch 19/25, Train Loss: 0.113, Test Loss: 0.103\n",
      "Epoch: 20/25, Step 9160, Train loss: 0.111\n",
      "Epoch: 20/25, Step 9200, Train loss: 0.116\n",
      "Epoch: 20/25, Step 9240, Train loss: 0.118\n",
      "Epoch: 20/25, Step 9280, Train loss: 0.108\n",
      "Epoch: 20/25, Step 9320, Train loss: 0.113\n",
      "Epoch: 20/25, Step 9360, Train loss: 0.114\n",
      "Epoch: 20/25, Step 9400, Train loss: 0.113\n",
      "Epoch: 20/25, Step 9440, Train loss: 0.113\n",
      "Epoch: 20/25, Step 9480, Train loss: 0.112\n",
      "Epoch: 20/25, Step 9520, Train loss: 0.112\n",
      "Epoch: 20/25, Step 9560, Train loss: 0.112\n",
      "Epoch: 20/25, Step 9600, Train loss: 0.117\n",
      "Epoch 20/25, Train Loss: 0.113, Test Loss: 0.115\n",
      "Epoch: 21/25, Step 9640, Train loss: 0.110\n",
      "Epoch: 21/25, Step 9680, Train loss: 0.114\n",
      "Epoch: 21/25, Step 9720, Train loss: 0.118\n",
      "Epoch: 21/25, Step 9760, Train loss: 0.108\n",
      "Epoch: 21/25, Step 9800, Train loss: 0.111\n",
      "Epoch: 21/25, Step 9840, Train loss: 0.113\n",
      "Epoch: 21/25, Step 9880, Train loss: 0.110\n",
      "Epoch: 21/25, Step 9920, Train loss: 0.110\n",
      "Epoch: 21/25, Step 9960, Train loss: 0.114\n",
      "Epoch: 21/25, Step 10000, Train loss: 0.113\n",
      "Epoch: 21/25, Step 10040, Train loss: 0.116\n",
      "Epoch: 21/25, Step 10080, Train loss: 0.121\n",
      "Epoch 21/25, Train Loss: 0.113, Test Loss: 0.113\n",
      "Epoch: 22/25, Step 10120, Train loss: 0.114\n",
      "Epoch: 22/25, Step 10160, Train loss: 0.121\n",
      "Epoch: 22/25, Step 10200, Train loss: 0.114\n",
      "Epoch: 22/25, Step 10240, Train loss: 0.116\n",
      "Epoch: 22/25, Step 10280, Train loss: 0.115\n",
      "Epoch: 22/25, Step 10320, Train loss: 0.109\n",
      "Epoch: 22/25, Step 10360, Train loss: 0.110\n",
      "Epoch: 22/25, Step 10400, Train loss: 0.116\n",
      "Epoch: 22/25, Step 10440, Train loss: 0.113\n",
      "Epoch: 22/25, Step 10480, Train loss: 0.111\n",
      "Epoch: 22/25, Step 10520, Train loss: 0.114\n",
      "Epoch: 22/25, Step 10560, Train loss: 0.113\n",
      "Epoch 22/25, Train Loss: 0.114, Test Loss: 0.118\n",
      "Epoch: 23/25, Step 10600, Train loss: 0.113\n",
      "Epoch: 23/25, Step 10640, Train loss: 0.116\n",
      "Epoch: 23/25, Step 10680, Train loss: 0.112\n",
      "Epoch: 23/25, Step 10720, Train loss: 0.112\n",
      "Epoch: 23/25, Step 10760, Train loss: 0.114\n",
      "Epoch: 23/25, Step 10800, Train loss: 0.113\n",
      "Epoch: 23/25, Step 10840, Train loss: 0.109\n",
      "Epoch: 23/25, Step 10880, Train loss: 0.116\n",
      "Epoch: 23/25, Step 10920, Train loss: 0.113\n",
      "Epoch: 23/25, Step 10960, Train loss: 0.120\n",
      "Epoch: 23/25, Step 11000, Train loss: 0.114\n",
      "Epoch: 23/25, Step 11040, Train loss: 0.111\n",
      "Epoch 23/25, Train Loss: 0.114, Test Loss: 0.120\n",
      "Epoch: 24/25, Step 11080, Train loss: 0.118\n",
      "Epoch: 24/25, Step 11120, Train loss: 0.110\n",
      "Epoch: 24/25, Step 11160, Train loss: 0.114\n",
      "Epoch: 24/25, Step 11200, Train loss: 0.113\n",
      "Epoch: 24/25, Step 11240, Train loss: 0.115\n",
      "Epoch: 24/25, Step 11280, Train loss: 0.112\n",
      "Epoch: 24/25, Step 11320, Train loss: 0.112\n",
      "Epoch: 24/25, Step 11360, Train loss: 0.114\n",
      "Epoch: 24/25, Step 11400, Train loss: 0.114\n",
      "Epoch: 24/25, Step 11440, Train loss: 0.115\n",
      "Epoch: 24/25, Step 11480, Train loss: 0.113\n",
      "Epoch: 24/25, Step 11520, Train loss: 0.112\n",
      "Epoch 24/25, Train Loss: 0.113, Test Loss: 0.113\n",
      "Epoch: 25/25, Step 11560, Train loss: 0.116\n",
      "Epoch: 25/25, Step 11600, Train loss: 0.111\n",
      "Epoch: 25/25, Step 11640, Train loss: 0.118\n",
      "Epoch: 25/25, Step 11680, Train loss: 0.113\n",
      "Epoch: 25/25, Step 11720, Train loss: 0.113\n",
      "Epoch: 25/25, Step 11760, Train loss: 0.113\n",
      "Epoch: 25/25, Step 11800, Train loss: 0.116\n",
      "Epoch: 25/25, Step 11840, Train loss: 0.111\n",
      "Epoch: 25/25, Step 11880, Train loss: 0.117\n",
      "Epoch: 25/25, Step 11920, Train loss: 0.117\n",
      "Epoch: 25/25, Step 11960, Train loss: 0.107\n",
      "Epoch: 25/25, Step 12000, Train loss: 0.108\n",
      "Epoch 25/25, Train Loss: 0.113, Test Loss: 0.117\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, steps = train_model(model=model, goal_hidden_dim=5, \n",
    "                                         optimizer=optimizer, loss_func=criterion, epochs=epochs,\n",
    "                                         trainloader=trainloader, testloader=testloader, print_every=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=32, out_features=1, bias=True)\n",
      "BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.bottleneck:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bottleneck[0].out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
