{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PCAAutoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, last_hidden_shape):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList(encoder)  # Ensure encoder is a ModuleList\n",
    "        self.decoder = nn.ModuleList(decoder)  # Ensure decoder is a ModuleList\n",
    "        self.last_hidden_shape = last_hidden_shape\n",
    "        self.bottleneck = nn.ModuleList([nn.Linear(in_features=self.last_hidden_shape, out_features=1),\n",
    "                                         nn.BatchNorm1d(num_features=1, affine=False)])\n",
    "\n",
    "    def increase_latentdim(self):\n",
    "        # Create new bottleneck expansion layer\n",
    "        new_bottleneck = nn.ModuleList([nn.Linear(in_features=self.last_hidden_shape, out_features=self.bottleneck[0].out_features + 1),\n",
    "                                        nn.BatchNorm1d(num_features=self.bottleneck[0].out_features + 1, affine=False)])\n",
    "        # Copying weights while freezing old neurons\n",
    "        with torch.no_grad():\n",
    "            new_bottleneck[0].weight[: self.bottleneck[0].out_features] = self.bottleneck[0].weight\n",
    "            new_bottleneck[0].bias[: self.bottleneck[0].out_features] = self.bottleneck[0].bias\n",
    "\n",
    "        self.bottleneck = new_bottleneck  # Replace the layer\n",
    "        self.bottleneck[0].requires_grad_(True)  # Allow gradients\n",
    "\n",
    "        # Freeze the old neurons using a hook\n",
    "        self.bottleneck[0].weight.register_hook(self._freeze_old_neurons_hook)\n",
    "        self.bottleneck[0].bias.register_hook(self._freeze_old_neurons_hook)\n",
    "\n",
    "        # Turn off gradients for all layers in the encoder (just in case)\n",
    "        for layer in self.encoder:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self._recreate_decoder()\n",
    "\n",
    "    def _freeze_old_neurons_hook(self, grad):\n",
    "        \"\"\"Backward hook: Freeze gradients for old neurons, allowing updates only for new ones\"\"\"\n",
    "        grad[: -1] = 0  # Zero out gradients for old neurons\n",
    "        return grad\n",
    "\n",
    "    def _recreate_decoder(self):\n",
    "        # Copying old decoder to new\n",
    "        new_decoder = nn.ModuleList()\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            if i == 0 and isinstance(layer, nn.Linear):\n",
    "                new_layer = nn.Linear(layer.in_features + 1, layer.out_features)\n",
    "                nn.init.xavier_uniform_(new_layer.weight)\n",
    "                if new_layer.bias is not None:\n",
    "                    nn.init.zeros_(new_layer.bias)\n",
    "                new_decoder.append(new_layer)\n",
    "            else:\n",
    "                new_decoder.append(layer)\n",
    "        \n",
    "        self.decoder = new_decoder  # Ensure it's still a ModuleList\n",
    "\n",
    "    def encode(self, x):\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        for layer in self.bottleneck:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "        return x  # Return the output\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encode(x)\n",
    "        out = self.decode(enc)\n",
    "        return out, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAAE_Loss(nn.Module):\n",
    "    def __init__(self, loss_func, lambda_cov=0.01):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        self.lambda_cov = lambda_cov\n",
    "\n",
    "    \n",
    "    def forward(self, y_hat, y, z):\n",
    "        recon_loss = self.loss_func(y_hat, y)\n",
    "\n",
    "        batch_size, latent_dim = z.shape\n",
    "        z_mean = torch.mean(z, dim=0, keepdim=True)\n",
    "        z_centered = z - z_mean\n",
    "\n",
    "        covariance_matrix = (z_centered.T @ z_centered) / batch_size\n",
    "        covariance_loss = torch.sum(covariance_matrix**2) - torch.sum(torch.diagonal(covariance_matrix)**2)\n",
    "\n",
    "        total_loss = recon_loss + self.lambda_cov * covariance_loss\n",
    "        return total_loss, recon_loss, covariance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, goal_hidden_dim, optimizer, loss_func, epochs, trainloader, testloader, print_every):\n",
    "    writer = SummaryWriter()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    outer_steps = 0\n",
    "    total_steps = 0\n",
    "    total_train_losses, total_test_losses = [], []\n",
    "    if isinstance(loss_func, PCAAE_Loss):\n",
    "        total_train_recon_losses, total_test_recon_losses = [], []\n",
    "        total_train_cov_losses, total_test_cov_losses = [], []\n",
    "    total_min_testloss = np.Inf\n",
    "    hidden_dim = 1\n",
    "    \n",
    "    while hidden_dim != goal_hidden_dim:\n",
    "        if not outer_steps == 0:\n",
    "            # Increasing latent space\n",
    "            model.increase_latentdim()\n",
    "            hidden_dim += 1\n",
    "            model.to(device)\n",
    "\n",
    "        outer_steps += 1\n",
    "        print(f\"Training with hidden dim: {hidden_dim}\")\n",
    "        steps = 0\n",
    "        train_losses, test_losses = [], []\n",
    "        if isinstance(loss_func, PCAAE_Loss):\n",
    "            train_recon_losses, test_recon_losses = [], []\n",
    "            train_cov_losses, test_cov_losses = [], []\n",
    "        min_test_loss = np.Inf\n",
    "\n",
    "        # Training loop\n",
    "        for e in range(epochs):\n",
    "            running_loss = 0\n",
    "            # Only for printing it\n",
    "            running_loss_ = 0\n",
    "            if isinstance(loss_func, PCAAE_Loss):\n",
    "                running_recon_loss = 0\n",
    "                running_cov_loss = 0\n",
    "                running_recon_loss_ = 0\n",
    "                running_cov_loss_ = 0\n",
    "\n",
    "            for X, y in trainloader:\n",
    "                steps += 1\n",
    "                total_steps += 1\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                y_hat, hidden = model(X)\n",
    "                if isinstance(loss_func, PCAAE_Loss):\n",
    "                    loss, recon_loss, cov_loss = loss_func(y_hat, y, hidden)\n",
    "                    running_recon_loss += recon_loss.item()*X.size(0)\n",
    "                    running_cov_loss += cov_loss.item()*X.size(0)\n",
    "                    running_recon_loss_ += recon_loss.item()\n",
    "                    running_cov_loss_ += cov_loss.item()\n",
    "                else:\n",
    "                    loss = loss_func(y_hat, y)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()*X.size(0)\n",
    "                running_loss_ += loss.item()\n",
    "\n",
    "                if steps % print_every == 0:\n",
    "                    if isinstance(loss_func, PCAAE_Loss):\n",
    "                        print(f\"Epoch: {e + 1}/{epochs}, Step {steps}, Train loss: {running_loss_/print_every:.3f} \" \n",
    "                              f\"Train reconstruction loss: {running_recon_loss_/print_every:.3f} \"\n",
    "                              f\"Train covariance loss: {running_cov_loss_/print_every:.3f}\")\n",
    "                        writer.add_scalar(\"Loss x steps/train\", running_loss_/print_every, steps)\n",
    "                        writer.add_scalar(\"Reconstruction loss x steps/train\", running_recon_loss/print_every, steps)\n",
    "                        writer.add_scalar(\"Covariance loss x steps/train\", running_cov_loss_/print_every, steps)\n",
    "                    else:\n",
    "                        print(f\"Epoch: {e + 1}/{epochs}, Step {steps}, Train loss: {running_loss_/print_every:.3f}\")\n",
    "                        writer.add_scalar(\"Loss x steps/train\", running_loss_/print_every, steps)\n",
    "                    running_loss_ = 0\n",
    "\n",
    "                    if isinstance(loss_func, PCAAE_Loss):\n",
    "                        running_recon_loss_ = 0\n",
    "                        running_cov_loss_ = 0\n",
    "\n",
    "            # Running model on the test data  \n",
    "            else:\n",
    "                running_testloss = 0\n",
    "                if isinstance(loss_func, PCAAE_Loss):\n",
    "                    running_test_recon_loss = 0\n",
    "                    running_test_cov_loss = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    for X, y in testloader:\n",
    "                        X, y = X.to(device), y.to(device)\n",
    "                        y_hat, hidden = model(X)\n",
    "                        if isinstance(loss_func, PCAAE_Loss):\n",
    "                            test_loss, test_recon_loss, test_cov_loss = loss_func(y_hat, y, hidden)\n",
    "                            running_test_recon_loss += test_recon_loss.item()*X.size(0)\n",
    "                            running_test_cov_loss += test_cov_loss.item()*X.size(0)\n",
    "                        else:\n",
    "                            loss = loss_func(y_hat, y)\n",
    "                        running_testloss += test_loss.item()*X.size(0)\n",
    "\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                train_losses.append(running_loss/len(trainloader.dataset))\n",
    "                total_train_losses.append(running_loss/len(trainloader.dataset))\n",
    "                test_losses.append(running_testloss/len(testloader.dataset))\n",
    "                total_test_losses.append(running_testloss/len(testloader.dataset))\n",
    "\n",
    "                if isinstance(loss_func, PCAAE_Loss):\n",
    "                    train_recon_losses.append(running_recon_loss/len(trainloader.dataset))\n",
    "                    total_train_recon_losses.append(running_recon_loss/len(trainloader.dataset))\n",
    "                    train_cov_losses.append(running_cov_loss/len(trainloader.dataset))\n",
    "                    total_train_cov_losses.append(running_cov_loss/len(trainloader.dataset))\n",
    "                    test_recon_losses.append(running_test_recon_loss/len(testloader.dataset))\n",
    "                    total_test_recon_losses.append(running_test_recon_loss/len(testloader.dataset))\n",
    "                    test_cov_losses.append(running_test_cov_loss/len(testloader.dataset))\n",
    "                    total_test_cov_losses.append(running_test_cov_loss/len(testloader.dataset))\n",
    "\n",
    "                # Saving model when test loss improved\n",
    "                if test_losses[-1] <= min_test_loss:\n",
    "                    print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(min_test_loss,test_losses[-1]))\n",
    "                    torch.save(model.state_dict(), f'PCAAE_hidden_dim{hidden_dim}.pt')\n",
    "                    min_test_loss = test_losses[-1]\n",
    "                \n",
    "                #TODO - Add logging of test losses to the tensorboard session as well\n",
    "                print(f\"Epoch {e+1}/{epochs},\\n Train Loss: {running_loss/len(trainloader.dataset):.3f} \"\n",
    "                      f\"Train reconstruction loss: {running_recon_loss_/len(trainloader.dataset):.3f} \"\n",
    "                      f\"Train covariance loss: {running_cov_loss_/len(trainloader.dataset):.3f}\\n\"\n",
    "                      f\"Test Loss: {running_testloss/len(testloader.dataset):.3f} \"\n",
    "                      f\"Test reconstruction loss: {running_test_recon_loss/len(testloader.dataset):.3f} \"\n",
    "                      f\"Test covariance loss: {running_test_cov_loss/len(testloader.dataset):.3f}\")\n",
    "\n",
    "    logs =  {\"train_losses\": total_train_losses, \n",
    "             \"train_recon_losses\": total_train_recon_losses, \n",
    "             \"train_cov_losses\": total_train_cov_losses, \n",
    "             \"test_losses\": total_test_losses, \n",
    "             \"test_recon_losses\": total_test_recon_losses, \n",
    "             \"test_cov_losses\": total_test_cov_losses, \n",
    "             \"steps\": total_steps}\n",
    "    \n",
    "    writer.clos()\n",
    "    return logs           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for tabular data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "inputFeature = pd.read_csv('../Data/NIBRS_ND_2021/processed/input.csv', index_col='Unnamed: 0')\n",
    "# Separating numerical and categorical features\n",
    "numerical_features=['population','victim_seq_num','age_num_victim','incident_hour','incident_month','incident_day','incident_dayofmonth','incident_weekofyear']\n",
    "categorical_features = ['resident_status_code','race_desc_victim',\n",
    "'ethnicity_name_victim','pub_agency_name','offense_name','location_name','weapon_name'\n",
    ",'injury_name','relationship_name','incident_isweekend']\n",
    "# Onehot-encoding categorical features\n",
    "inputFeature_1h = pd.get_dummies(inputFeature, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object columns to numeric if they represent categories\n",
    "for column in inputFeature_1h.select_dtypes(include=['object']):\n",
    "    inputFeature_1h[column] = inputFeature_1h[column].astype('category').cat.codes\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(inputFeature_1h, test_size=0.1, random_state=42)\n",
    "\n",
    "# Normalizing numerical features\n",
    "for feature in numerical_features:\n",
    "  train[feature] = (train[feature] - train[feature].min()) / (train[feature].max() - train[feature].min())\n",
    "  test[feature] = (test[feature] - test[feature].min()) / (test[feature].max() - test[feature].min())\n",
    "\n",
    "# Converting data to tensors\n",
    "X_train = torch.nan_to_num(torch.Tensor(train.values.astype(np.float32)))\n",
    "y_train = torch.nan_to_num(torch.Tensor(train.values.astype(np.float32)))\n",
    "\n",
    "X_test = torch.nan_to_num(torch.Tensor(test.values.astype(np.float32)))\n",
    "y_test = torch.nan_to_num(torch.Tensor(test.values.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "trainset = MyDataset(X_train, y_train)\n",
    "testset = MyDataset(X_test, y_test)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sizes for the layers\n",
    "layer_sizes = [227, 64, 32]  # Example decreasing sizes for the encoder\n",
    "\n",
    "# Create the encoder ModuleList\n",
    "encoder = nn.ModuleList()\n",
    "for i in range(len(layer_sizes) - 1):\n",
    "    encoder.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "    encoder.append(nn.ReLU())\n",
    "\n",
    "# Create the decoder ModuleList (mirror of the encoder)\n",
    "decoder = nn.ModuleList()\n",
    "decoder.append(nn.Linear(1, layer_sizes[len(layer_sizes) - 1]))\n",
    "for i in range(len(layer_sizes) - 1, 0, -1):\n",
    "    decoder.append(nn.Linear(layer_sizes[i], layer_sizes[i - 1]))\n",
    "    decoder.append(nn.ReLU())\n",
    "\n",
    "# Remove the last ReLU from the decoder (optional, depending on use case)\n",
    "decoder = decoder[:-1]\n",
    "criterion = PCAAE_Loss(nn.MSELoss())\n",
    "model = PCAAutoencoder(encoder, decoder, 32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 25\n",
    "print_every = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden dim: 1\n",
      "Epoch: 1/25, Step 40, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.720\n",
      "Epoch: 1/25, Step 80, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.712\n",
      "Epoch: 1/25, Step 120, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.662\n",
      "Epoch: 1/25, Step 160, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.710\n",
      "Epoch: 1/25, Step 200, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.688\n",
      "Epoch: 1/25, Step 240, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.501\n",
      "Epoch: 1/25, Step 280, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.696\n",
      "Epoch: 1/25, Step 320, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.608\n",
      "Epoch: 1/25, Step 360, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.648\n",
      "Epoch: 1/25, Step 400, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.634\n",
      "Epoch: 1/25, Step 440, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.524\n",
      "Epoch: 1/25, Step 480, Train loss: 0.098 Train reconstruction loss: 0.018 Train covariance loss: 7.988\n",
      "Test loss decreased (inf --> 0.102196).  Saving model ...\n",
      "Epoch 1/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.102 Test reconstruction loss: 0.018 Test covariance loss: 8.455\n",
      "Epoch: 2/25, Step 520, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.533\n",
      "Epoch: 2/25, Step 560, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.814\n",
      "Epoch: 2/25, Step 600, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.710\n",
      "Epoch: 2/25, Step 640, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.745\n",
      "Epoch: 2/25, Step 680, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.727\n",
      "Epoch: 2/25, Step 720, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.819\n",
      "Epoch: 2/25, Step 760, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.637\n",
      "Epoch: 2/25, Step 800, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.746\n",
      "Epoch: 2/25, Step 840, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.555\n",
      "Epoch: 2/25, Step 880, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.838\n",
      "Epoch: 2/25, Step 920, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.759\n",
      "Epoch: 2/25, Step 960, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.714\n",
      "Test loss decreased (0.102196 --> 0.093017).  Saving model ...\n",
      "Epoch 2/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.093 Test reconstruction loss: 0.018 Test covariance loss: 7.538\n",
      "Epoch: 3/25, Step 1000, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.616\n",
      "Epoch: 3/25, Step 1040, Train loss: 0.098 Train reconstruction loss: 0.018 Train covariance loss: 7.924\n",
      "Epoch: 3/25, Step 1080, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.757\n",
      "Epoch: 3/25, Step 1120, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.745\n",
      "Epoch: 3/25, Step 1160, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.636\n",
      "Epoch: 3/25, Step 1200, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.616\n",
      "Epoch: 3/25, Step 1240, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.687\n",
      "Epoch: 3/25, Step 1280, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.740\n",
      "Epoch: 3/25, Step 1320, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.601\n",
      "Epoch: 3/25, Step 1360, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.504\n",
      "Epoch: 3/25, Step 1400, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.500\n",
      "Epoch: 3/25, Step 1440, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.787\n",
      "Epoch 3/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.101 Test reconstruction loss: 0.018 Test covariance loss: 8.333\n",
      "Epoch: 4/25, Step 1480, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.796\n",
      "Epoch: 4/25, Step 1520, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.541\n",
      "Epoch: 4/25, Step 1560, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.658\n",
      "Epoch: 4/25, Step 1600, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.695\n",
      "Epoch: 4/25, Step 1640, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.550\n",
      "Epoch: 4/25, Step 1680, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.675\n",
      "Epoch: 4/25, Step 1720, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.579\n",
      "Epoch: 4/25, Step 1760, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.552\n",
      "Epoch: 4/25, Step 1800, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.568\n",
      "Epoch: 4/25, Step 1840, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.700\n",
      "Epoch: 4/25, Step 1880, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.792\n",
      "Epoch: 4/25, Step 1920, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.815\n",
      "Epoch 4/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.095 Test reconstruction loss: 0.018 Test covariance loss: 7.782\n",
      "Epoch: 5/25, Step 1960, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.669\n",
      "Epoch: 5/25, Step 2000, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.802\n",
      "Epoch: 5/25, Step 2040, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.653\n",
      "Epoch: 5/25, Step 2080, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.686\n",
      "Epoch: 5/25, Step 2120, Train loss: 0.095 Train reconstruction loss: 0.019 Train covariance loss: 7.628\n",
      "Epoch: 5/25, Step 2160, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.775\n",
      "Epoch: 5/25, Step 2200, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.823\n",
      "Epoch: 5/25, Step 2240, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.790\n",
      "Epoch: 5/25, Step 2280, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.545\n",
      "Epoch: 5/25, Step 2320, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.753\n",
      "Epoch: 5/25, Step 2360, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.538\n",
      "Epoch: 5/25, Step 2400, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.738\n",
      "Epoch 5/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.095 Test reconstruction loss: 0.018 Test covariance loss: 7.755\n",
      "Epoch: 6/25, Step 2440, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.620\n",
      "Epoch: 6/25, Step 2480, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.463\n",
      "Epoch: 6/25, Step 2520, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.612\n",
      "Epoch: 6/25, Step 2560, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.777\n",
      "Epoch: 6/25, Step 2600, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.557\n",
      "Epoch: 6/25, Step 2640, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.588\n",
      "Epoch: 6/25, Step 2680, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.710\n",
      "Epoch: 6/25, Step 2720, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.756\n",
      "Epoch: 6/25, Step 2760, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.782\n",
      "Epoch: 6/25, Step 2800, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.781\n",
      "Epoch: 6/25, Step 2840, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.717\n",
      "Epoch: 6/25, Step 2880, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.664\n",
      "Epoch 6/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.106 Test reconstruction loss: 0.018 Test covariance loss: 8.832\n",
      "Epoch: 7/25, Step 2920, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.640\n",
      "Epoch: 7/25, Step 2960, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.587\n",
      "Epoch: 7/25, Step 3000, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.793\n",
      "Epoch: 7/25, Step 3040, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.716\n",
      "Epoch: 7/25, Step 3080, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.690\n",
      "Epoch: 7/25, Step 3120, Train loss: 0.097 Train reconstruction loss: 0.019 Train covariance loss: 7.822\n",
      "Epoch: 7/25, Step 3160, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.737\n",
      "Epoch: 7/25, Step 3200, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.614\n",
      "Epoch: 7/25, Step 3240, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.893\n",
      "Epoch: 7/25, Step 3280, Train loss: 0.098 Train reconstruction loss: 0.018 Train covariance loss: 7.945\n",
      "Epoch: 7/25, Step 3320, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.772\n",
      "Epoch: 7/25, Step 3360, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.577\n",
      "Epoch 7/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.110 Test reconstruction loss: 0.018 Test covariance loss: 9.265\n",
      "Epoch: 8/25, Step 3400, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.707\n",
      "Epoch: 8/25, Step 3440, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.636\n",
      "Epoch: 8/25, Step 3480, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.712\n",
      "Epoch: 8/25, Step 3520, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.684\n",
      "Epoch: 8/25, Step 3560, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.494\n",
      "Epoch: 8/25, Step 3600, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.566\n",
      "Epoch: 8/25, Step 3640, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.824\n",
      "Epoch: 8/25, Step 3680, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.754\n",
      "Epoch: 8/25, Step 3720, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.735\n",
      "Epoch: 8/25, Step 3760, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.681\n",
      "Epoch: 8/25, Step 3800, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.867\n",
      "Epoch: 8/25, Step 3840, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.654\n",
      "Epoch 8/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.094 Test reconstruction loss: 0.018 Test covariance loss: 7.660\n",
      "Epoch: 9/25, Step 3880, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.604\n",
      "Epoch: 9/25, Step 3920, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.671\n",
      "Epoch: 9/25, Step 3960, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.621\n",
      "Epoch: 9/25, Step 4000, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.684\n",
      "Epoch: 9/25, Step 4040, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.506\n",
      "Epoch: 9/25, Step 4080, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.747\n",
      "Epoch: 9/25, Step 4120, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.701\n",
      "Epoch: 9/25, Step 4160, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.853\n",
      "Epoch: 9/25, Step 4200, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.570\n",
      "Epoch: 9/25, Step 4240, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.785\n",
      "Epoch: 9/25, Step 4280, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.765\n",
      "Epoch: 9/25, Step 4320, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.681\n",
      "Epoch 9/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.093 Test reconstruction loss: 0.018 Test covariance loss: 7.554\n",
      "Epoch: 10/25, Step 4360, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.617\n",
      "Epoch: 10/25, Step 4400, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.855\n",
      "Epoch: 10/25, Step 4440, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.710\n",
      "Epoch: 10/25, Step 4480, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.711\n",
      "Epoch: 10/25, Step 4520, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.655\n",
      "Epoch: 10/25, Step 4560, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.822\n",
      "Epoch: 10/25, Step 4600, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.709\n",
      "Epoch: 10/25, Step 4640, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.657\n",
      "Epoch: 10/25, Step 4680, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.880\n",
      "Epoch: 10/25, Step 4720, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.719\n",
      "Epoch: 10/25, Step 4760, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.716\n",
      "Epoch: 10/25, Step 4800, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.706\n",
      "Epoch 10/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.101 Test reconstruction loss: 0.018 Test covariance loss: 8.316\n",
      "Epoch: 11/25, Step 4840, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.517\n",
      "Epoch: 11/25, Step 4880, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.554\n",
      "Epoch: 11/25, Step 4920, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.611\n",
      "Epoch: 11/25, Step 4960, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.747\n",
      "Epoch: 11/25, Step 5000, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.747\n",
      "Epoch: 11/25, Step 5040, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.815\n",
      "Epoch: 11/25, Step 5080, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.690\n",
      "Epoch: 11/25, Step 5120, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.868\n",
      "Epoch: 11/25, Step 5160, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.786\n",
      "Epoch: 11/25, Step 5200, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.896\n",
      "Epoch: 11/25, Step 5240, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.670\n",
      "Epoch: 11/25, Step 5280, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.832\n",
      "Epoch 11/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.095 Test reconstruction loss: 0.018 Test covariance loss: 7.719\n",
      "Epoch: 12/25, Step 5320, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.737\n",
      "Epoch: 12/25, Step 5360, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.598\n",
      "Epoch: 12/25, Step 5400, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.651\n",
      "Epoch: 12/25, Step 5440, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.644\n",
      "Epoch: 12/25, Step 5480, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.647\n",
      "Epoch: 12/25, Step 5520, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.644\n",
      "Epoch: 12/25, Step 5560, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.583\n",
      "Epoch: 12/25, Step 5600, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.510\n",
      "Epoch: 12/25, Step 5640, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.897\n",
      "Epoch: 12/25, Step 5680, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.823\n",
      "Epoch: 12/25, Step 5720, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.586\n",
      "Epoch: 12/25, Step 5760, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.694\n",
      "Test loss decreased (0.093017 --> 0.087730).  Saving model ...\n",
      "Epoch 12/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.088 Test reconstruction loss: 0.018 Test covariance loss: 7.013\n",
      "Epoch: 13/25, Step 5800, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.474\n",
      "Epoch: 13/25, Step 5840, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.677\n",
      "Epoch: 13/25, Step 5880, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.650\n",
      "Epoch: 13/25, Step 5920, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.704\n",
      "Epoch: 13/25, Step 5960, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.911\n",
      "Epoch: 13/25, Step 6000, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.587\n",
      "Epoch: 13/25, Step 6040, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.685\n",
      "Epoch: 13/25, Step 6080, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.575\n",
      "Epoch: 13/25, Step 6120, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.701\n",
      "Epoch: 13/25, Step 6160, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.811\n",
      "Epoch: 13/25, Step 6200, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.685\n",
      "Epoch: 13/25, Step 6240, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.747\n",
      "Epoch 13/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.101 Test reconstruction loss: 0.018 Test covariance loss: 8.329\n",
      "Epoch: 14/25, Step 6280, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.570\n",
      "Epoch: 14/25, Step 6320, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.691\n",
      "Epoch: 14/25, Step 6360, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.621\n",
      "Epoch: 14/25, Step 6400, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.875\n",
      "Epoch: 14/25, Step 6440, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.620\n",
      "Epoch: 14/25, Step 6480, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.566\n",
      "Epoch: 14/25, Step 6520, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.592\n",
      "Epoch: 14/25, Step 6560, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.489\n",
      "Epoch: 14/25, Step 6600, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.750\n",
      "Epoch: 14/25, Step 6640, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.686\n",
      "Epoch: 14/25, Step 6680, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.688\n",
      "Epoch: 14/25, Step 6720, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.926\n",
      "Epoch 14/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.097 Test reconstruction loss: 0.018 Test covariance loss: 7.940\n",
      "Epoch: 15/25, Step 6760, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.917\n",
      "Epoch: 15/25, Step 6800, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.776\n",
      "Epoch: 15/25, Step 6840, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.680\n",
      "Epoch: 15/25, Step 6880, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.800\n",
      "Epoch: 15/25, Step 6920, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.575\n",
      "Epoch: 15/25, Step 6960, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.898\n",
      "Epoch: 15/25, Step 7000, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.656\n",
      "Epoch: 15/25, Step 7040, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.818\n",
      "Epoch: 15/25, Step 7080, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.586\n",
      "Epoch: 15/25, Step 7120, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.703\n",
      "Epoch: 15/25, Step 7160, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.596\n",
      "Epoch: 15/25, Step 7200, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.711\n",
      "Epoch 15/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.107 Test reconstruction loss: 0.017 Test covariance loss: 8.913\n",
      "Epoch: 16/25, Step 7240, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.780\n",
      "Epoch: 16/25, Step 7280, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.613\n",
      "Epoch: 16/25, Step 7320, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.778\n",
      "Epoch: 16/25, Step 7360, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.689\n",
      "Epoch: 16/25, Step 7400, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.707\n",
      "Epoch: 16/25, Step 7440, Train loss: 0.099 Train reconstruction loss: 0.018 Train covariance loss: 8.091\n",
      "Epoch: 16/25, Step 7480, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.634\n",
      "Epoch: 16/25, Step 7520, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.583\n",
      "Epoch: 16/25, Step 7560, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.795\n",
      "Epoch: 16/25, Step 7600, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.485\n",
      "Epoch: 16/25, Step 7640, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.528\n",
      "Epoch: 16/25, Step 7680, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.813\n",
      "Epoch 16/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.098 Test reconstruction loss: 0.018 Test covariance loss: 8.051\n",
      "Epoch: 17/25, Step 7720, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.515\n",
      "Epoch: 17/25, Step 7760, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.615\n",
      "Epoch: 17/25, Step 7800, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.678\n",
      "Epoch: 17/25, Step 7840, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.863\n",
      "Epoch: 17/25, Step 7880, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.576\n",
      "Epoch: 17/25, Step 7920, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.838\n",
      "Epoch: 17/25, Step 7960, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.652\n",
      "Epoch: 17/25, Step 8000, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.870\n",
      "Epoch: 17/25, Step 8040, Train loss: 0.098 Train reconstruction loss: 0.018 Train covariance loss: 7.988\n",
      "Epoch: 17/25, Step 8080, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.797\n",
      "Epoch: 17/25, Step 8120, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.654\n",
      "Epoch: 17/25, Step 8160, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.736\n",
      "Epoch 17/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.090 Test reconstruction loss: 0.017 Test covariance loss: 7.296\n",
      "Epoch: 18/25, Step 8200, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.637\n",
      "Epoch: 18/25, Step 8240, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.671\n",
      "Epoch: 18/25, Step 8280, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.773\n",
      "Epoch: 18/25, Step 8320, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.747\n",
      "Epoch: 18/25, Step 8360, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.632\n",
      "Epoch: 18/25, Step 8400, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.791\n",
      "Epoch: 18/25, Step 8440, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.698\n",
      "Epoch: 18/25, Step 8480, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.906\n",
      "Epoch: 18/25, Step 8520, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.608\n",
      "Epoch: 18/25, Step 8560, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.462\n",
      "Epoch: 18/25, Step 8600, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.679\n",
      "Epoch: 18/25, Step 8640, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.638\n",
      "Epoch 18/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.112 Test reconstruction loss: 0.018 Test covariance loss: 9.451\n",
      "Epoch: 19/25, Step 8680, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.666\n",
      "Epoch: 19/25, Step 8720, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.769\n",
      "Epoch: 19/25, Step 8760, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.736\n",
      "Epoch: 19/25, Step 8800, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.883\n",
      "Epoch: 19/25, Step 8840, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.871\n",
      "Epoch: 19/25, Step 8880, Train loss: 0.092 Train reconstruction loss: 0.018 Train covariance loss: 7.457\n",
      "Epoch: 19/25, Step 8920, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.702\n",
      "Epoch: 19/25, Step 8960, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.642\n",
      "Epoch: 19/25, Step 9000, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.601\n",
      "Epoch: 19/25, Step 9040, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.811\n",
      "Epoch: 19/25, Step 9080, Train loss: 0.098 Train reconstruction loss: 0.018 Train covariance loss: 7.968\n",
      "Epoch: 19/25, Step 9120, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.825\n",
      "Epoch 19/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.102 Test reconstruction loss: 0.017 Test covariance loss: 8.408\n",
      "Epoch: 20/25, Step 9160, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.757\n",
      "Epoch: 20/25, Step 9200, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.605\n",
      "Epoch: 20/25, Step 9240, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.614\n",
      "Epoch: 20/25, Step 9280, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.703\n",
      "Epoch: 20/25, Step 9320, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.630\n",
      "Epoch: 20/25, Step 9360, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.759\n",
      "Epoch: 20/25, Step 9400, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.683\n",
      "Epoch: 20/25, Step 9440, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.847\n",
      "Epoch: 20/25, Step 9480, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.599\n",
      "Epoch: 20/25, Step 9520, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.703\n",
      "Epoch: 20/25, Step 9560, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.798\n",
      "Epoch: 20/25, Step 9600, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.704\n",
      "Epoch 20/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.100 Test reconstruction loss: 0.017 Test covariance loss: 8.233\n",
      "Epoch: 21/25, Step 9640, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.547\n",
      "Epoch: 21/25, Step 9680, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.733\n",
      "Epoch: 21/25, Step 9720, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.688\n",
      "Epoch: 21/25, Step 9760, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.838\n",
      "Epoch: 21/25, Step 9800, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.481\n",
      "Epoch: 21/25, Step 9840, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.725\n",
      "Epoch: 21/25, Step 9880, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.608\n",
      "Epoch: 21/25, Step 9920, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.617\n",
      "Epoch: 21/25, Step 9960, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.784\n",
      "Epoch: 21/25, Step 10000, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.664\n",
      "Epoch: 21/25, Step 10040, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.649\n",
      "Epoch: 21/25, Step 10080, Train loss: 0.098 Train reconstruction loss: 0.018 Train covariance loss: 8.013\n",
      "Epoch 21/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.110 Test reconstruction loss: 0.018 Test covariance loss: 9.273\n",
      "Epoch: 22/25, Step 10120, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.674\n",
      "Epoch: 22/25, Step 10160, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.720\n",
      "Epoch: 22/25, Step 10200, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.653\n",
      "Epoch: 22/25, Step 10240, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.707\n",
      "Epoch: 22/25, Step 10280, Train loss: 0.097 Train reconstruction loss: 0.019 Train covariance loss: 7.880\n",
      "Epoch: 22/25, Step 10320, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.729\n",
      "Epoch: 22/25, Step 10360, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.614\n",
      "Epoch: 22/25, Step 10400, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.591\n",
      "Epoch: 22/25, Step 10440, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.595\n",
      "Epoch: 22/25, Step 10480, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.735\n",
      "Epoch: 22/25, Step 10520, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.737\n",
      "Epoch: 22/25, Step 10560, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.597\n",
      "Epoch 22/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.096 Test reconstruction loss: 0.017 Test covariance loss: 7.833\n",
      "Epoch: 23/25, Step 10600, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.564\n",
      "Epoch: 23/25, Step 10640, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.644\n",
      "Epoch: 23/25, Step 10680, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.637\n",
      "Epoch: 23/25, Step 10720, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.717\n",
      "Epoch: 23/25, Step 10760, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.764\n",
      "Epoch: 23/25, Step 10800, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.640\n",
      "Epoch: 23/25, Step 10840, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.825\n",
      "Epoch: 23/25, Step 10880, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.693\n",
      "Epoch: 23/25, Step 10920, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.605\n",
      "Epoch: 23/25, Step 10960, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.657\n",
      "Epoch: 23/25, Step 11000, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.665\n",
      "Epoch: 23/25, Step 11040, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.552\n",
      "Epoch 23/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.093 Test reconstruction loss: 0.017 Test covariance loss: 7.577\n",
      "Epoch: 24/25, Step 11080, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.756\n",
      "Epoch: 24/25, Step 11120, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.618\n",
      "Epoch: 24/25, Step 11160, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.711\n",
      "Epoch: 24/25, Step 11200, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.844\n",
      "Epoch: 24/25, Step 11240, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.729\n",
      "Epoch: 24/25, Step 11280, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.814\n",
      "Epoch: 24/25, Step 11320, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.478\n",
      "Epoch: 24/25, Step 11360, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.623\n",
      "Epoch: 24/25, Step 11400, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.587\n",
      "Epoch: 24/25, Step 11440, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.755\n",
      "Epoch: 24/25, Step 11480, Train loss: 0.093 Train reconstruction loss: 0.018 Train covariance loss: 7.526\n",
      "Epoch: 24/25, Step 11520, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.769\n",
      "Epoch 24/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.105 Test reconstruction loss: 0.017 Test covariance loss: 8.726\n",
      "Epoch: 25/25, Step 11560, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.654\n",
      "Epoch: 25/25, Step 11600, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.710\n",
      "Epoch: 25/25, Step 11640, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.915\n",
      "Epoch: 25/25, Step 11680, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.674\n",
      "Epoch: 25/25, Step 11720, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.612\n",
      "Epoch: 25/25, Step 11760, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.605\n",
      "Epoch: 25/25, Step 11800, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.682\n",
      "Epoch: 25/25, Step 11840, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.587\n",
      "Epoch: 25/25, Step 11880, Train loss: 0.094 Train reconstruction loss: 0.018 Train covariance loss: 7.655\n",
      "Epoch: 25/25, Step 11920, Train loss: 0.096 Train reconstruction loss: 0.018 Train covariance loss: 7.786\n",
      "Epoch: 25/25, Step 11960, Train loss: 0.095 Train reconstruction loss: 0.018 Train covariance loss: 7.694\n",
      "Epoch: 25/25, Step 12000, Train loss: 0.097 Train reconstruction loss: 0.018 Train covariance loss: 7.908\n",
      "Epoch 25/25,\n",
      " Train Loss: 0.095 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.100 Test reconstruction loss: 0.017 Test covariance loss: 8.270\n",
      "Training with hidden dim: 2\n",
      "Epoch: 1/25, Step 40, Train loss: 0.172 Train reconstruction loss: 0.039 Train covariance loss: 13.292\n",
      "Epoch: 1/25, Step 80, Train loss: 0.154 Train reconstruction loss: 0.022 Train covariance loss: 13.215\n",
      "Epoch: 1/25, Step 120, Train loss: 0.152 Train reconstruction loss: 0.021 Train covariance loss: 13.154\n",
      "Epoch: 1/25, Step 160, Train loss: 0.154 Train reconstruction loss: 0.021 Train covariance loss: 13.302\n",
      "Epoch: 1/25, Step 200, Train loss: 0.152 Train reconstruction loss: 0.020 Train covariance loss: 13.180\n",
      "Epoch: 1/25, Step 240, Train loss: 0.151 Train reconstruction loss: 0.020 Train covariance loss: 13.104\n",
      "Epoch: 1/25, Step 280, Train loss: 0.150 Train reconstruction loss: 0.020 Train covariance loss: 12.978\n",
      "Epoch: 1/25, Step 320, Train loss: 0.152 Train reconstruction loss: 0.020 Train covariance loss: 13.169\n",
      "Epoch: 1/25, Step 360, Train loss: 0.150 Train reconstruction loss: 0.020 Train covariance loss: 13.022\n",
      "Epoch: 1/25, Step 400, Train loss: 0.154 Train reconstruction loss: 0.020 Train covariance loss: 13.426\n",
      "Epoch: 1/25, Step 440, Train loss: 0.153 Train reconstruction loss: 0.020 Train covariance loss: 13.305\n",
      "Epoch: 1/25, Step 480, Train loss: 0.151 Train reconstruction loss: 0.020 Train covariance loss: 13.167\n",
      "Test loss decreased (inf --> 0.154162).  Saving model ...\n",
      "Epoch 1/25,\n",
      " Train Loss: 0.154 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.154 Test reconstruction loss: 0.019 Test covariance loss: 13.517\n",
      "Epoch: 2/25, Step 520, Train loss: 0.151 Train reconstruction loss: 0.020 Train covariance loss: 13.154\n",
      "Epoch: 2/25, Step 560, Train loss: 0.151 Train reconstruction loss: 0.020 Train covariance loss: 13.193\n",
      "Epoch: 2/25, Step 600, Train loss: 0.152 Train reconstruction loss: 0.020 Train covariance loss: 13.232\n",
      "Epoch: 2/25, Step 640, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.155\n",
      "Epoch: 2/25, Step 680, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.087\n",
      "Epoch: 2/25, Step 720, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.265\n",
      "Epoch: 2/25, Step 760, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.073\n",
      "Epoch: 2/25, Step 800, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.358\n",
      "Epoch: 2/25, Step 840, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 12.987\n",
      "Epoch: 2/25, Step 880, Train loss: 0.152 Train reconstruction loss: 0.020 Train covariance loss: 13.246\n",
      "Epoch: 2/25, Step 920, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.363\n",
      "Epoch: 2/25, Step 960, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.149\n",
      "Test loss decreased (0.154162 --> 0.153384).  Saving model ...\n",
      "Epoch 2/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.153 Test reconstruction loss: 0.019 Test covariance loss: 13.476\n",
      "Epoch: 3/25, Step 1000, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.286\n",
      "Epoch: 3/25, Step 1040, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.346\n",
      "Epoch: 3/25, Step 1080, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.412\n",
      "Epoch: 3/25, Step 1120, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.348\n",
      "Epoch: 3/25, Step 1160, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.354\n",
      "Epoch: 3/25, Step 1200, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.068\n",
      "Epoch: 3/25, Step 1240, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.137\n",
      "Epoch: 3/25, Step 1280, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.085\n",
      "Epoch: 3/25, Step 1320, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.372\n",
      "Epoch: 3/25, Step 1360, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.056\n",
      "Epoch: 3/25, Step 1400, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.303\n",
      "Epoch: 3/25, Step 1440, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 13.058\n",
      "Epoch 3/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.159 Test reconstruction loss: 0.018 Test covariance loss: 14.103\n",
      "Epoch: 4/25, Step 1480, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.249\n",
      "Epoch: 4/25, Step 1520, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 13.008\n",
      "Epoch: 4/25, Step 1560, Train loss: 0.148 Train reconstruction loss: 0.019 Train covariance loss: 12.949\n",
      "Epoch: 4/25, Step 1600, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.345\n",
      "Epoch: 4/25, Step 1640, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.427\n",
      "Epoch: 4/25, Step 1680, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.071\n",
      "Epoch: 4/25, Step 1720, Train loss: 0.154 Train reconstruction loss: 0.019 Train covariance loss: 13.506\n",
      "Epoch: 4/25, Step 1760, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 12.955\n",
      "Epoch: 4/25, Step 1800, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.299\n",
      "Epoch: 4/25, Step 1840, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.398\n",
      "Epoch: 4/25, Step 1880, Train loss: 0.148 Train reconstruction loss: 0.019 Train covariance loss: 12.956\n",
      "Epoch: 4/25, Step 1920, Train loss: 0.148 Train reconstruction loss: 0.019 Train covariance loss: 12.949\n",
      "Epoch 4/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.162 Test reconstruction loss: 0.018 Test covariance loss: 14.415\n",
      "Epoch: 5/25, Step 1960, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.103\n",
      "Epoch: 5/25, Step 2000, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.078\n",
      "Epoch: 5/25, Step 2040, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 13.038\n",
      "Epoch: 5/25, Step 2080, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.150\n",
      "Epoch: 5/25, Step 2120, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.091\n",
      "Epoch: 5/25, Step 2160, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.413\n",
      "Epoch: 5/25, Step 2200, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.191\n",
      "Epoch: 5/25, Step 2240, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.144\n",
      "Epoch: 5/25, Step 2280, Train loss: 0.154 Train reconstruction loss: 0.019 Train covariance loss: 13.517\n",
      "Epoch: 5/25, Step 2320, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.372\n",
      "Epoch: 5/25, Step 2360, Train loss: 0.148 Train reconstruction loss: 0.019 Train covariance loss: 12.916\n",
      "Epoch: 5/25, Step 2400, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.461\n",
      "Epoch 5/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.156 Test reconstruction loss: 0.018 Test covariance loss: 13.786\n",
      "Epoch: 6/25, Step 2440, Train loss: 0.148 Train reconstruction loss: 0.019 Train covariance loss: 12.976\n",
      "Epoch: 6/25, Step 2480, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 13.011\n",
      "Epoch: 6/25, Step 2520, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.239\n",
      "Epoch: 6/25, Step 2560, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.259\n",
      "Epoch: 6/25, Step 2600, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.254\n",
      "Epoch: 6/25, Step 2640, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.276\n",
      "Epoch: 6/25, Step 2680, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.264\n",
      "Epoch: 6/25, Step 2720, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.171\n",
      "Epoch: 6/25, Step 2760, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.411\n",
      "Epoch: 6/25, Step 2800, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.305\n",
      "Epoch: 6/25, Step 2840, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.278\n",
      "Epoch: 6/25, Step 2880, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 12.982\n",
      "Epoch 6/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.157 Test reconstruction loss: 0.018 Test covariance loss: 13.933\n",
      "Epoch: 7/25, Step 2920, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.192\n",
      "Epoch: 7/25, Step 2960, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 13.044\n",
      "Epoch: 7/25, Step 3000, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.135\n",
      "Epoch: 7/25, Step 3040, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.311\n",
      "Epoch: 7/25, Step 3080, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.052\n",
      "Epoch: 7/25, Step 3120, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.297\n",
      "Epoch: 7/25, Step 3160, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.374\n",
      "Epoch: 7/25, Step 3200, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 13.010\n",
      "Epoch: 7/25, Step 3240, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.307\n",
      "Epoch: 7/25, Step 3280, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.309\n",
      "Epoch: 7/25, Step 3320, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.164\n",
      "Epoch: 7/25, Step 3360, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.110\n",
      "Epoch 7/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.174 Test reconstruction loss: 0.018 Test covariance loss: 15.602\n",
      "Epoch: 8/25, Step 3400, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.264\n",
      "Epoch: 8/25, Step 3440, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.234\n",
      "Epoch: 8/25, Step 3480, Train loss: 0.147 Train reconstruction loss: 0.018 Train covariance loss: 12.868\n",
      "Epoch: 8/25, Step 3520, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.128\n",
      "Epoch: 8/25, Step 3560, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 13.062\n",
      "Epoch: 8/25, Step 3600, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.276\n",
      "Epoch: 8/25, Step 3640, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 13.017\n",
      "Epoch: 8/25, Step 3680, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.387\n",
      "Epoch: 8/25, Step 3720, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.185\n",
      "Epoch: 8/25, Step 3760, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.384\n",
      "Epoch: 8/25, Step 3800, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.173\n",
      "Epoch: 8/25, Step 3840, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.384\n",
      "Test loss decreased (0.153384 --> 0.147012).  Saving model ...\n",
      "Epoch 8/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.147 Test reconstruction loss: 0.018 Test covariance loss: 12.902\n",
      "Epoch: 9/25, Step 3880, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.187\n",
      "Epoch: 9/25, Step 3920, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.403\n",
      "Epoch: 9/25, Step 3960, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.268\n",
      "Epoch: 9/25, Step 4000, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.125\n",
      "Epoch: 9/25, Step 4040, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 13.068\n",
      "Epoch: 9/25, Step 4080, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.058\n",
      "Epoch: 9/25, Step 4120, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.284\n",
      "Epoch: 9/25, Step 4160, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.398\n",
      "Epoch: 9/25, Step 4200, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.280\n",
      "Epoch: 9/25, Step 4240, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.365\n",
      "Epoch: 9/25, Step 4280, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.207\n",
      "Epoch: 9/25, Step 4320, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.169\n",
      "Epoch 9/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.158 Test reconstruction loss: 0.018 Test covariance loss: 13.989\n",
      "Epoch: 10/25, Step 4360, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.100\n",
      "Epoch: 10/25, Step 4400, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.404\n",
      "Epoch: 10/25, Step 4440, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.206\n",
      "Epoch: 10/25, Step 4480, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.179\n",
      "Epoch: 10/25, Step 4520, Train loss: 0.153 Train reconstruction loss: 0.019 Train covariance loss: 13.468\n",
      "Epoch: 10/25, Step 4560, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.023\n",
      "Epoch: 10/25, Step 4600, Train loss: 0.154 Train reconstruction loss: 0.019 Train covariance loss: 13.496\n",
      "Epoch: 10/25, Step 4640, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.247\n",
      "Epoch: 10/25, Step 4680, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.113\n",
      "Epoch: 10/25, Step 4720, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.220\n",
      "Epoch: 10/25, Step 4760, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.332\n",
      "Epoch: 10/25, Step 4800, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.398\n",
      "Epoch 10/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.153 Test reconstruction loss: 0.018 Test covariance loss: 13.477\n",
      "Epoch: 11/25, Step 4840, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.216\n",
      "Epoch: 11/25, Step 4880, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.028\n",
      "Epoch: 11/25, Step 4920, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.377\n",
      "Epoch: 11/25, Step 4960, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.181\n",
      "Epoch: 11/25, Step 5000, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.250\n",
      "Epoch: 11/25, Step 5040, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.118\n",
      "Epoch: 11/25, Step 5080, Train loss: 0.148 Train reconstruction loss: 0.019 Train covariance loss: 12.904\n",
      "Epoch: 11/25, Step 5120, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.132\n",
      "Epoch: 11/25, Step 5160, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 12.945\n",
      "Epoch: 11/25, Step 5200, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.327\n",
      "Epoch: 11/25, Step 5240, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.297\n",
      "Epoch: 11/25, Step 5280, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.225\n",
      "Epoch 11/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.174 Test reconstruction loss: 0.018 Test covariance loss: 15.612\n",
      "Epoch: 12/25, Step 5320, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.215\n",
      "Epoch: 12/25, Step 5360, Train loss: 0.147 Train reconstruction loss: 0.018 Train covariance loss: 12.907\n",
      "Epoch: 12/25, Step 5400, Train loss: 0.152 Train reconstruction loss: 0.019 Train covariance loss: 13.390\n",
      "Epoch: 12/25, Step 5440, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.036\n",
      "Epoch: 12/25, Step 5480, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.299\n",
      "Epoch: 12/25, Step 5520, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.152\n",
      "Epoch: 12/25, Step 5560, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.196\n",
      "Epoch: 12/25, Step 5600, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.015\n",
      "Epoch: 12/25, Step 5640, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.372\n",
      "Epoch: 12/25, Step 5680, Train loss: 0.154 Train reconstruction loss: 0.019 Train covariance loss: 13.525\n",
      "Epoch: 12/25, Step 5720, Train loss: 0.153 Train reconstruction loss: 0.018 Train covariance loss: 13.428\n",
      "Epoch: 12/25, Step 5760, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.144\n",
      "Epoch 12/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.156 Test reconstruction loss: 0.018 Test covariance loss: 13.842\n",
      "Epoch: 13/25, Step 5800, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.407\n",
      "Epoch: 13/25, Step 5840, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.342\n",
      "Epoch: 13/25, Step 5880, Train loss: 0.147 Train reconstruction loss: 0.018 Train covariance loss: 12.829\n",
      "Epoch: 13/25, Step 5920, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.400\n",
      "Epoch: 13/25, Step 5960, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.122\n",
      "Epoch: 13/25, Step 6000, Train loss: 0.147 Train reconstruction loss: 0.018 Train covariance loss: 12.891\n",
      "Epoch: 13/25, Step 6040, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.266\n",
      "Epoch: 13/25, Step 6080, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.288\n",
      "Epoch: 13/25, Step 6120, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.275\n",
      "Epoch: 13/25, Step 6160, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.222\n",
      "Epoch: 13/25, Step 6200, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.231\n",
      "Epoch: 13/25, Step 6240, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.389\n",
      "Epoch 13/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.162 Test reconstruction loss: 0.018 Test covariance loss: 14.430\n",
      "Epoch: 14/25, Step 6280, Train loss: 0.149 Train reconstruction loss: 0.019 Train covariance loss: 13.028\n",
      "Epoch: 14/25, Step 6320, Train loss: 0.154 Train reconstruction loss: 0.018 Train covariance loss: 13.613\n",
      "Epoch: 14/25, Step 6360, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.255\n",
      "Epoch: 14/25, Step 6400, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.128\n",
      "Epoch: 14/25, Step 6440, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.131\n",
      "Epoch: 14/25, Step 6480, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.267\n",
      "Epoch: 14/25, Step 6520, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.201\n",
      "Epoch: 14/25, Step 6560, Train loss: 0.147 Train reconstruction loss: 0.018 Train covariance loss: 12.917\n",
      "Epoch: 14/25, Step 6600, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 13.017\n",
      "Epoch: 14/25, Step 6640, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.034\n",
      "Epoch: 14/25, Step 6680, Train loss: 0.146 Train reconstruction loss: 0.018 Train covariance loss: 12.794\n",
      "Epoch: 14/25, Step 6720, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.384\n",
      "Epoch 14/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.149 Test reconstruction loss: 0.018 Test covariance loss: 13.095\n",
      "Epoch: 15/25, Step 6760, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.323\n",
      "Epoch: 15/25, Step 6800, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.415\n",
      "Epoch: 15/25, Step 6840, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.166\n",
      "Epoch: 15/25, Step 6880, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.089\n",
      "Epoch: 15/25, Step 6920, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.164\n",
      "Epoch: 15/25, Step 6960, Train loss: 0.154 Train reconstruction loss: 0.018 Train covariance loss: 13.535\n",
      "Epoch: 15/25, Step 7000, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.202\n",
      "Epoch: 15/25, Step 7040, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 12.983\n",
      "Epoch: 15/25, Step 7080, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.157\n",
      "Epoch: 15/25, Step 7120, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.119\n",
      "Epoch: 15/25, Step 7160, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.092\n",
      "Epoch: 15/25, Step 7200, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.217\n",
      "Test loss decreased (0.147012 --> 0.143154).  Saving model ...\n",
      "Epoch 15/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.143 Test reconstruction loss: 0.018 Test covariance loss: 12.538\n",
      "Epoch: 16/25, Step 7240, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.117\n",
      "Epoch: 16/25, Step 7280, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.255\n",
      "Epoch: 16/25, Step 7320, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.206\n",
      "Epoch: 16/25, Step 7360, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 12.950\n",
      "Epoch: 16/25, Step 7400, Train loss: 0.153 Train reconstruction loss: 0.018 Train covariance loss: 13.489\n",
      "Epoch: 16/25, Step 7440, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 12.935\n",
      "Epoch: 16/25, Step 7480, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.142\n",
      "Epoch: 16/25, Step 7520, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.367\n",
      "Epoch: 16/25, Step 7560, Train loss: 0.147 Train reconstruction loss: 0.018 Train covariance loss: 12.916\n",
      "Epoch: 16/25, Step 7600, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.339\n",
      "Epoch: 16/25, Step 7640, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.261\n",
      "Epoch: 16/25, Step 7680, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.186\n",
      "Epoch 16/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.175 Test reconstruction loss: 0.018 Test covariance loss: 15.740\n",
      "Epoch: 17/25, Step 7720, Train loss: 0.153 Train reconstruction loss: 0.018 Train covariance loss: 13.470\n",
      "Epoch: 17/25, Step 7760, Train loss: 0.153 Train reconstruction loss: 0.018 Train covariance loss: 13.436\n",
      "Epoch: 17/25, Step 7800, Train loss: 0.153 Train reconstruction loss: 0.018 Train covariance loss: 13.510\n",
      "Epoch: 17/25, Step 7840, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.374\n",
      "Epoch: 17/25, Step 7880, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.333\n",
      "Epoch: 17/25, Step 7920, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.131\n",
      "Epoch: 17/25, Step 7960, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.314\n",
      "Epoch: 17/25, Step 8000, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.251\n",
      "Epoch: 17/25, Step 8040, Train loss: 0.147 Train reconstruction loss: 0.018 Train covariance loss: 12.858\n",
      "Epoch: 17/25, Step 8080, Train loss: 0.150 Train reconstruction loss: 0.017 Train covariance loss: 13.231\n",
      "Epoch: 17/25, Step 8120, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 12.962\n",
      "Epoch: 17/25, Step 8160, Train loss: 0.154 Train reconstruction loss: 0.018 Train covariance loss: 13.526\n",
      "Epoch 17/25,\n",
      " Train Loss: 0.151 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.146 Test reconstruction loss: 0.018 Test covariance loss: 12.859\n",
      "Epoch: 18/25, Step 8200, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 12.980\n",
      "Epoch: 18/25, Step 8240, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.261\n",
      "Epoch: 18/25, Step 8280, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.390\n",
      "Epoch: 18/25, Step 8320, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.175\n",
      "Epoch: 18/25, Step 8360, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.276\n",
      "Epoch: 18/25, Step 8400, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.039\n",
      "Epoch: 18/25, Step 8440, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.288\n",
      "Epoch: 18/25, Step 8480, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.430\n",
      "Epoch: 18/25, Step 8520, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.208\n",
      "Epoch: 18/25, Step 8560, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.253\n",
      "Epoch: 18/25, Step 8600, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.354\n",
      "Epoch: 18/25, Step 8640, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.170\n",
      "Test loss decreased (0.143154 --> 0.141075).  Saving model ...\n",
      "Epoch 18/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.141 Test reconstruction loss: 0.018 Test covariance loss: 12.340\n",
      "Epoch: 19/25, Step 8680, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 12.984\n",
      "Epoch: 19/25, Step 8720, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.209\n",
      "Epoch: 19/25, Step 8760, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.117\n",
      "Epoch: 19/25, Step 8800, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.143\n",
      "Epoch: 19/25, Step 8840, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.274\n",
      "Epoch: 19/25, Step 8880, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.105\n",
      "Epoch: 19/25, Step 8920, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.333\n",
      "Epoch: 19/25, Step 8960, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.212\n",
      "Epoch: 19/25, Step 9000, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.244\n",
      "Epoch: 19/25, Step 9040, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.162\n",
      "Epoch: 19/25, Step 9080, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.356\n",
      "Epoch: 19/25, Step 9120, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.060\n",
      "Epoch 19/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.157 Test reconstruction loss: 0.018 Test covariance loss: 13.890\n",
      "Epoch: 20/25, Step 9160, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.072\n",
      "Epoch: 20/25, Step 9200, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.224\n",
      "Epoch: 20/25, Step 9240, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.153\n",
      "Epoch: 20/25, Step 9280, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.326\n",
      "Epoch: 20/25, Step 9320, Train loss: 0.151 Train reconstruction loss: 0.019 Train covariance loss: 13.296\n",
      "Epoch: 20/25, Step 9360, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.156\n",
      "Epoch: 20/25, Step 9400, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.177\n",
      "Epoch: 20/25, Step 9440, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.157\n",
      "Epoch: 20/25, Step 9480, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.107\n",
      "Epoch: 20/25, Step 9520, Train loss: 0.150 Train reconstruction loss: 0.019 Train covariance loss: 13.177\n",
      "Epoch: 20/25, Step 9560, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.429\n",
      "Epoch: 20/25, Step 9600, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.198\n",
      "Epoch 20/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.156 Test reconstruction loss: 0.018 Test covariance loss: 13.819\n",
      "Epoch: 21/25, Step 9640, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.219\n",
      "Epoch: 21/25, Step 9680, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.353\n",
      "Epoch: 21/25, Step 9720, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.115\n",
      "Epoch: 21/25, Step 9760, Train loss: 0.153 Train reconstruction loss: 0.018 Train covariance loss: 13.499\n",
      "Epoch: 21/25, Step 9800, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.178\n",
      "Epoch: 21/25, Step 9840, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.050\n",
      "Epoch: 21/25, Step 9880, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.165\n",
      "Epoch: 21/25, Step 9920, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.213\n",
      "Epoch: 21/25, Step 9960, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.260\n",
      "Epoch: 21/25, Step 10000, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.108\n",
      "Epoch: 21/25, Step 10040, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 13.013\n",
      "Epoch: 21/25, Step 10080, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.121\n",
      "Epoch 21/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.152 Test reconstruction loss: 0.018 Test covariance loss: 13.487\n",
      "Epoch: 22/25, Step 10120, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.250\n",
      "Epoch: 22/25, Step 10160, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.252\n",
      "Epoch: 22/25, Step 10200, Train loss: 0.153 Train reconstruction loss: 0.018 Train covariance loss: 13.526\n",
      "Epoch: 22/25, Step 10240, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.197\n",
      "Epoch: 22/25, Step 10280, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.411\n",
      "Epoch: 22/25, Step 10320, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.200\n",
      "Epoch: 22/25, Step 10360, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 13.003\n",
      "Epoch: 22/25, Step 10400, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.321\n",
      "Epoch: 22/25, Step 10440, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.056\n",
      "Epoch: 22/25, Step 10480, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.165\n",
      "Epoch: 22/25, Step 10520, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.187\n",
      "Epoch: 22/25, Step 10560, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.354\n",
      "Epoch 22/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.142 Test reconstruction loss: 0.018 Test covariance loss: 12.397\n",
      "Epoch: 23/25, Step 10600, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 13.002\n",
      "Epoch: 23/25, Step 10640, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.110\n",
      "Epoch: 23/25, Step 10680, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.219\n",
      "Epoch: 23/25, Step 10720, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.362\n",
      "Epoch: 23/25, Step 10760, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 12.949\n",
      "Epoch: 23/25, Step 10800, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.199\n",
      "Epoch: 23/25, Step 10840, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.116\n",
      "Epoch: 23/25, Step 10880, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 13.048\n",
      "Epoch: 23/25, Step 10920, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.224\n",
      "Epoch: 23/25, Step 10960, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.074\n",
      "Epoch: 23/25, Step 11000, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.255\n",
      "Epoch: 23/25, Step 11040, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.224\n",
      "Epoch 23/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.146 Test reconstruction loss: 0.018 Test covariance loss: 12.849\n",
      "Epoch: 24/25, Step 11080, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 12.968\n",
      "Epoch: 24/25, Step 11120, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.189\n",
      "Epoch: 24/25, Step 11160, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 12.952\n",
      "Epoch: 24/25, Step 11200, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.272\n",
      "Epoch: 24/25, Step 11240, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.449\n",
      "Epoch: 24/25, Step 11280, Train loss: 0.147 Train reconstruction loss: 0.018 Train covariance loss: 12.933\n",
      "Epoch: 24/25, Step 11320, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.113\n",
      "Epoch: 24/25, Step 11360, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.338\n",
      "Epoch: 24/25, Step 11400, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.379\n",
      "Epoch: 24/25, Step 11440, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.240\n",
      "Epoch: 24/25, Step 11480, Train loss: 0.148 Train reconstruction loss: 0.018 Train covariance loss: 13.015\n",
      "Epoch: 24/25, Step 11520, Train loss: 0.153 Train reconstruction loss: 0.018 Train covariance loss: 13.500\n",
      "Epoch 24/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.175 Test reconstruction loss: 0.018 Test covariance loss: 15.691\n",
      "Epoch: 25/25, Step 11560, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.244\n",
      "Epoch: 25/25, Step 11600, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.258\n",
      "Epoch: 25/25, Step 11640, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.119\n",
      "Epoch: 25/25, Step 11680, Train loss: 0.153 Train reconstruction loss: 0.018 Train covariance loss: 13.543\n",
      "Epoch: 25/25, Step 11720, Train loss: 0.151 Train reconstruction loss: 0.018 Train covariance loss: 13.350\n",
      "Epoch: 25/25, Step 11760, Train loss: 0.149 Train reconstruction loss: 0.018 Train covariance loss: 13.114\n",
      "Epoch: 25/25, Step 11800, Train loss: 0.147 Train reconstruction loss: 0.018 Train covariance loss: 12.953\n",
      "Epoch: 25/25, Step 11840, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.352\n",
      "Epoch: 25/25, Step 11880, Train loss: 0.152 Train reconstruction loss: 0.018 Train covariance loss: 13.427\n",
      "Epoch: 25/25, Step 11920, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.144\n",
      "Epoch: 25/25, Step 11960, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.243\n",
      "Epoch: 25/25, Step 12000, Train loss: 0.150 Train reconstruction loss: 0.018 Train covariance loss: 13.242\n",
      "Epoch 25/25,\n",
      " Train Loss: 0.150 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.154 Test reconstruction loss: 0.018 Test covariance loss: 13.634\n",
      "Training with hidden dim: 3\n",
      "Epoch: 1/25, Step 40, Train loss: 0.188 Train reconstruction loss: 0.040 Train covariance loss: 14.769\n",
      "Epoch: 1/25, Step 80, Train loss: 0.171 Train reconstruction loss: 0.022 Train covariance loss: 14.977\n",
      "Epoch: 1/25, Step 120, Train loss: 0.168 Train reconstruction loss: 0.021 Train covariance loss: 14.698\n",
      "Epoch: 1/25, Step 160, Train loss: 0.168 Train reconstruction loss: 0.020 Train covariance loss: 14.825\n",
      "Epoch: 1/25, Step 200, Train loss: 0.170 Train reconstruction loss: 0.020 Train covariance loss: 15.012\n",
      "Epoch: 1/25, Step 240, Train loss: 0.171 Train reconstruction loss: 0.020 Train covariance loss: 15.103\n",
      "Epoch: 1/25, Step 280, Train loss: 0.165 Train reconstruction loss: 0.019 Train covariance loss: 14.575\n",
      "Epoch: 1/25, Step 320, Train loss: 0.168 Train reconstruction loss: 0.020 Train covariance loss: 14.815\n",
      "Epoch: 1/25, Step 360, Train loss: 0.166 Train reconstruction loss: 0.019 Train covariance loss: 14.694\n",
      "Epoch: 1/25, Step 400, Train loss: 0.170 Train reconstruction loss: 0.019 Train covariance loss: 15.042\n",
      "Epoch: 1/25, Step 440, Train loss: 0.164 Train reconstruction loss: 0.019 Train covariance loss: 14.509\n",
      "Epoch: 1/25, Step 480, Train loss: 0.166 Train reconstruction loss: 0.019 Train covariance loss: 14.688\n",
      "Test loss decreased (inf --> 0.145038).  Saving model ...\n",
      "Epoch 1/25,\n",
      " Train Loss: 0.170 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.145 Test reconstruction loss: 0.019 Test covariance loss: 12.645\n",
      "Epoch: 2/25, Step 520, Train loss: 0.172 Train reconstruction loss: 0.019 Train covariance loss: 15.240\n",
      "Epoch: 2/25, Step 560, Train loss: 0.166 Train reconstruction loss: 0.019 Train covariance loss: 14.721\n",
      "Epoch: 2/25, Step 600, Train loss: 0.168 Train reconstruction loss: 0.019 Train covariance loss: 14.879\n",
      "Epoch: 2/25, Step 640, Train loss: 0.167 Train reconstruction loss: 0.019 Train covariance loss: 14.772\n",
      "Epoch: 2/25, Step 680, Train loss: 0.165 Train reconstruction loss: 0.019 Train covariance loss: 14.589\n",
      "Epoch: 2/25, Step 720, Train loss: 0.171 Train reconstruction loss: 0.019 Train covariance loss: 15.200\n",
      "Epoch: 2/25, Step 760, Train loss: 0.169 Train reconstruction loss: 0.019 Train covariance loss: 14.969\n",
      "Epoch: 2/25, Step 800, Train loss: 0.166 Train reconstruction loss: 0.019 Train covariance loss: 14.692\n",
      "Epoch: 2/25, Step 840, Train loss: 0.166 Train reconstruction loss: 0.019 Train covariance loss: 14.699\n",
      "Epoch: 2/25, Step 880, Train loss: 0.168 Train reconstruction loss: 0.019 Train covariance loss: 14.929\n",
      "Epoch: 2/25, Step 920, Train loss: 0.173 Train reconstruction loss: 0.019 Train covariance loss: 15.384\n",
      "Epoch: 2/25, Step 960, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.057\n",
      "Epoch 2/25,\n",
      " Train Loss: 0.168 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.169 Test reconstruction loss: 0.018 Test covariance loss: 15.042\n",
      "Epoch: 3/25, Step 1000, Train loss: 0.167 Train reconstruction loss: 0.019 Train covariance loss: 14.875\n",
      "Epoch: 3/25, Step 1040, Train loss: 0.165 Train reconstruction loss: 0.019 Train covariance loss: 14.583\n",
      "Epoch: 3/25, Step 1080, Train loss: 0.172 Train reconstruction loss: 0.019 Train covariance loss: 15.361\n",
      "Epoch: 3/25, Step 1120, Train loss: 0.170 Train reconstruction loss: 0.019 Train covariance loss: 15.124\n",
      "Epoch: 3/25, Step 1160, Train loss: 0.165 Train reconstruction loss: 0.019 Train covariance loss: 14.676\n",
      "Epoch: 3/25, Step 1200, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.666\n",
      "Epoch: 3/25, Step 1240, Train loss: 0.173 Train reconstruction loss: 0.019 Train covariance loss: 15.386\n",
      "Epoch: 3/25, Step 1280, Train loss: 0.167 Train reconstruction loss: 0.019 Train covariance loss: 14.800\n",
      "Epoch: 3/25, Step 1320, Train loss: 0.172 Train reconstruction loss: 0.018 Train covariance loss: 15.348\n",
      "Epoch: 3/25, Step 1360, Train loss: 0.165 Train reconstruction loss: 0.019 Train covariance loss: 14.588\n",
      "Epoch: 3/25, Step 1400, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.554\n",
      "Epoch: 3/25, Step 1440, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.079\n",
      "Epoch 3/25,\n",
      " Train Loss: 0.168 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.176 Test reconstruction loss: 0.018 Test covariance loss: 15.806\n",
      "Epoch: 4/25, Step 1480, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.674\n",
      "Epoch: 4/25, Step 1520, Train loss: 0.170 Train reconstruction loss: 0.018 Train covariance loss: 15.162\n",
      "Epoch: 4/25, Step 1560, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.753\n",
      "Epoch: 4/25, Step 1600, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.634\n",
      "Epoch: 4/25, Step 1640, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.819\n",
      "Epoch: 4/25, Step 1680, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.492\n",
      "Epoch: 4/25, Step 1720, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.479\n",
      "Epoch: 4/25, Step 1760, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.582\n",
      "Epoch: 4/25, Step 1800, Train loss: 0.168 Train reconstruction loss: 0.019 Train covariance loss: 14.901\n",
      "Epoch: 4/25, Step 1840, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.825\n",
      "Epoch: 4/25, Step 1880, Train loss: 0.170 Train reconstruction loss: 0.019 Train covariance loss: 15.192\n",
      "Epoch: 4/25, Step 1920, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.077\n",
      "Epoch 4/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.168 Test reconstruction loss: 0.018 Test covariance loss: 15.001\n",
      "Epoch: 5/25, Step 1960, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.631\n",
      "Epoch: 5/25, Step 2000, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.845\n",
      "Epoch: 5/25, Step 2040, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.605\n",
      "Epoch: 5/25, Step 2080, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.941\n",
      "Epoch: 5/25, Step 2120, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.911\n",
      "Epoch: 5/25, Step 2160, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.649\n",
      "Epoch: 5/25, Step 2200, Train loss: 0.162 Train reconstruction loss: 0.018 Train covariance loss: 14.365\n",
      "Epoch: 5/25, Step 2240, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.678\n",
      "Epoch: 5/25, Step 2280, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.916\n",
      "Epoch: 5/25, Step 2320, Train loss: 0.171 Train reconstruction loss: 0.018 Train covariance loss: 15.308\n",
      "Epoch: 5/25, Step 2360, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.921\n",
      "Epoch: 5/25, Step 2400, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.692\n",
      "Epoch 5/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.168 Test reconstruction loss: 0.018 Test covariance loss: 15.009\n",
      "Epoch: 6/25, Step 2440, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.855\n",
      "Epoch: 6/25, Step 2480, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.035\n",
      "Epoch: 6/25, Step 2520, Train loss: 0.171 Train reconstruction loss: 0.018 Train covariance loss: 15.260\n",
      "Epoch: 6/25, Step 2560, Train loss: 0.163 Train reconstruction loss: 0.019 Train covariance loss: 14.423\n",
      "Epoch: 6/25, Step 2600, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.878\n",
      "Epoch: 6/25, Step 2640, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.794\n",
      "Epoch: 6/25, Step 2680, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.470\n",
      "Epoch: 6/25, Step 2720, Train loss: 0.172 Train reconstruction loss: 0.018 Train covariance loss: 15.365\n",
      "Epoch: 6/25, Step 2760, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.782\n",
      "Epoch: 6/25, Step 2800, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.730\n",
      "Epoch: 6/25, Step 2840, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.850\n",
      "Epoch: 6/25, Step 2880, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.639\n",
      "Epoch 6/25,\n",
      " Train Loss: 0.167 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.174 Test reconstruction loss: 0.018 Test covariance loss: 15.594\n",
      "Epoch: 7/25, Step 2920, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.544\n",
      "Epoch: 7/25, Step 2960, Train loss: 0.162 Train reconstruction loss: 0.018 Train covariance loss: 14.373\n",
      "Epoch: 7/25, Step 3000, Train loss: 0.166 Train reconstruction loss: 0.019 Train covariance loss: 14.704\n",
      "Epoch: 7/25, Step 3040, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.959\n",
      "Epoch: 7/25, Step 3080, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.940\n",
      "Epoch: 7/25, Step 3120, Train loss: 0.161 Train reconstruction loss: 0.018 Train covariance loss: 14.268\n",
      "Epoch: 7/25, Step 3160, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.736\n",
      "Epoch: 7/25, Step 3200, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.027\n",
      "Epoch: 7/25, Step 3240, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.925\n",
      "Epoch: 7/25, Step 3280, Train loss: 0.172 Train reconstruction loss: 0.018 Train covariance loss: 15.409\n",
      "Epoch: 7/25, Step 3320, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.445\n",
      "Epoch: 7/25, Step 3360, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.553\n",
      "Epoch 7/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.173 Test reconstruction loss: 0.018 Test covariance loss: 15.496\n",
      "Epoch: 8/25, Step 3400, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.828\n",
      "Epoch: 8/25, Step 3440, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.785\n",
      "Epoch: 8/25, Step 3480, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.128\n",
      "Epoch: 8/25, Step 3520, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.662\n",
      "Epoch: 8/25, Step 3560, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.756\n",
      "Epoch: 8/25, Step 3600, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.592\n",
      "Epoch: 8/25, Step 3640, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.947\n",
      "Epoch: 8/25, Step 3680, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.866\n",
      "Epoch: 8/25, Step 3720, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.074\n",
      "Epoch: 8/25, Step 3760, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.813\n",
      "Epoch: 8/25, Step 3800, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.883\n",
      "Epoch: 8/25, Step 3840, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.727\n",
      "Epoch 8/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.167 Test reconstruction loss: 0.018 Test covariance loss: 14.983\n",
      "Epoch: 9/25, Step 3880, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.468\n",
      "Epoch: 9/25, Step 3920, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.986\n",
      "Epoch: 9/25, Step 3960, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.979\n",
      "Epoch: 9/25, Step 4000, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.691\n",
      "Epoch: 9/25, Step 4040, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.604\n",
      "Epoch: 9/25, Step 4080, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.049\n",
      "Epoch: 9/25, Step 4120, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.650\n",
      "Epoch: 9/25, Step 4160, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.000\n",
      "Epoch: 9/25, Step 4200, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.828\n",
      "Epoch: 9/25, Step 4240, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.957\n",
      "Epoch: 9/25, Step 4280, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.583\n",
      "Epoch: 9/25, Step 4320, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.536\n",
      "Epoch 9/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.160 Test reconstruction loss: 0.018 Test covariance loss: 14.218\n",
      "Epoch: 10/25, Step 4360, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.620\n",
      "Epoch: 10/25, Step 4400, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.595\n",
      "Epoch: 10/25, Step 4440, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.848\n",
      "Epoch: 10/25, Step 4480, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.873\n",
      "Epoch: 10/25, Step 4520, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.468\n",
      "Epoch: 10/25, Step 4560, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.588\n",
      "Epoch: 10/25, Step 4600, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.840\n",
      "Epoch: 10/25, Step 4640, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.788\n",
      "Epoch: 10/25, Step 4680, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.104\n",
      "Epoch: 10/25, Step 4720, Train loss: 0.170 Train reconstruction loss: 0.018 Train covariance loss: 15.196\n",
      "Epoch: 10/25, Step 4760, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.963\n",
      "Epoch: 10/25, Step 4800, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.701\n",
      "Epoch 10/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.172 Test reconstruction loss: 0.018 Test covariance loss: 15.479\n",
      "Epoch: 11/25, Step 4840, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.511\n",
      "Epoch: 11/25, Step 4880, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.695\n",
      "Epoch: 11/25, Step 4920, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.903\n",
      "Epoch: 11/25, Step 4960, Train loss: 0.170 Train reconstruction loss: 0.018 Train covariance loss: 15.148\n",
      "Epoch: 11/25, Step 5000, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.796\n",
      "Epoch: 11/25, Step 5040, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.561\n",
      "Epoch: 11/25, Step 5080, Train loss: 0.171 Train reconstruction loss: 0.018 Train covariance loss: 15.354\n",
      "Epoch: 11/25, Step 5120, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.898\n",
      "Epoch: 11/25, Step 5160, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.094\n",
      "Epoch: 11/25, Step 5200, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.619\n",
      "Epoch: 11/25, Step 5240, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.588\n",
      "Epoch: 11/25, Step 5280, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.565\n",
      "Epoch 11/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.182 Test reconstruction loss: 0.018 Test covariance loss: 16.475\n",
      "Epoch: 12/25, Step 5320, Train loss: 0.162 Train reconstruction loss: 0.018 Train covariance loss: 14.451\n",
      "Epoch: 12/25, Step 5360, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.906\n",
      "Epoch: 12/25, Step 5400, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.961\n",
      "Epoch: 12/25, Step 5440, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.806\n",
      "Epoch: 12/25, Step 5480, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.029\n",
      "Epoch: 12/25, Step 5520, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.749\n",
      "Epoch: 12/25, Step 5560, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.990\n",
      "Epoch: 12/25, Step 5600, Train loss: 0.170 Train reconstruction loss: 0.018 Train covariance loss: 15.243\n",
      "Epoch: 12/25, Step 5640, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.640\n",
      "Epoch: 12/25, Step 5680, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.785\n",
      "Epoch: 12/25, Step 5720, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.819\n",
      "Epoch: 12/25, Step 5760, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.626\n",
      "Epoch 12/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.175 Test reconstruction loss: 0.017 Test covariance loss: 15.766\n",
      "Epoch: 13/25, Step 5800, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.534\n",
      "Epoch: 13/25, Step 5840, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.859\n",
      "Epoch: 13/25, Step 5880, Train loss: 0.162 Train reconstruction loss: 0.018 Train covariance loss: 14.429\n",
      "Epoch: 13/25, Step 5920, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.025\n",
      "Epoch: 13/25, Step 5960, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.520\n",
      "Epoch: 13/25, Step 6000, Train loss: 0.170 Train reconstruction loss: 0.018 Train covariance loss: 15.149\n",
      "Epoch: 13/25, Step 6040, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.951\n",
      "Epoch: 13/25, Step 6080, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.719\n",
      "Epoch: 13/25, Step 6120, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.805\n",
      "Epoch: 13/25, Step 6160, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.866\n",
      "Epoch: 13/25, Step 6200, Train loss: 0.170 Train reconstruction loss: 0.018 Train covariance loss: 15.220\n",
      "Epoch: 13/25, Step 6240, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.934\n",
      "Epoch 13/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.171 Test reconstruction loss: 0.017 Test covariance loss: 15.345\n",
      "Epoch: 14/25, Step 6280, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.997\n",
      "Epoch: 14/25, Step 6320, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.144\n",
      "Epoch: 14/25, Step 6360, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.758\n",
      "Epoch: 14/25, Step 6400, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.569\n",
      "Epoch: 14/25, Step 6440, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.704\n",
      "Epoch: 14/25, Step 6480, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.576\n",
      "Epoch: 14/25, Step 6520, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.652\n",
      "Epoch: 14/25, Step 6560, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.144\n",
      "Epoch: 14/25, Step 6600, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.928\n",
      "Epoch: 14/25, Step 6640, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.568\n",
      "Epoch: 14/25, Step 6680, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.808\n",
      "Epoch: 14/25, Step 6720, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.590\n",
      "Epoch 14/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.154 Test reconstruction loss: 0.017 Test covariance loss: 13.688\n",
      "Epoch: 15/25, Step 6760, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.108\n",
      "Epoch: 15/25, Step 6800, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.032\n",
      "Epoch: 15/25, Step 6840, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.812\n",
      "Epoch: 15/25, Step 6880, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.058\n",
      "Epoch: 15/25, Step 6920, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.614\n",
      "Epoch: 15/25, Step 6960, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.806\n",
      "Epoch: 15/25, Step 7000, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.805\n",
      "Epoch: 15/25, Step 7040, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.893\n",
      "Epoch: 15/25, Step 7080, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.596\n",
      "Epoch: 15/25, Step 7120, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.530\n",
      "Epoch: 15/25, Step 7160, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.730\n",
      "Epoch: 15/25, Step 7200, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.759\n",
      "Epoch 15/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.175 Test reconstruction loss: 0.017 Test covariance loss: 15.786\n",
      "Epoch: 16/25, Step 7240, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.945\n",
      "Epoch: 16/25, Step 7280, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.653\n",
      "Epoch: 16/25, Step 7320, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.803\n",
      "Epoch: 16/25, Step 7360, Train loss: 0.162 Train reconstruction loss: 0.018 Train covariance loss: 14.453\n",
      "Epoch: 16/25, Step 7400, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.810\n",
      "Epoch: 16/25, Step 7440, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.490\n",
      "Epoch: 16/25, Step 7480, Train loss: 0.162 Train reconstruction loss: 0.018 Train covariance loss: 14.362\n",
      "Epoch: 16/25, Step 7520, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.054\n",
      "Epoch: 16/25, Step 7560, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.050\n",
      "Epoch: 16/25, Step 7600, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.908\n",
      "Epoch: 16/25, Step 7640, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.976\n",
      "Epoch: 16/25, Step 7680, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.886\n",
      "Epoch 16/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.175 Test reconstruction loss: 0.017 Test covariance loss: 15.789\n",
      "Epoch: 17/25, Step 7720, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.890\n",
      "Epoch: 17/25, Step 7760, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.882\n",
      "Epoch: 17/25, Step 7800, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.005\n",
      "Epoch: 17/25, Step 7840, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.667\n",
      "Epoch: 17/25, Step 7880, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.465\n",
      "Epoch: 17/25, Step 7920, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.082\n",
      "Epoch: 17/25, Step 7960, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.128\n",
      "Epoch: 17/25, Step 8000, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.104\n",
      "Epoch: 17/25, Step 8040, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.707\n",
      "Epoch: 17/25, Step 8080, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.843\n",
      "Epoch: 17/25, Step 8120, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.650\n",
      "Epoch: 17/25, Step 8160, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.815\n",
      "Epoch 17/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.161 Test reconstruction loss: 0.017 Test covariance loss: 14.358\n",
      "Epoch: 18/25, Step 8200, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.537\n",
      "Epoch: 18/25, Step 8240, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.071\n",
      "Epoch: 18/25, Step 8280, Train loss: 0.162 Train reconstruction loss: 0.018 Train covariance loss: 14.394\n",
      "Epoch: 18/25, Step 8320, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.861\n",
      "Epoch: 18/25, Step 8360, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.719\n",
      "Epoch: 18/25, Step 8400, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.991\n",
      "Epoch: 18/25, Step 8440, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.833\n",
      "Epoch: 18/25, Step 8480, Train loss: 0.165 Train reconstruction loss: 0.017 Train covariance loss: 14.802\n",
      "Epoch: 18/25, Step 8520, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.833\n",
      "Epoch: 18/25, Step 8560, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.910\n",
      "Epoch: 18/25, Step 8600, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.843\n",
      "Epoch: 18/25, Step 8640, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.772\n",
      "Epoch 18/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.184 Test reconstruction loss: 0.017 Test covariance loss: 16.642\n",
      "Epoch: 19/25, Step 8680, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.969\n",
      "Epoch: 19/25, Step 8720, Train loss: 0.170 Train reconstruction loss: 0.018 Train covariance loss: 15.237\n",
      "Epoch: 19/25, Step 8760, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.011\n",
      "Epoch: 19/25, Step 8800, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.717\n",
      "Epoch: 19/25, Step 8840, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.895\n",
      "Epoch: 19/25, Step 8880, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.740\n",
      "Epoch: 19/25, Step 8920, Train loss: 0.161 Train reconstruction loss: 0.018 Train covariance loss: 14.354\n",
      "Epoch: 19/25, Step 8960, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.829\n",
      "Epoch: 19/25, Step 9000, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.704\n",
      "Epoch: 19/25, Step 9040, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.950\n",
      "Epoch: 19/25, Step 9080, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.979\n",
      "Epoch: 19/25, Step 9120, Train loss: 0.162 Train reconstruction loss: 0.018 Train covariance loss: 14.412\n",
      "Epoch 19/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.160 Test reconstruction loss: 0.017 Test covariance loss: 14.278\n",
      "Epoch: 20/25, Step 9160, Train loss: 0.170 Train reconstruction loss: 0.018 Train covariance loss: 15.174\n",
      "Epoch: 20/25, Step 9200, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.806\n",
      "Epoch: 20/25, Step 9240, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.834\n",
      "Epoch: 20/25, Step 9280, Train loss: 0.165 Train reconstruction loss: 0.017 Train covariance loss: 14.789\n",
      "Epoch: 20/25, Step 9320, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.802\n",
      "Epoch: 20/25, Step 9360, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.913\n",
      "Epoch: 20/25, Step 9400, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.614\n",
      "Epoch: 20/25, Step 9440, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.122\n",
      "Epoch: 20/25, Step 9480, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.748\n",
      "Epoch: 20/25, Step 9520, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.882\n",
      "Epoch: 20/25, Step 9560, Train loss: 0.164 Train reconstruction loss: 0.017 Train covariance loss: 14.660\n",
      "Epoch: 20/25, Step 9600, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.697\n",
      "Epoch 20/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.151 Test reconstruction loss: 0.017 Test covariance loss: 13.354\n",
      "Epoch: 21/25, Step 9640, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.550\n",
      "Epoch: 21/25, Step 9680, Train loss: 0.162 Train reconstruction loss: 0.018 Train covariance loss: 14.439\n",
      "Epoch: 21/25, Step 9720, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.034\n",
      "Epoch: 21/25, Step 9760, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.726\n",
      "Epoch: 21/25, Step 9800, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 14.976\n",
      "Epoch: 21/25, Step 9840, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.675\n",
      "Epoch: 21/25, Step 9880, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.797\n",
      "Epoch: 21/25, Step 9920, Train loss: 0.168 Train reconstruction loss: 0.017 Train covariance loss: 15.094\n",
      "Epoch: 21/25, Step 9960, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.077\n",
      "Epoch: 21/25, Step 10000, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.924\n",
      "Epoch: 21/25, Step 10040, Train loss: 0.169 Train reconstruction loss: 0.017 Train covariance loss: 15.181\n",
      "Epoch: 21/25, Step 10080, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.957\n",
      "Epoch 21/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.173 Test reconstruction loss: 0.017 Test covariance loss: 15.594\n",
      "Epoch: 22/25, Step 10120, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.135\n",
      "Epoch: 22/25, Step 10160, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.718\n",
      "Epoch: 22/25, Step 10200, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.751\n",
      "Epoch: 22/25, Step 10240, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.669\n",
      "Epoch: 22/25, Step 10280, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.964\n",
      "Epoch: 22/25, Step 10320, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.685\n",
      "Epoch: 22/25, Step 10360, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.552\n",
      "Epoch: 22/25, Step 10400, Train loss: 0.170 Train reconstruction loss: 0.018 Train covariance loss: 15.222\n",
      "Epoch: 22/25, Step 10440, Train loss: 0.166 Train reconstruction loss: 0.017 Train covariance loss: 14.862\n",
      "Epoch: 22/25, Step 10480, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.865\n",
      "Epoch: 22/25, Step 10520, Train loss: 0.170 Train reconstruction loss: 0.018 Train covariance loss: 15.269\n",
      "Epoch: 22/25, Step 10560, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.634\n",
      "Epoch 22/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.170 Test reconstruction loss: 0.017 Test covariance loss: 15.253\n",
      "Epoch: 23/25, Step 10600, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.058\n",
      "Epoch: 23/25, Step 10640, Train loss: 0.162 Train reconstruction loss: 0.017 Train covariance loss: 14.446\n",
      "Epoch: 23/25, Step 10680, Train loss: 0.165 Train reconstruction loss: 0.017 Train covariance loss: 14.750\n",
      "Epoch: 23/25, Step 10720, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.049\n",
      "Epoch: 23/25, Step 10760, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.799\n",
      "Epoch: 23/25, Step 10800, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.852\n",
      "Epoch: 23/25, Step 10840, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.651\n",
      "Epoch: 23/25, Step 10880, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.111\n",
      "Epoch: 23/25, Step 10920, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.861\n",
      "Epoch: 23/25, Step 10960, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.907\n",
      "Epoch: 23/25, Step 11000, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.937\n",
      "Epoch: 23/25, Step 11040, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.769\n",
      "Epoch 23/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.157 Test reconstruction loss: 0.017 Test covariance loss: 13.936\n",
      "Epoch: 24/25, Step 11080, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.761\n",
      "Epoch: 24/25, Step 11120, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.897\n",
      "Epoch: 24/25, Step 11160, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.960\n",
      "Epoch: 24/25, Step 11200, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.587\n",
      "Epoch: 24/25, Step 11240, Train loss: 0.166 Train reconstruction loss: 0.018 Train covariance loss: 14.848\n",
      "Epoch: 24/25, Step 11280, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.706\n",
      "Epoch: 24/25, Step 11320, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.790\n",
      "Epoch: 24/25, Step 11360, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.081\n",
      "Epoch: 24/25, Step 11400, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.644\n",
      "Epoch: 24/25, Step 11440, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.044\n",
      "Epoch: 24/25, Step 11480, Train loss: 0.162 Train reconstruction loss: 0.018 Train covariance loss: 14.438\n",
      "Epoch: 24/25, Step 11520, Train loss: 0.167 Train reconstruction loss: 0.018 Train covariance loss: 14.911\n",
      "Epoch 24/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.161 Test reconstruction loss: 0.017 Test covariance loss: 14.381\n",
      "Epoch: 25/25, Step 11560, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.146\n",
      "Epoch: 25/25, Step 11600, Train loss: 0.163 Train reconstruction loss: 0.018 Train covariance loss: 14.557\n",
      "Epoch: 25/25, Step 11640, Train loss: 0.169 Train reconstruction loss: 0.018 Train covariance loss: 15.088\n",
      "Epoch: 25/25, Step 11680, Train loss: 0.160 Train reconstruction loss: 0.017 Train covariance loss: 14.255\n",
      "Epoch: 25/25, Step 11720, Train loss: 0.163 Train reconstruction loss: 0.017 Train covariance loss: 14.584\n",
      "Epoch: 25/25, Step 11760, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.645\n",
      "Epoch: 25/25, Step 11800, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.710\n",
      "Epoch: 25/25, Step 11840, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.772\n",
      "Epoch: 25/25, Step 11880, Train loss: 0.170 Train reconstruction loss: 0.017 Train covariance loss: 15.234\n",
      "Epoch: 25/25, Step 11920, Train loss: 0.164 Train reconstruction loss: 0.018 Train covariance loss: 14.652\n",
      "Epoch: 25/25, Step 11960, Train loss: 0.165 Train reconstruction loss: 0.018 Train covariance loss: 14.770\n",
      "Epoch: 25/25, Step 12000, Train loss: 0.168 Train reconstruction loss: 0.018 Train covariance loss: 15.007\n",
      "Epoch 25/25,\n",
      " Train Loss: 0.166 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.161 Test reconstruction loss: 0.017 Test covariance loss: 14.348\n",
      "Training with hidden dim: 4\n",
      "Epoch: 1/25, Step 40, Train loss: 0.257 Train reconstruction loss: 0.042 Train covariance loss: 21.467\n",
      "Epoch: 1/25, Step 80, Train loss: 0.235 Train reconstruction loss: 0.022 Train covariance loss: 21.375\n",
      "Epoch: 1/25, Step 120, Train loss: 0.235 Train reconstruction loss: 0.021 Train covariance loss: 21.342\n",
      "Epoch: 1/25, Step 160, Train loss: 0.235 Train reconstruction loss: 0.020 Train covariance loss: 21.407\n",
      "Epoch: 1/25, Step 200, Train loss: 0.230 Train reconstruction loss: 0.020 Train covariance loss: 20.976\n",
      "Epoch: 1/25, Step 240, Train loss: 0.230 Train reconstruction loss: 0.020 Train covariance loss: 21.032\n",
      "Epoch: 1/25, Step 280, Train loss: 0.234 Train reconstruction loss: 0.020 Train covariance loss: 21.388\n",
      "Epoch: 1/25, Step 320, Train loss: 0.233 Train reconstruction loss: 0.020 Train covariance loss: 21.289\n",
      "Epoch: 1/25, Step 360, Train loss: 0.240 Train reconstruction loss: 0.019 Train covariance loss: 22.003\n",
      "Epoch: 1/25, Step 400, Train loss: 0.230 Train reconstruction loss: 0.019 Train covariance loss: 21.113\n",
      "Epoch: 1/25, Step 440, Train loss: 0.231 Train reconstruction loss: 0.019 Train covariance loss: 21.213\n",
      "Epoch: 1/25, Step 480, Train loss: 0.233 Train reconstruction loss: 0.019 Train covariance loss: 21.429\n",
      "Test loss decreased (inf --> 0.252032).  Saving model ...\n",
      "Epoch 1/25,\n",
      " Train Loss: 0.235 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.252 Test reconstruction loss: 0.019 Test covariance loss: 23.339\n",
      "Epoch: 2/25, Step 520, Train loss: 0.229 Train reconstruction loss: 0.019 Train covariance loss: 20.999\n",
      "Epoch: 2/25, Step 560, Train loss: 0.229 Train reconstruction loss: 0.019 Train covariance loss: 21.000\n",
      "Epoch: 2/25, Step 600, Train loss: 0.233 Train reconstruction loss: 0.019 Train covariance loss: 21.381\n",
      "Epoch: 2/25, Step 640, Train loss: 0.234 Train reconstruction loss: 0.019 Train covariance loss: 21.458\n",
      "Epoch: 2/25, Step 680, Train loss: 0.235 Train reconstruction loss: 0.019 Train covariance loss: 21.581\n",
      "Epoch: 2/25, Step 720, Train loss: 0.231 Train reconstruction loss: 0.019 Train covariance loss: 21.224\n",
      "Epoch: 2/25, Step 760, Train loss: 0.230 Train reconstruction loss: 0.019 Train covariance loss: 21.120\n",
      "Epoch: 2/25, Step 800, Train loss: 0.230 Train reconstruction loss: 0.019 Train covariance loss: 21.155\n",
      "Epoch: 2/25, Step 840, Train loss: 0.231 Train reconstruction loss: 0.019 Train covariance loss: 21.250\n",
      "Epoch: 2/25, Step 880, Train loss: 0.235 Train reconstruction loss: 0.019 Train covariance loss: 21.618\n",
      "Epoch: 2/25, Step 920, Train loss: 0.229 Train reconstruction loss: 0.019 Train covariance loss: 21.038\n",
      "Epoch: 2/25, Step 960, Train loss: 0.232 Train reconstruction loss: 0.019 Train covariance loss: 21.338\n",
      "Epoch 2/25,\n",
      " Train Loss: 0.232 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.261 Test reconstruction loss: 0.018 Test covariance loss: 24.284\n",
      "Epoch: 3/25, Step 1000, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.204\n",
      "Epoch: 3/25, Step 1040, Train loss: 0.233 Train reconstruction loss: 0.019 Train covariance loss: 21.408\n",
      "Epoch: 3/25, Step 1080, Train loss: 0.235 Train reconstruction loss: 0.019 Train covariance loss: 21.618\n",
      "Epoch: 3/25, Step 1120, Train loss: 0.225 Train reconstruction loss: 0.018 Train covariance loss: 20.697\n",
      "Epoch: 3/25, Step 1160, Train loss: 0.232 Train reconstruction loss: 0.019 Train covariance loss: 21.324\n",
      "Epoch: 3/25, Step 1200, Train loss: 0.236 Train reconstruction loss: 0.019 Train covariance loss: 21.750\n",
      "Epoch: 3/25, Step 1240, Train loss: 0.228 Train reconstruction loss: 0.019 Train covariance loss: 20.994\n",
      "Epoch: 3/25, Step 1280, Train loss: 0.232 Train reconstruction loss: 0.019 Train covariance loss: 21.281\n",
      "Epoch: 3/25, Step 1320, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.257\n",
      "Epoch: 3/25, Step 1360, Train loss: 0.230 Train reconstruction loss: 0.019 Train covariance loss: 21.119\n",
      "Epoch: 3/25, Step 1400, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.413\n",
      "Epoch: 3/25, Step 1440, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.566\n",
      "Test loss decreased (0.252032 --> 0.248629).  Saving model ...\n",
      "Epoch 3/25,\n",
      " Train Loss: 0.232 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.249 Test reconstruction loss: 0.018 Test covariance loss: 23.069\n",
      "Epoch: 4/25, Step 1480, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.544\n",
      "Epoch: 4/25, Step 1520, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.434\n",
      "Epoch: 4/25, Step 1560, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.291\n",
      "Epoch: 4/25, Step 1600, Train loss: 0.226 Train reconstruction loss: 0.019 Train covariance loss: 20.760\n",
      "Epoch: 4/25, Step 1640, Train loss: 0.232 Train reconstruction loss: 0.019 Train covariance loss: 21.368\n",
      "Epoch: 4/25, Step 1680, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.483\n",
      "Epoch: 4/25, Step 1720, Train loss: 0.236 Train reconstruction loss: 0.018 Train covariance loss: 21.796\n",
      "Epoch: 4/25, Step 1760, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.159\n",
      "Epoch: 4/25, Step 1800, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.925\n",
      "Epoch: 4/25, Step 1840, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.086\n",
      "Epoch: 4/25, Step 1880, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.847\n",
      "Epoch: 4/25, Step 1920, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.048\n",
      "Epoch 4/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.267 Test reconstruction loss: 0.018 Test covariance loss: 24.894\n",
      "Epoch: 5/25, Step 1960, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.483\n",
      "Epoch: 5/25, Step 2000, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 20.939\n",
      "Epoch: 5/25, Step 2040, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 20.987\n",
      "Epoch: 5/25, Step 2080, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.883\n",
      "Epoch: 5/25, Step 2120, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.626\n",
      "Epoch: 5/25, Step 2160, Train loss: 0.234 Train reconstruction loss: 0.019 Train covariance loss: 21.524\n",
      "Epoch: 5/25, Step 2200, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.215\n",
      "Epoch: 5/25, Step 2240, Train loss: 0.239 Train reconstruction loss: 0.019 Train covariance loss: 22.055\n",
      "Epoch: 5/25, Step 2280, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.257\n",
      "Epoch: 5/25, Step 2320, Train loss: 0.236 Train reconstruction loss: 0.018 Train covariance loss: 21.791\n",
      "Epoch: 5/25, Step 2360, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.548\n",
      "Epoch: 5/25, Step 2400, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.384\n",
      "Epoch 5/25,\n",
      " Train Loss: 0.232 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.252 Test reconstruction loss: 0.018 Test covariance loss: 23.470\n",
      "Epoch: 6/25, Step 2440, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.294\n",
      "Epoch: 6/25, Step 2480, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.637\n",
      "Epoch: 6/25, Step 2520, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.445\n",
      "Epoch: 6/25, Step 2560, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.563\n",
      "Epoch: 6/25, Step 2600, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.177\n",
      "Epoch: 6/25, Step 2640, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.888\n",
      "Epoch: 6/25, Step 2680, Train loss: 0.230 Train reconstruction loss: 0.019 Train covariance loss: 21.140\n",
      "Epoch: 6/25, Step 2720, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.649\n",
      "Epoch: 6/25, Step 2760, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.633\n",
      "Epoch: 6/25, Step 2800, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.134\n",
      "Epoch: 6/25, Step 2840, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.213\n",
      "Epoch: 6/25, Step 2880, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.402\n",
      "Test loss decreased (0.248629 --> 0.223268).  Saving model ...\n",
      "Epoch 6/25,\n",
      " Train Loss: 0.232 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.223 Test reconstruction loss: 0.018 Test covariance loss: 20.562\n",
      "Epoch: 7/25, Step 2920, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.100\n",
      "Epoch: 7/25, Step 2960, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.574\n",
      "Epoch: 7/25, Step 3000, Train loss: 0.236 Train reconstruction loss: 0.018 Train covariance loss: 21.768\n",
      "Epoch: 7/25, Step 3040, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.091\n",
      "Epoch: 7/25, Step 3080, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.936\n",
      "Epoch: 7/25, Step 3120, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.591\n",
      "Epoch: 7/25, Step 3160, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.352\n",
      "Epoch: 7/25, Step 3200, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.552\n",
      "Epoch: 7/25, Step 3240, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.344\n",
      "Epoch: 7/25, Step 3280, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.532\n",
      "Epoch: 7/25, Step 3320, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.812\n",
      "Epoch: 7/25, Step 3360, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.722\n",
      "Epoch 7/25,\n",
      " Train Loss: 0.232 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.248 Test reconstruction loss: 0.018 Test covariance loss: 23.031\n",
      "Epoch: 8/25, Step 3400, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.378\n",
      "Epoch: 8/25, Step 3440, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.856\n",
      "Epoch: 8/25, Step 3480, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.777\n",
      "Epoch: 8/25, Step 3520, Train loss: 0.225 Train reconstruction loss: 0.018 Train covariance loss: 20.698\n",
      "Epoch: 8/25, Step 3560, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.604\n",
      "Epoch: 8/25, Step 3600, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.533\n",
      "Epoch: 8/25, Step 3640, Train loss: 0.239 Train reconstruction loss: 0.018 Train covariance loss: 22.137\n",
      "Epoch: 8/25, Step 3680, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.195\n",
      "Epoch: 8/25, Step 3720, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.511\n",
      "Epoch: 8/25, Step 3760, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.373\n",
      "Epoch: 8/25, Step 3800, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 20.957\n",
      "Epoch: 8/25, Step 3840, Train loss: 0.237 Train reconstruction loss: 0.018 Train covariance loss: 21.937\n",
      "Epoch 8/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.250 Test reconstruction loss: 0.017 Test covariance loss: 23.246\n",
      "Epoch: 9/25, Step 3880, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.310\n",
      "Epoch: 9/25, Step 3920, Train loss: 0.237 Train reconstruction loss: 0.018 Train covariance loss: 21.872\n",
      "Epoch: 9/25, Step 3960, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.124\n",
      "Epoch: 9/25, Step 4000, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 20.980\n",
      "Epoch: 9/25, Step 4040, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.025\n",
      "Epoch: 9/25, Step 4080, Train loss: 0.224 Train reconstruction loss: 0.018 Train covariance loss: 20.599\n",
      "Epoch: 9/25, Step 4120, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.098\n",
      "Epoch: 9/25, Step 4160, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.323\n",
      "Epoch: 9/25, Step 4200, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.837\n",
      "Epoch: 9/25, Step 4240, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.566\n",
      "Epoch: 9/25, Step 4280, Train loss: 0.240 Train reconstruction loss: 0.018 Train covariance loss: 22.155\n",
      "Epoch: 9/25, Step 4320, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.439\n",
      "Epoch 9/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.259 Test reconstruction loss: 0.017 Test covariance loss: 24.180\n",
      "Epoch: 10/25, Step 4360, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.339\n",
      "Epoch: 10/25, Step 4400, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.355\n",
      "Epoch: 10/25, Step 4440, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.520\n",
      "Epoch: 10/25, Step 4480, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.663\n",
      "Epoch: 10/25, Step 4520, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.982\n",
      "Epoch: 10/25, Step 4560, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.947\n",
      "Epoch: 10/25, Step 4600, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.165\n",
      "Epoch: 10/25, Step 4640, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.977\n",
      "Epoch: 10/25, Step 4680, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.474\n",
      "Epoch: 10/25, Step 4720, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.056\n",
      "Epoch: 10/25, Step 4760, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.367\n",
      "Epoch: 10/25, Step 4800, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.622\n",
      "Epoch 10/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.282 Test reconstruction loss: 0.017 Test covariance loss: 26.454\n",
      "Epoch: 11/25, Step 4840, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.866\n",
      "Epoch: 11/25, Step 4880, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.542\n",
      "Epoch: 11/25, Step 4920, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.840\n",
      "Epoch: 11/25, Step 4960, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.244\n",
      "Epoch: 11/25, Step 5000, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.731\n",
      "Epoch: 11/25, Step 5040, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.451\n",
      "Epoch: 11/25, Step 5080, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.562\n",
      "Epoch: 11/25, Step 5120, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.647\n",
      "Epoch: 11/25, Step 5160, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.397\n",
      "Epoch: 11/25, Step 5200, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.209\n",
      "Epoch: 11/25, Step 5240, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.020\n",
      "Epoch: 11/25, Step 5280, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.426\n",
      "Epoch 11/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.249 Test reconstruction loss: 0.017 Test covariance loss: 23.114\n",
      "Epoch: 12/25, Step 5320, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.534\n",
      "Epoch: 12/25, Step 5360, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.530\n",
      "Epoch: 12/25, Step 5400, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.573\n",
      "Epoch: 12/25, Step 5440, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.012\n",
      "Epoch: 12/25, Step 5480, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.128\n",
      "Epoch: 12/25, Step 5520, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.466\n",
      "Epoch: 12/25, Step 5560, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.315\n",
      "Epoch: 12/25, Step 5600, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.060\n",
      "Epoch: 12/25, Step 5640, Train loss: 0.240 Train reconstruction loss: 0.018 Train covariance loss: 22.263\n",
      "Epoch: 12/25, Step 5680, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.381\n",
      "Epoch: 12/25, Step 5720, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.136\n",
      "Epoch: 12/25, Step 5760, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.928\n",
      "Epoch 12/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.231 Test reconstruction loss: 0.017 Test covariance loss: 21.407\n",
      "Epoch: 13/25, Step 5800, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.172\n",
      "Epoch: 13/25, Step 5840, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.766\n",
      "Epoch: 13/25, Step 5880, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.475\n",
      "Epoch: 13/25, Step 5920, Train loss: 0.238 Train reconstruction loss: 0.018 Train covariance loss: 22.061\n",
      "Epoch: 13/25, Step 5960, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.700\n",
      "Epoch: 13/25, Step 6000, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.285\n",
      "Epoch: 13/25, Step 6040, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.111\n",
      "Epoch: 13/25, Step 6080, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.018\n",
      "Epoch: 13/25, Step 6120, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.060\n",
      "Epoch: 13/25, Step 6160, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.308\n",
      "Epoch: 13/25, Step 6200, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.082\n",
      "Epoch: 13/25, Step 6240, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.545\n",
      "Test loss decreased (0.223268 --> 0.210883).  Saving model ...\n",
      "Epoch 13/25,\n",
      " Train Loss: 0.232 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.211 Test reconstruction loss: 0.017 Test covariance loss: 19.353\n",
      "Epoch: 14/25, Step 6280, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.165\n",
      "Epoch: 14/25, Step 6320, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.513\n",
      "Epoch: 14/25, Step 6360, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.968\n",
      "Epoch: 14/25, Step 6400, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.876\n",
      "Epoch: 14/25, Step 6440, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.151\n",
      "Epoch: 14/25, Step 6480, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.453\n",
      "Epoch: 14/25, Step 6520, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.466\n",
      "Epoch: 14/25, Step 6560, Train loss: 0.231 Train reconstruction loss: 0.017 Train covariance loss: 21.317\n",
      "Epoch: 14/25, Step 6600, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.460\n",
      "Epoch: 14/25, Step 6640, Train loss: 0.231 Train reconstruction loss: 0.017 Train covariance loss: 21.352\n",
      "Epoch: 14/25, Step 6680, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.249\n",
      "Epoch: 14/25, Step 6720, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.307\n",
      "Epoch 14/25,\n",
      " Train Loss: 0.230 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.228 Test reconstruction loss: 0.017 Test covariance loss: 21.095\n",
      "Epoch: 15/25, Step 6760, Train loss: 0.238 Train reconstruction loss: 0.018 Train covariance loss: 22.007\n",
      "Epoch: 15/25, Step 6800, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.364\n",
      "Epoch: 15/25, Step 6840, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.403\n",
      "Epoch: 15/25, Step 6880, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.141\n",
      "Epoch: 15/25, Step 6920, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.054\n",
      "Epoch: 15/25, Step 6960, Train loss: 0.230 Train reconstruction loss: 0.017 Train covariance loss: 21.273\n",
      "Epoch: 15/25, Step 7000, Train loss: 0.226 Train reconstruction loss: 0.017 Train covariance loss: 20.831\n",
      "Epoch: 15/25, Step 7040, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.410\n",
      "Epoch: 15/25, Step 7080, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.032\n",
      "Epoch: 15/25, Step 7120, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.380\n",
      "Epoch: 15/25, Step 7160, Train loss: 0.233 Train reconstruction loss: 0.017 Train covariance loss: 21.512\n",
      "Epoch: 15/25, Step 7200, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.171\n",
      "Epoch 15/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.247 Test reconstruction loss: 0.017 Test covariance loss: 23.008\n",
      "Epoch: 16/25, Step 7240, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.196\n",
      "Epoch: 16/25, Step 7280, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.134\n",
      "Epoch: 16/25, Step 7320, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.735\n",
      "Epoch: 16/25, Step 7360, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.454\n",
      "Epoch: 16/25, Step 7400, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.828\n",
      "Epoch: 16/25, Step 7440, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.965\n",
      "Epoch: 16/25, Step 7480, Train loss: 0.227 Train reconstruction loss: 0.017 Train covariance loss: 20.932\n",
      "Epoch: 16/25, Step 7520, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.751\n",
      "Epoch: 16/25, Step 7560, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.523\n",
      "Epoch: 16/25, Step 7600, Train loss: 0.232 Train reconstruction loss: 0.017 Train covariance loss: 21.476\n",
      "Epoch: 16/25, Step 7640, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.146\n",
      "Epoch: 16/25, Step 7680, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.463\n",
      "Epoch 16/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.247 Test reconstruction loss: 0.017 Test covariance loss: 22.996\n",
      "Epoch: 17/25, Step 7720, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.330\n",
      "Epoch: 17/25, Step 7760, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.324\n",
      "Epoch: 17/25, Step 7800, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.671\n",
      "Epoch: 17/25, Step 7840, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.063\n",
      "Epoch: 17/25, Step 7880, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.948\n",
      "Epoch: 17/25, Step 7920, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.289\n",
      "Epoch: 17/25, Step 7960, Train loss: 0.227 Train reconstruction loss: 0.017 Train covariance loss: 21.011\n",
      "Epoch: 17/25, Step 8000, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.375\n",
      "Epoch: 17/25, Step 8040, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.073\n",
      "Epoch: 17/25, Step 8080, Train loss: 0.225 Train reconstruction loss: 0.018 Train covariance loss: 20.766\n",
      "Epoch: 17/25, Step 8120, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.039\n",
      "Epoch: 17/25, Step 8160, Train loss: 0.240 Train reconstruction loss: 0.018 Train covariance loss: 22.265\n",
      "Epoch 17/25,\n",
      " Train Loss: 0.230 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.231 Test reconstruction loss: 0.017 Test covariance loss: 21.328\n",
      "Epoch: 18/25, Step 8200, Train loss: 0.232 Train reconstruction loss: 0.017 Train covariance loss: 21.462\n",
      "Epoch: 18/25, Step 8240, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.246\n",
      "Epoch: 18/25, Step 8280, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.392\n",
      "Epoch: 18/25, Step 8320, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.841\n",
      "Epoch: 18/25, Step 8360, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.673\n",
      "Epoch: 18/25, Step 8400, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.381\n",
      "Epoch: 18/25, Step 8440, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.411\n",
      "Epoch: 18/25, Step 8480, Train loss: 0.224 Train reconstruction loss: 0.018 Train covariance loss: 20.601\n",
      "Epoch: 18/25, Step 8520, Train loss: 0.235 Train reconstruction loss: 0.017 Train covariance loss: 21.761\n",
      "Epoch: 18/25, Step 8560, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.082\n",
      "Epoch: 18/25, Step 8600, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.579\n",
      "Epoch: 18/25, Step 8640, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 20.999\n",
      "Epoch 18/25,\n",
      " Train Loss: 0.230 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.242 Test reconstruction loss: 0.017 Test covariance loss: 22.444\n",
      "Epoch: 19/25, Step 8680, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.271\n",
      "Epoch: 19/25, Step 8720, Train loss: 0.231 Train reconstruction loss: 0.017 Train covariance loss: 21.310\n",
      "Epoch: 19/25, Step 8760, Train loss: 0.230 Train reconstruction loss: 0.017 Train covariance loss: 21.234\n",
      "Epoch: 19/25, Step 8800, Train loss: 0.221 Train reconstruction loss: 0.017 Train covariance loss: 20.354\n",
      "Epoch: 19/25, Step 8840, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.091\n",
      "Epoch: 19/25, Step 8880, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.584\n",
      "Epoch: 19/25, Step 8920, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.694\n",
      "Epoch: 19/25, Step 8960, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.737\n",
      "Epoch: 19/25, Step 9000, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.214\n",
      "Epoch: 19/25, Step 9040, Train loss: 0.232 Train reconstruction loss: 0.017 Train covariance loss: 21.487\n",
      "Epoch: 19/25, Step 9080, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.406\n",
      "Epoch: 19/25, Step 9120, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.270\n",
      "Epoch 19/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.234 Test reconstruction loss: 0.017 Test covariance loss: 21.638\n",
      "Epoch: 20/25, Step 9160, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.540\n",
      "Epoch: 20/25, Step 9200, Train loss: 0.225 Train reconstruction loss: 0.017 Train covariance loss: 20.798\n",
      "Epoch: 20/25, Step 9240, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.218\n",
      "Epoch: 20/25, Step 9280, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.252\n",
      "Epoch: 20/25, Step 9320, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.178\n",
      "Epoch: 20/25, Step 9360, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.386\n",
      "Epoch: 20/25, Step 9400, Train loss: 0.235 Train reconstruction loss: 0.018 Train covariance loss: 21.751\n",
      "Epoch: 20/25, Step 9440, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.233\n",
      "Epoch: 20/25, Step 9480, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.419\n",
      "Epoch: 20/25, Step 9520, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.017\n",
      "Epoch: 20/25, Step 9560, Train loss: 0.237 Train reconstruction loss: 0.017 Train covariance loss: 21.925\n",
      "Epoch: 20/25, Step 9600, Train loss: 0.223 Train reconstruction loss: 0.017 Train covariance loss: 20.533\n",
      "Epoch 20/25,\n",
      " Train Loss: 0.230 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.265 Test reconstruction loss: 0.017 Test covariance loss: 24.827\n",
      "Epoch: 21/25, Step 9640, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.524\n",
      "Epoch: 21/25, Step 9680, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.630\n",
      "Epoch: 21/25, Step 9720, Train loss: 0.231 Train reconstruction loss: 0.017 Train covariance loss: 21.386\n",
      "Epoch: 21/25, Step 9760, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.942\n",
      "Epoch: 21/25, Step 9800, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.855\n",
      "Epoch: 21/25, Step 9840, Train loss: 0.226 Train reconstruction loss: 0.017 Train covariance loss: 20.910\n",
      "Epoch: 21/25, Step 9880, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.589\n",
      "Epoch: 21/25, Step 9920, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.918\n",
      "Epoch: 21/25, Step 9960, Train loss: 0.230 Train reconstruction loss: 0.017 Train covariance loss: 21.282\n",
      "Epoch: 21/25, Step 10000, Train loss: 0.231 Train reconstruction loss: 0.017 Train covariance loss: 21.333\n",
      "Epoch: 21/25, Step 10040, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.384\n",
      "Epoch: 21/25, Step 10080, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.588\n",
      "Test loss decreased (0.210883 --> 0.206782).  Saving model ...\n",
      "Epoch 21/25,\n",
      " Train Loss: 0.230 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.207 Test reconstruction loss: 0.017 Test covariance loss: 18.957\n",
      "Epoch: 22/25, Step 10120, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.287\n",
      "Epoch: 22/25, Step 10160, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.228\n",
      "Epoch: 22/25, Step 10200, Train loss: 0.235 Train reconstruction loss: 0.017 Train covariance loss: 21.726\n",
      "Epoch: 22/25, Step 10240, Train loss: 0.234 Train reconstruction loss: 0.017 Train covariance loss: 21.625\n",
      "Epoch: 22/25, Step 10280, Train loss: 0.229 Train reconstruction loss: 0.017 Train covariance loss: 21.163\n",
      "Epoch: 22/25, Step 10320, Train loss: 0.233 Train reconstruction loss: 0.017 Train covariance loss: 21.520\n",
      "Epoch: 22/25, Step 10360, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 21.020\n",
      "Epoch: 22/25, Step 10400, Train loss: 0.228 Train reconstruction loss: 0.017 Train covariance loss: 21.027\n",
      "Epoch: 22/25, Step 10440, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.574\n",
      "Epoch: 22/25, Step 10480, Train loss: 0.232 Train reconstruction loss: 0.017 Train covariance loss: 21.422\n",
      "Epoch: 22/25, Step 10520, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.216\n",
      "Epoch: 22/25, Step 10560, Train loss: 0.232 Train reconstruction loss: 0.017 Train covariance loss: 21.412\n",
      "Epoch 22/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.253 Test reconstruction loss: 0.017 Test covariance loss: 23.620\n",
      "Epoch: 23/25, Step 10600, Train loss: 0.227 Train reconstruction loss: 0.017 Train covariance loss: 20.915\n",
      "Epoch: 23/25, Step 10640, Train loss: 0.228 Train reconstruction loss: 0.017 Train covariance loss: 21.069\n",
      "Epoch: 23/25, Step 10680, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.257\n",
      "Epoch: 23/25, Step 10720, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.257\n",
      "Epoch: 23/25, Step 10760, Train loss: 0.233 Train reconstruction loss: 0.017 Train covariance loss: 21.581\n",
      "Epoch: 23/25, Step 10800, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.620\n",
      "Epoch: 23/25, Step 10840, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.837\n",
      "Epoch: 23/25, Step 10880, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.191\n",
      "Epoch: 23/25, Step 10920, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.658\n",
      "Epoch: 23/25, Step 10960, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.526\n",
      "Epoch: 23/25, Step 11000, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.275\n",
      "Epoch: 23/25, Step 11040, Train loss: 0.233 Train reconstruction loss: 0.017 Train covariance loss: 21.531\n",
      "Epoch 23/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.261 Test reconstruction loss: 0.017 Test covariance loss: 24.423\n",
      "Epoch: 24/25, Step 11080, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.092\n",
      "Epoch: 24/25, Step 11120, Train loss: 0.237 Train reconstruction loss: 0.018 Train covariance loss: 21.911\n",
      "Epoch: 24/25, Step 11160, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.094\n",
      "Epoch: 24/25, Step 11200, Train loss: 0.231 Train reconstruction loss: 0.017 Train covariance loss: 21.408\n",
      "Epoch: 24/25, Step 11240, Train loss: 0.234 Train reconstruction loss: 0.018 Train covariance loss: 21.606\n",
      "Epoch: 24/25, Step 11280, Train loss: 0.233 Train reconstruction loss: 0.017 Train covariance loss: 21.512\n",
      "Epoch: 24/25, Step 11320, Train loss: 0.226 Train reconstruction loss: 0.018 Train covariance loss: 20.843\n",
      "Epoch: 24/25, Step 11360, Train loss: 0.230 Train reconstruction loss: 0.018 Train covariance loss: 21.180\n",
      "Epoch: 24/25, Step 11400, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.194\n",
      "Epoch: 24/25, Step 11440, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.923\n",
      "Epoch: 24/25, Step 11480, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.448\n",
      "Epoch: 24/25, Step 11520, Train loss: 0.227 Train reconstruction loss: 0.017 Train covariance loss: 20.957\n",
      "Epoch 24/25,\n",
      " Train Loss: 0.230 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.242 Test reconstruction loss: 0.017 Test covariance loss: 22.500\n",
      "Epoch: 25/25, Step 11560, Train loss: 0.225 Train reconstruction loss: 0.017 Train covariance loss: 20.723\n",
      "Epoch: 25/25, Step 11600, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.581\n",
      "Epoch: 25/25, Step 11640, Train loss: 0.231 Train reconstruction loss: 0.018 Train covariance loss: 21.387\n",
      "Epoch: 25/25, Step 11680, Train loss: 0.232 Train reconstruction loss: 0.018 Train covariance loss: 21.487\n",
      "Epoch: 25/25, Step 11720, Train loss: 0.233 Train reconstruction loss: 0.018 Train covariance loss: 21.509\n",
      "Epoch: 25/25, Step 11760, Train loss: 0.233 Train reconstruction loss: 0.017 Train covariance loss: 21.601\n",
      "Epoch: 25/25, Step 11800, Train loss: 0.237 Train reconstruction loss: 0.018 Train covariance loss: 21.955\n",
      "Epoch: 25/25, Step 11840, Train loss: 0.227 Train reconstruction loss: 0.018 Train covariance loss: 20.910\n",
      "Epoch: 25/25, Step 11880, Train loss: 0.228 Train reconstruction loss: 0.018 Train covariance loss: 20.984\n",
      "Epoch: 25/25, Step 11920, Train loss: 0.232 Train reconstruction loss: 0.017 Train covariance loss: 21.422\n",
      "Epoch: 25/25, Step 11960, Train loss: 0.229 Train reconstruction loss: 0.018 Train covariance loss: 21.134\n",
      "Epoch: 25/25, Step 12000, Train loss: 0.229 Train reconstruction loss: 0.017 Train covariance loss: 21.165\n",
      "Epoch 25/25,\n",
      " Train Loss: 0.231 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.231 Test reconstruction loss: 0.017 Test covariance loss: 21.334\n",
      "Training with hidden dim: 5\n",
      "Epoch: 1/25, Step 40, Train loss: 0.380 Train reconstruction loss: 0.086 Train covariance loss: 29.411\n",
      "Epoch: 1/25, Step 80, Train loss: 0.313 Train reconstruction loss: 0.024 Train covariance loss: 28.898\n",
      "Epoch: 1/25, Step 120, Train loss: 0.307 Train reconstruction loss: 0.022 Train covariance loss: 28.519\n",
      "Epoch: 1/25, Step 160, Train loss: 0.309 Train reconstruction loss: 0.021 Train covariance loss: 28.791\n",
      "Epoch: 1/25, Step 200, Train loss: 0.313 Train reconstruction loss: 0.021 Train covariance loss: 29.197\n",
      "Epoch: 1/25, Step 240, Train loss: 0.314 Train reconstruction loss: 0.021 Train covariance loss: 29.334\n",
      "Epoch: 1/25, Step 280, Train loss: 0.313 Train reconstruction loss: 0.021 Train covariance loss: 29.227\n",
      "Epoch: 1/25, Step 320, Train loss: 0.311 Train reconstruction loss: 0.020 Train covariance loss: 29.050\n",
      "Epoch: 1/25, Step 360, Train loss: 0.314 Train reconstruction loss: 0.020 Train covariance loss: 29.416\n",
      "Epoch: 1/25, Step 400, Train loss: 0.307 Train reconstruction loss: 0.020 Train covariance loss: 28.650\n",
      "Epoch: 1/25, Step 440, Train loss: 0.315 Train reconstruction loss: 0.020 Train covariance loss: 29.498\n",
      "Epoch: 1/25, Step 480, Train loss: 0.313 Train reconstruction loss: 0.020 Train covariance loss: 29.338\n",
      "Test loss decreased (inf --> 0.308809).  Saving model ...\n",
      "Epoch 1/25,\n",
      " Train Loss: 0.317 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.309 Test reconstruction loss: 0.019 Test covariance loss: 29.007\n",
      "Epoch: 2/25, Step 520, Train loss: 0.311 Train reconstruction loss: 0.020 Train covariance loss: 29.120\n",
      "Epoch: 2/25, Step 560, Train loss: 0.313 Train reconstruction loss: 0.020 Train covariance loss: 29.349\n",
      "Epoch: 2/25, Step 600, Train loss: 0.308 Train reconstruction loss: 0.019 Train covariance loss: 28.932\n",
      "Epoch: 2/25, Step 640, Train loss: 0.310 Train reconstruction loss: 0.019 Train covariance loss: 29.125\n",
      "Epoch: 2/25, Step 680, Train loss: 0.313 Train reconstruction loss: 0.019 Train covariance loss: 29.371\n",
      "Epoch: 2/25, Step 720, Train loss: 0.318 Train reconstruction loss: 0.019 Train covariance loss: 29.837\n",
      "Epoch: 2/25, Step 760, Train loss: 0.310 Train reconstruction loss: 0.019 Train covariance loss: 29.062\n",
      "Epoch: 2/25, Step 800, Train loss: 0.315 Train reconstruction loss: 0.019 Train covariance loss: 29.549\n",
      "Epoch: 2/25, Step 840, Train loss: 0.311 Train reconstruction loss: 0.019 Train covariance loss: 29.189\n",
      "Epoch: 2/25, Step 880, Train loss: 0.313 Train reconstruction loss: 0.019 Train covariance loss: 29.364\n",
      "Epoch: 2/25, Step 920, Train loss: 0.309 Train reconstruction loss: 0.019 Train covariance loss: 28.957\n",
      "Epoch: 2/25, Step 960, Train loss: 0.306 Train reconstruction loss: 0.019 Train covariance loss: 28.685\n",
      "Epoch 2/25,\n",
      " Train Loss: 0.311 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.315 Test reconstruction loss: 0.018 Test covariance loss: 29.633\n",
      "Epoch: 3/25, Step 1000, Train loss: 0.307 Train reconstruction loss: 0.019 Train covariance loss: 28.780\n",
      "Epoch: 3/25, Step 1040, Train loss: 0.311 Train reconstruction loss: 0.019 Train covariance loss: 29.150\n",
      "Epoch: 3/25, Step 1080, Train loss: 0.312 Train reconstruction loss: 0.019 Train covariance loss: 29.358\n",
      "Epoch: 3/25, Step 1120, Train loss: 0.312 Train reconstruction loss: 0.019 Train covariance loss: 29.257\n",
      "Epoch: 3/25, Step 1160, Train loss: 0.306 Train reconstruction loss: 0.019 Train covariance loss: 28.779\n",
      "Epoch: 3/25, Step 1200, Train loss: 0.314 Train reconstruction loss: 0.019 Train covariance loss: 29.529\n",
      "Epoch: 3/25, Step 1240, Train loss: 0.311 Train reconstruction loss: 0.019 Train covariance loss: 29.167\n",
      "Epoch: 3/25, Step 1280, Train loss: 0.309 Train reconstruction loss: 0.019 Train covariance loss: 29.060\n",
      "Epoch: 3/25, Step 1320, Train loss: 0.312 Train reconstruction loss: 0.019 Train covariance loss: 29.373\n",
      "Epoch: 3/25, Step 1360, Train loss: 0.307 Train reconstruction loss: 0.019 Train covariance loss: 28.843\n",
      "Epoch: 3/25, Step 1400, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.289\n",
      "Epoch: 3/25, Step 1440, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.307\n",
      "Epoch 3/25,\n",
      " Train Loss: 0.310 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.361 Test reconstruction loss: 0.018 Test covariance loss: 34.263\n",
      "Epoch: 4/25, Step 1480, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.624\n",
      "Epoch: 4/25, Step 1520, Train loss: 0.313 Train reconstruction loss: 0.019 Train covariance loss: 29.399\n",
      "Epoch: 4/25, Step 1560, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.114\n",
      "Epoch: 4/25, Step 1600, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.730\n",
      "Epoch: 4/25, Step 1640, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.124\n",
      "Epoch: 4/25, Step 1680, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.803\n",
      "Epoch: 4/25, Step 1720, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.058\n",
      "Epoch: 4/25, Step 1760, Train loss: 0.307 Train reconstruction loss: 0.019 Train covariance loss: 28.870\n",
      "Epoch: 4/25, Step 1800, Train loss: 0.304 Train reconstruction loss: 0.019 Train covariance loss: 28.570\n",
      "Epoch: 4/25, Step 1840, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 28.964\n",
      "Epoch: 4/25, Step 1880, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.420\n",
      "Epoch: 4/25, Step 1920, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.487\n",
      "Epoch 4/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.329 Test reconstruction loss: 0.018 Test covariance loss: 31.144\n",
      "Epoch: 5/25, Step 1960, Train loss: 0.302 Train reconstruction loss: 0.019 Train covariance loss: 28.365\n",
      "Epoch: 5/25, Step 2000, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.853\n",
      "Epoch: 5/25, Step 2040, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.149\n",
      "Epoch: 5/25, Step 2080, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.426\n",
      "Epoch: 5/25, Step 2120, Train loss: 0.310 Train reconstruction loss: 0.019 Train covariance loss: 29.183\n",
      "Epoch: 5/25, Step 2160, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.417\n",
      "Epoch: 5/25, Step 2200, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.887\n",
      "Epoch: 5/25, Step 2240, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.554\n",
      "Epoch: 5/25, Step 2280, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.587\n",
      "Epoch: 5/25, Step 2320, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.178\n",
      "Epoch: 5/25, Step 2360, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.597\n",
      "Epoch: 5/25, Step 2400, Train loss: 0.314 Train reconstruction loss: 0.019 Train covariance loss: 29.588\n",
      "Epoch 5/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.333 Test reconstruction loss: 0.018 Test covariance loss: 31.546\n",
      "Epoch: 6/25, Step 2440, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.719\n",
      "Epoch: 6/25, Step 2480, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.341\n",
      "Epoch: 6/25, Step 2520, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.056\n",
      "Epoch: 6/25, Step 2560, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.197\n",
      "Epoch: 6/25, Step 2600, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.140\n",
      "Epoch: 6/25, Step 2640, Train loss: 0.316 Train reconstruction loss: 0.018 Train covariance loss: 29.753\n",
      "Epoch: 6/25, Step 2680, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.642\n",
      "Epoch: 6/25, Step 2720, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.031\n",
      "Epoch: 6/25, Step 2760, Train loss: 0.300 Train reconstruction loss: 0.018 Train covariance loss: 28.198\n",
      "Epoch: 6/25, Step 2800, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.053\n",
      "Epoch: 6/25, Step 2840, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.853\n",
      "Epoch: 6/25, Step 2880, Train loss: 0.317 Train reconstruction loss: 0.018 Train covariance loss: 29.906\n",
      "Epoch 6/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.336 Test reconstruction loss: 0.018 Test covariance loss: 31.879\n",
      "Epoch: 7/25, Step 2920, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.356\n",
      "Epoch: 7/25, Step 2960, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.025\n",
      "Epoch: 7/25, Step 3000, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.900\n",
      "Epoch: 7/25, Step 3040, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.156\n",
      "Epoch: 7/25, Step 3080, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.055\n",
      "Epoch: 7/25, Step 3120, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.247\n",
      "Epoch: 7/25, Step 3160, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.765\n",
      "Epoch: 7/25, Step 3200, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.293\n",
      "Epoch: 7/25, Step 3240, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.584\n",
      "Epoch: 7/25, Step 3280, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.339\n",
      "Epoch: 7/25, Step 3320, Train loss: 0.318 Train reconstruction loss: 0.018 Train covariance loss: 30.030\n",
      "Epoch: 7/25, Step 3360, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.818\n",
      "Epoch 7/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.319 Test reconstruction loss: 0.017 Test covariance loss: 30.122\n",
      "Epoch: 8/25, Step 3400, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.322\n",
      "Epoch: 8/25, Step 3440, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.021\n",
      "Epoch: 8/25, Step 3480, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.850\n",
      "Epoch: 8/25, Step 3520, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.141\n",
      "Epoch: 8/25, Step 3560, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.205\n",
      "Epoch: 8/25, Step 3600, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 28.981\n",
      "Epoch: 8/25, Step 3640, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.359\n",
      "Epoch: 8/25, Step 3680, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.898\n",
      "Epoch: 8/25, Step 3720, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 28.942\n",
      "Epoch: 8/25, Step 3760, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.365\n",
      "Epoch: 8/25, Step 3800, Train loss: 0.303 Train reconstruction loss: 0.018 Train covariance loss: 28.523\n",
      "Epoch: 8/25, Step 3840, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.223\n",
      "Test loss decreased (0.308809 --> 0.304343).  Saving model ...\n",
      "Epoch 8/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.304 Test reconstruction loss: 0.017 Test covariance loss: 28.687\n",
      "Epoch: 9/25, Step 3880, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.944\n",
      "Epoch: 9/25, Step 3920, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.742\n",
      "Epoch: 9/25, Step 3960, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.318\n",
      "Epoch: 9/25, Step 4000, Train loss: 0.314 Train reconstruction loss: 0.018 Train covariance loss: 29.588\n",
      "Epoch: 9/25, Step 4040, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.901\n",
      "Epoch: 9/25, Step 4080, Train loss: 0.301 Train reconstruction loss: 0.018 Train covariance loss: 28.333\n",
      "Epoch: 9/25, Step 4120, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.775\n",
      "Epoch: 9/25, Step 4160, Train loss: 0.300 Train reconstruction loss: 0.018 Train covariance loss: 28.154\n",
      "Epoch: 9/25, Step 4200, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.058\n",
      "Epoch: 9/25, Step 4240, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.513\n",
      "Epoch: 9/25, Step 4280, Train loss: 0.317 Train reconstruction loss: 0.018 Train covariance loss: 29.939\n",
      "Epoch: 9/25, Step 4320, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.553\n",
      "Epoch 9/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.330 Test reconstruction loss: 0.017 Test covariance loss: 31.273\n",
      "Epoch: 10/25, Step 4360, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.089\n",
      "Epoch: 10/25, Step 4400, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.587\n",
      "Epoch: 10/25, Step 4440, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.823\n",
      "Epoch: 10/25, Step 4480, Train loss: 0.314 Train reconstruction loss: 0.018 Train covariance loss: 29.574\n",
      "Epoch: 10/25, Step 4520, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.812\n",
      "Epoch: 10/25, Step 4560, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.346\n",
      "Epoch: 10/25, Step 4600, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.420\n",
      "Epoch: 10/25, Step 4640, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.610\n",
      "Epoch: 10/25, Step 4680, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.310\n",
      "Epoch: 10/25, Step 4720, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 28.990\n",
      "Epoch: 10/25, Step 4760, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.313\n",
      "Epoch: 10/25, Step 4800, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.539\n",
      "Test loss decreased (0.304343 --> 0.288768).  Saving model ...\n",
      "Epoch 10/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.289 Test reconstruction loss: 0.017 Test covariance loss: 27.143\n",
      "Epoch: 11/25, Step 4840, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.258\n",
      "Epoch: 11/25, Step 4880, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.155\n",
      "Epoch: 11/25, Step 4920, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.209\n",
      "Epoch: 11/25, Step 4960, Train loss: 0.303 Train reconstruction loss: 0.018 Train covariance loss: 28.482\n",
      "Epoch: 11/25, Step 5000, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.264\n",
      "Epoch: 11/25, Step 5040, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.845\n",
      "Epoch: 11/25, Step 5080, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.066\n",
      "Epoch: 11/25, Step 5120, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.390\n",
      "Epoch: 11/25, Step 5160, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.197\n",
      "Epoch: 11/25, Step 5200, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.249\n",
      "Epoch: 11/25, Step 5240, Train loss: 0.315 Train reconstruction loss: 0.018 Train covariance loss: 29.699\n",
      "Epoch: 11/25, Step 5280, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.886\n",
      "Epoch 11/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.331 Test reconstruction loss: 0.017 Test covariance loss: 31.373\n",
      "Epoch: 12/25, Step 5320, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.966\n",
      "Epoch: 12/25, Step 5360, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.273\n",
      "Epoch: 12/25, Step 5400, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.450\n",
      "Epoch: 12/25, Step 5440, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.382\n",
      "Epoch: 12/25, Step 5480, Train loss: 0.302 Train reconstruction loss: 0.018 Train covariance loss: 28.375\n",
      "Epoch: 12/25, Step 5520, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.923\n",
      "Epoch: 12/25, Step 5560, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.543\n",
      "Epoch: 12/25, Step 5600, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.252\n",
      "Epoch: 12/25, Step 5640, Train loss: 0.307 Train reconstruction loss: 0.017 Train covariance loss: 28.977\n",
      "Epoch: 12/25, Step 5680, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.603\n",
      "Epoch: 12/25, Step 5720, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.256\n",
      "Epoch: 12/25, Step 5760, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.019\n",
      "Epoch 12/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.336 Test reconstruction loss: 0.017 Test covariance loss: 31.919\n",
      "Epoch: 13/25, Step 5800, Train loss: 0.314 Train reconstruction loss: 0.018 Train covariance loss: 29.627\n",
      "Epoch: 13/25, Step 5840, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.820\n",
      "Epoch: 13/25, Step 5880, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.702\n",
      "Epoch: 13/25, Step 5920, Train loss: 0.301 Train reconstruction loss: 0.018 Train covariance loss: 28.294\n",
      "Epoch: 13/25, Step 5960, Train loss: 0.320 Train reconstruction loss: 0.018 Train covariance loss: 30.207\n",
      "Epoch: 13/25, Step 6000, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.656\n",
      "Epoch: 13/25, Step 6040, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.507\n",
      "Epoch: 13/25, Step 6080, Train loss: 0.314 Train reconstruction loss: 0.017 Train covariance loss: 29.707\n",
      "Epoch: 13/25, Step 6120, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.117\n",
      "Epoch: 13/25, Step 6160, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.701\n",
      "Epoch: 13/25, Step 6200, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.873\n",
      "Epoch: 13/25, Step 6240, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.048\n",
      "Epoch 13/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.302 Test reconstruction loss: 0.017 Test covariance loss: 28.466\n",
      "Epoch: 14/25, Step 6280, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.798\n",
      "Epoch: 14/25, Step 6320, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.941\n",
      "Epoch: 14/25, Step 6360, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.375\n",
      "Epoch: 14/25, Step 6400, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.451\n",
      "Epoch: 14/25, Step 6440, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.039\n",
      "Epoch: 14/25, Step 6480, Train loss: 0.309 Train reconstruction loss: 0.017 Train covariance loss: 29.164\n",
      "Epoch: 14/25, Step 6520, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.039\n",
      "Epoch: 14/25, Step 6560, Train loss: 0.302 Train reconstruction loss: 0.018 Train covariance loss: 28.464\n",
      "Epoch: 14/25, Step 6600, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.829\n",
      "Epoch: 14/25, Step 6640, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.390\n",
      "Epoch: 14/25, Step 6680, Train loss: 0.312 Train reconstruction loss: 0.017 Train covariance loss: 29.428\n",
      "Epoch: 14/25, Step 6720, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.333\n",
      "Epoch 14/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.318 Test reconstruction loss: 0.017 Test covariance loss: 30.037\n",
      "Epoch: 15/25, Step 6760, Train loss: 0.309 Train reconstruction loss: 0.017 Train covariance loss: 29.184\n",
      "Epoch: 15/25, Step 6800, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.834\n",
      "Epoch: 15/25, Step 6840, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.186\n",
      "Epoch: 15/25, Step 6880, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.966\n",
      "Epoch: 15/25, Step 6920, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.753\n",
      "Epoch: 15/25, Step 6960, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.721\n",
      "Epoch: 15/25, Step 7000, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.196\n",
      "Epoch: 15/25, Step 7040, Train loss: 0.310 Train reconstruction loss: 0.017 Train covariance loss: 29.272\n",
      "Epoch: 15/25, Step 7080, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.497\n",
      "Epoch: 15/25, Step 7120, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.275\n",
      "Epoch: 15/25, Step 7160, Train loss: 0.303 Train reconstruction loss: 0.018 Train covariance loss: 28.524\n",
      "Epoch: 15/25, Step 7200, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.062\n",
      "Epoch 15/25,\n",
      " Train Loss: 0.308 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.328 Test reconstruction loss: 0.017 Test covariance loss: 31.078\n",
      "Epoch: 16/25, Step 7240, Train loss: 0.308 Train reconstruction loss: 0.017 Train covariance loss: 29.094\n",
      "Epoch: 16/25, Step 7280, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.749\n",
      "Epoch: 16/25, Step 7320, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.819\n",
      "Epoch: 16/25, Step 7360, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.766\n",
      "Epoch: 16/25, Step 7400, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.875\n",
      "Epoch: 16/25, Step 7440, Train loss: 0.312 Train reconstruction loss: 0.017 Train covariance loss: 29.497\n",
      "Epoch: 16/25, Step 7480, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.261\n",
      "Epoch: 16/25, Step 7520, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.028\n",
      "Epoch: 16/25, Step 7560, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.518\n",
      "Epoch: 16/25, Step 7600, Train loss: 0.308 Train reconstruction loss: 0.017 Train covariance loss: 29.084\n",
      "Epoch: 16/25, Step 7640, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.739\n",
      "Epoch: 16/25, Step 7680, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.896\n",
      "Epoch 16/25,\n",
      " Train Loss: 0.308 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.314 Test reconstruction loss: 0.017 Test covariance loss: 29.678\n",
      "Epoch: 17/25, Step 7720, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.240\n",
      "Epoch: 17/25, Step 7760, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.218\n",
      "Epoch: 17/25, Step 7800, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.723\n",
      "Epoch: 17/25, Step 7840, Train loss: 0.314 Train reconstruction loss: 0.018 Train covariance loss: 29.648\n",
      "Epoch: 17/25, Step 7880, Train loss: 0.316 Train reconstruction loss: 0.017 Train covariance loss: 29.875\n",
      "Epoch: 17/25, Step 7920, Train loss: 0.303 Train reconstruction loss: 0.018 Train covariance loss: 28.578\n",
      "Epoch: 17/25, Step 7960, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.351\n",
      "Epoch: 17/25, Step 8000, Train loss: 0.308 Train reconstruction loss: 0.017 Train covariance loss: 29.089\n",
      "Epoch: 17/25, Step 8040, Train loss: 0.309 Train reconstruction loss: 0.017 Train covariance loss: 29.189\n",
      "Epoch: 17/25, Step 8080, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.378\n",
      "Epoch: 17/25, Step 8120, Train loss: 0.295 Train reconstruction loss: 0.018 Train covariance loss: 27.754\n",
      "Epoch: 17/25, Step 8160, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.064\n",
      "Epoch 17/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.315 Test reconstruction loss: 0.017 Test covariance loss: 29.772\n",
      "Epoch: 18/25, Step 8200, Train loss: 0.303 Train reconstruction loss: 0.017 Train covariance loss: 28.526\n",
      "Epoch: 18/25, Step 8240, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.169\n",
      "Epoch: 18/25, Step 8280, Train loss: 0.308 Train reconstruction loss: 0.017 Train covariance loss: 29.125\n",
      "Epoch: 18/25, Step 8320, Train loss: 0.311 Train reconstruction loss: 0.017 Train covariance loss: 29.366\n",
      "Epoch: 18/25, Step 8360, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.652\n",
      "Epoch: 18/25, Step 8400, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.013\n",
      "Epoch: 18/25, Step 8440, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.126\n",
      "Epoch: 18/25, Step 8480, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.517\n",
      "Epoch: 18/25, Step 8520, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.134\n",
      "Epoch: 18/25, Step 8560, Train loss: 0.309 Train reconstruction loss: 0.017 Train covariance loss: 29.157\n",
      "Epoch: 18/25, Step 8600, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.349\n",
      "Epoch: 18/25, Step 8640, Train loss: 0.311 Train reconstruction loss: 0.017 Train covariance loss: 29.379\n",
      "Test loss decreased (0.288768 --> 0.286499).  Saving model ...\n",
      "Epoch 18/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.286 Test reconstruction loss: 0.017 Test covariance loss: 26.935\n",
      "Epoch: 19/25, Step 8680, Train loss: 0.309 Train reconstruction loss: 0.017 Train covariance loss: 29.190\n",
      "Epoch: 19/25, Step 8720, Train loss: 0.309 Train reconstruction loss: 0.017 Train covariance loss: 29.139\n",
      "Epoch: 19/25, Step 8760, Train loss: 0.306 Train reconstruction loss: 0.017 Train covariance loss: 28.858\n",
      "Epoch: 19/25, Step 8800, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.031\n",
      "Epoch: 19/25, Step 8840, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.522\n",
      "Epoch: 19/25, Step 8880, Train loss: 0.314 Train reconstruction loss: 0.017 Train covariance loss: 29.640\n",
      "Epoch: 19/25, Step 8920, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.822\n",
      "Epoch: 19/25, Step 8960, Train loss: 0.316 Train reconstruction loss: 0.018 Train covariance loss: 29.826\n",
      "Epoch: 19/25, Step 9000, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.638\n",
      "Epoch: 19/25, Step 9040, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.919\n",
      "Epoch: 19/25, Step 9080, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.207\n",
      "Epoch: 19/25, Step 9120, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.229\n",
      "Epoch 19/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.295 Test reconstruction loss: 0.017 Test covariance loss: 27.821\n",
      "Epoch: 20/25, Step 9160, Train loss: 0.311 Train reconstruction loss: 0.017 Train covariance loss: 29.344\n",
      "Epoch: 20/25, Step 9200, Train loss: 0.303 Train reconstruction loss: 0.018 Train covariance loss: 28.512\n",
      "Epoch: 20/25, Step 9240, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.871\n",
      "Epoch: 20/25, Step 9280, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.332\n",
      "Epoch: 20/25, Step 9320, Train loss: 0.313 Train reconstruction loss: 0.017 Train covariance loss: 29.623\n",
      "Epoch: 20/25, Step 9360, Train loss: 0.310 Train reconstruction loss: 0.017 Train covariance loss: 29.223\n",
      "Epoch: 20/25, Step 9400, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.212\n",
      "Epoch: 20/25, Step 9440, Train loss: 0.304 Train reconstruction loss: 0.018 Train covariance loss: 28.614\n",
      "Epoch: 20/25, Step 9480, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.561\n",
      "Epoch: 20/25, Step 9520, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.886\n",
      "Epoch: 20/25, Step 9560, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.213\n",
      "Epoch: 20/25, Step 9600, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.744\n",
      "Epoch 20/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.301 Test reconstruction loss: 0.017 Test covariance loss: 28.354\n",
      "Epoch: 21/25, Step 9640, Train loss: 0.309 Train reconstruction loss: 0.017 Train covariance loss: 29.192\n",
      "Epoch: 21/25, Step 9680, Train loss: 0.304 Train reconstruction loss: 0.017 Train covariance loss: 28.642\n",
      "Epoch: 21/25, Step 9720, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.255\n",
      "Epoch: 21/25, Step 9760, Train loss: 0.306 Train reconstruction loss: 0.018 Train covariance loss: 28.818\n",
      "Epoch: 21/25, Step 9800, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.345\n",
      "Epoch: 21/25, Step 9840, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.984\n",
      "Epoch: 21/25, Step 9880, Train loss: 0.309 Train reconstruction loss: 0.018 Train covariance loss: 29.174\n",
      "Epoch: 21/25, Step 9920, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.530\n",
      "Epoch: 21/25, Step 9960, Train loss: 0.306 Train reconstruction loss: 0.017 Train covariance loss: 28.815\n",
      "Epoch: 21/25, Step 10000, Train loss: 0.312 Train reconstruction loss: 0.018 Train covariance loss: 29.457\n",
      "Epoch: 21/25, Step 10040, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.679\n",
      "Epoch: 21/25, Step 10080, Train loss: 0.308 Train reconstruction loss: 0.017 Train covariance loss: 29.069\n",
      "Epoch 21/25,\n",
      " Train Loss: 0.308 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.316 Test reconstruction loss: 0.017 Test covariance loss: 29.913\n",
      "Epoch: 22/25, Step 10120, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.511\n",
      "Epoch: 22/25, Step 10160, Train loss: 0.307 Train reconstruction loss: 0.017 Train covariance loss: 28.929\n",
      "Epoch: 22/25, Step 10200, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.719\n",
      "Epoch: 22/25, Step 10240, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.930\n",
      "Epoch: 22/25, Step 10280, Train loss: 0.310 Train reconstruction loss: 0.017 Train covariance loss: 29.313\n",
      "Epoch: 22/25, Step 10320, Train loss: 0.307 Train reconstruction loss: 0.017 Train covariance loss: 28.950\n",
      "Epoch: 22/25, Step 10360, Train loss: 0.315 Train reconstruction loss: 0.018 Train covariance loss: 29.691\n",
      "Epoch: 22/25, Step 10400, Train loss: 0.310 Train reconstruction loss: 0.017 Train covariance loss: 29.251\n",
      "Epoch: 22/25, Step 10440, Train loss: 0.305 Train reconstruction loss: 0.017 Train covariance loss: 28.798\n",
      "Epoch: 22/25, Step 10480, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.297\n",
      "Epoch: 22/25, Step 10520, Train loss: 0.308 Train reconstruction loss: 0.017 Train covariance loss: 29.085\n",
      "Epoch: 22/25, Step 10560, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.881\n",
      "Epoch 22/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.345 Test reconstruction loss: 0.017 Test covariance loss: 32.760\n",
      "Epoch: 23/25, Step 10600, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.298\n",
      "Epoch: 23/25, Step 10640, Train loss: 0.306 Train reconstruction loss: 0.017 Train covariance loss: 28.893\n",
      "Epoch: 23/25, Step 10680, Train loss: 0.305 Train reconstruction loss: 0.017 Train covariance loss: 28.828\n",
      "Epoch: 23/25, Step 10720, Train loss: 0.308 Train reconstruction loss: 0.017 Train covariance loss: 29.050\n",
      "Epoch: 23/25, Step 10760, Train loss: 0.303 Train reconstruction loss: 0.017 Train covariance loss: 28.522\n",
      "Epoch: 23/25, Step 10800, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.478\n",
      "Epoch: 23/25, Step 10840, Train loss: 0.305 Train reconstruction loss: 0.017 Train covariance loss: 28.795\n",
      "Epoch: 23/25, Step 10880, Train loss: 0.305 Train reconstruction loss: 0.018 Train covariance loss: 28.702\n",
      "Epoch: 23/25, Step 10920, Train loss: 0.306 Train reconstruction loss: 0.017 Train covariance loss: 28.899\n",
      "Epoch: 23/25, Step 10960, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.383\n",
      "Epoch: 23/25, Step 11000, Train loss: 0.313 Train reconstruction loss: 0.018 Train covariance loss: 29.532\n",
      "Epoch: 23/25, Step 11040, Train loss: 0.308 Train reconstruction loss: 0.017 Train covariance loss: 29.080\n",
      "Epoch 23/25,\n",
      " Train Loss: 0.308 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.314 Test reconstruction loss: 0.017 Test covariance loss: 29.659\n",
      "Epoch: 24/25, Step 11080, Train loss: 0.301 Train reconstruction loss: 0.018 Train covariance loss: 28.282\n",
      "Epoch: 24/25, Step 11120, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.387\n",
      "Epoch: 24/25, Step 11160, Train loss: 0.307 Train reconstruction loss: 0.018 Train covariance loss: 28.923\n",
      "Epoch: 24/25, Step 11200, Train loss: 0.307 Train reconstruction loss: 0.017 Train covariance loss: 28.942\n",
      "Epoch: 24/25, Step 11240, Train loss: 0.304 Train reconstruction loss: 0.017 Train covariance loss: 28.637\n",
      "Epoch: 24/25, Step 11280, Train loss: 0.312 Train reconstruction loss: 0.017 Train covariance loss: 29.463\n",
      "Epoch: 24/25, Step 11320, Train loss: 0.303 Train reconstruction loss: 0.017 Train covariance loss: 28.601\n",
      "Epoch: 24/25, Step 11360, Train loss: 0.313 Train reconstruction loss: 0.017 Train covariance loss: 29.559\n",
      "Epoch: 24/25, Step 11400, Train loss: 0.305 Train reconstruction loss: 0.017 Train covariance loss: 28.760\n",
      "Epoch: 24/25, Step 11440, Train loss: 0.306 Train reconstruction loss: 0.017 Train covariance loss: 28.934\n",
      "Epoch: 24/25, Step 11480, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.028\n",
      "Epoch: 24/25, Step 11520, Train loss: 0.310 Train reconstruction loss: 0.018 Train covariance loss: 29.187\n",
      "Test loss decreased (0.286499 --> 0.282750).  Saving model ...\n",
      "Epoch 24/25,\n",
      " Train Loss: 0.307 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.283 Test reconstruction loss: 0.017 Test covariance loss: 26.577\n",
      "Epoch: 25/25, Step 11560, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.294\n",
      "Epoch: 25/25, Step 11600, Train loss: 0.305 Train reconstruction loss: 0.017 Train covariance loss: 28.749\n",
      "Epoch: 25/25, Step 11640, Train loss: 0.303 Train reconstruction loss: 0.018 Train covariance loss: 28.566\n",
      "Epoch: 25/25, Step 11680, Train loss: 0.305 Train reconstruction loss: 0.017 Train covariance loss: 28.769\n",
      "Epoch: 25/25, Step 11720, Train loss: 0.311 Train reconstruction loss: 0.017 Train covariance loss: 29.361\n",
      "Epoch: 25/25, Step 11760, Train loss: 0.311 Train reconstruction loss: 0.018 Train covariance loss: 29.377\n",
      "Epoch: 25/25, Step 11800, Train loss: 0.306 Train reconstruction loss: 0.017 Train covariance loss: 28.920\n",
      "Epoch: 25/25, Step 11840, Train loss: 0.308 Train reconstruction loss: 0.018 Train covariance loss: 29.013\n",
      "Epoch: 25/25, Step 11880, Train loss: 0.313 Train reconstruction loss: 0.017 Train covariance loss: 29.515\n",
      "Epoch: 25/25, Step 11920, Train loss: 0.314 Train reconstruction loss: 0.017 Train covariance loss: 29.697\n",
      "Epoch: 25/25, Step 11960, Train loss: 0.305 Train reconstruction loss: 0.017 Train covariance loss: 28.759\n",
      "Epoch: 25/25, Step 12000, Train loss: 0.315 Train reconstruction loss: 0.017 Train covariance loss: 29.702\n",
      "Epoch 25/25,\n",
      " Train Loss: 0.309 Train reconstruction loss: 0.000 Train covariance loss: 0.000\n",
      "Test Loss: 0.315 Test reconstruction loss: 0.017 Test covariance loss: 29.766\n"
     ]
    }
   ],
   "source": [
    "logs = train_model(model=model, goal_hidden_dim=5, \n",
    "                                         optimizer=optimizer, loss_func=criterion, epochs=epochs,\n",
    "                                         trainloader=trainloader, testloader=testloader, print_every=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax[0].plot(logs[\"train_losses\"], label=\"Train\")\n",
    "ax[0].plot(logs[\"test_losses\"], label=\"Validation\")\n",
    "ax[0].set_title(\"Combined loss\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(logs[\"train_recon_losses\"], label=\"Train\")\n",
    "ax[1].plot(logs[\"test_recon_losses\"], label=\"Validation\")\n",
    "ax[1].set_title(\"Reconstruction loss\")\n",
    "ax[1].legend()\n",
    "ax[2].plot(logs[\"train_cov_losses\"], label=\"Train\")\n",
    "ax[2].plot(logs[\"test_cov_losses\"], label=\"Validation\")\n",
    "ax[2].set_title(\"Covariance loss\")\n",
    "ax[2].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
